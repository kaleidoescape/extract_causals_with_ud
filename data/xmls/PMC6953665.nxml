<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Ecol Evol</journal-id><journal-id journal-id-type="iso-abbrev">Ecol Evol</journal-id><journal-id journal-id-type="doi">10.1002/(ISSN)2045-7758</journal-id><journal-id journal-id-type="publisher-id">ECE3</journal-id><journal-title-group><journal-title>Ecology and Evolution</journal-title></journal-title-group><issn pub-type="epub">2045-7758</issn><publisher><publisher-name>John Wiley and Sons Inc.</publisher-name><publisher-loc>Hoboken</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">31938476</article-id><article-id pub-id-type="pmc">6953665</article-id><article-id pub-id-type="doi">10.1002/ece3.5767</article-id><article-id pub-id-type="publisher-id">ECE35767</article-id><article-categories><subj-group subj-group-type="overline"><subject>Original Research</subject></subj-group><subj-group subj-group-type="heading"><subject>Original Research</subject></subj-group></article-categories><title-group><article-title>Design patterns for wildlife&#x02010;related camera trap image analysis</article-title><alt-title alt-title-type="left-running-head">GREENBERG et al.</alt-title></title-group><contrib-group><contrib id="ece35767-cr-0001" contrib-type="author" corresp="yes"><name><surname>Greenberg</surname><given-names>Saul</given-names></name><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-0174-9665</contrib-id><xref ref-type="aff" rid="ece35767-aff-0001">
<sup>1</sup>
</xref><address><email>saul@ucalgary.ca</email></address></contrib><contrib id="ece35767-cr-0002" contrib-type="author"><name><surname>Godin</surname><given-names>Theresa</given-names></name><xref ref-type="aff" rid="ece35767-aff-0002">
<sup>2</sup>
</xref></contrib><contrib id="ece35767-cr-0003" contrib-type="author"><name><surname>Whittington</surname><given-names>Jesse</given-names></name><xref ref-type="aff" rid="ece35767-aff-0003">
<sup>3</sup>
</xref></contrib></contrib-group><aff id="ece35767-aff-0001">
<label><sup>1</sup></label>
<named-content content-type="organisation-division">Department of Computer Science</named-content>
<institution>University of Calgary</institution>
<city>Calgary</city>
<named-content content-type="country-part">AB</named-content>
<country country="CA">Canada</country>
</aff><aff id="ece35767-aff-0002">
<label><sup>2</sup></label>
<named-content content-type="organisation-division">Freshwater Fisheries Society of BC Research Evaluation &#x00026; Development Section</named-content>
<institution>University of British Columbia</institution>
<city>Vancouver</city>
<named-content content-type="country-part">BC</named-content>
<country country="CA">Canada</country>
</aff><aff id="ece35767-aff-0003">
<label><sup>3</sup></label>
<institution>Parks Canada, Banff National Park</institution>
<city>Banff</city>
<named-content content-type="country-part">AB</named-content>
<country country="CA">Canada</country>
</aff><author-notes><corresp id="correspondenceTo"><label>*</label><bold>Correspondence</bold><break/>
Saul Greenberg, Department of Computer Science, University of Calgary, 2500 University Dr. NW, Calgary, AB T2N 1N4, Canada.<break/>
Email: <email>saul@ucalgary.ca</email><break/></corresp></author-notes><pub-date pub-type="epub"><day>02</day><month>12</month><year>2019</year></pub-date><pub-date pub-type="collection"><month>12</month><year>2019</year></pub-date><volume>9</volume><issue>24</issue><issue-id pub-id-type="doi">10.1002/ece3.v9.24</issue-id><fpage>13706</fpage><lpage>13730</lpage><history><date date-type="received"><day>18</day><month>6</month><year>2019</year></date><date date-type="rev-recd"><day>15</day><month>8</month><year>2019</year></date><date date-type="accepted"><day>30</day><month>8</month><year>2019</year></date></history><permissions><!--<copyright-statement content-type="issue-copyright"> &#x000a9; 2019 Published by John Wiley & Sons Ltd. <copyright-statement>--><copyright-statement content-type="article-copyright">&#x000a9; 2019 The Authors. <italic>Ecology and Evolution</italic> published by John Wiley &#x00026; Sons Ltd</copyright-statement><license license-type="creativeCommonsBy"><license-p>This is an open access article under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link> License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="file:ECE3-9-13706.pdf"/><abstract id="ece35767-abs-0001"><title>Abstract</title><p>This paper describes and explains <italic>design patterns</italic> for software that supports how analysts can efficiently inspect and classify camera trap images for wildlife&#x02010;related ecological attributes. Broadly speaking, a design pattern identifies a commonly occurring problem and a general reusable design approach to solve that problem. A developer can then use that design approach to create a specific software solution appropriate to the particular situation under consideration. In particular, design patterns for camera trap image analysis by wildlife biologists address solutions to commonly occurring problems they face while inspecting a large number of images and entering ecological data describing image attributes. We developed design patterns for image classification based on our understanding of biologists' needs that we acquired over 8&#x000a0;years during development and application of the freely available Timelapse image analysis system. For each design pattern presented, we describe the problem, a design approach that solves that problem, and a concrete example of how Timelapse addresses the design pattern. Our design patterns offer both general and specific solutions related to: maintaining data consistency, efficiencies in image inspection, methods for navigating between images, efficiencies in data entry including highly repetitious data entry, and sorting and filtering image into sequences, episodes, and subsets. These design patterns can inform the design of other camera trap systems and can help biologists assess how competing software products address their project&#x02010;specific needs along with determining an efficient workflow.</p></abstract><abstract abstract-type="graphical" id="ece35767-abs-0002"><p>We describe design patterns for software that supports how ecologists can efficiently inspect and classify camera trap images for wildlife&#x02010;related ecological attributes. Each design pattern addresses a problem and possible solutions to common issues faced by ecologists when inspecting and entering tagging data for a massive number of images. These design patterns inform the design of other camera trap analysis systems, and help biologists assess how competing software products address their project&#x02010;specific needs.<boxed-text position="anchor" content-type="graphic" id="ece35767-blkfxd-0001" orientation="portrait"><graphic xlink:href="ECE3-9-13706-g020.jpg" position="anchor" id="nlm-graphic-1" orientation="portrait"/></boxed-text>
</p></abstract><kwd-group kwd-group-type="author-generated"><kwd id="ece35767-kwd-0001">camera traps</kwd><kwd id="ece35767-kwd-0002">data encoding and acquisition</kwd><kwd id="ece35767-kwd-0003">design patterns</kwd><kwd id="ece35767-kwd-0004">experience design</kwd><kwd id="ece35767-kwd-0005">human&#x02013;computer interaction</kwd><kwd id="ece35767-kwd-0006">image inspection</kwd><kwd id="ece35767-kwd-0007">tagging</kwd><kwd id="ece35767-kwd-0008">wildlife monitoring</kwd></kwd-group><funding-group><award-group id="funding-0001"><funding-source>National Science and Engineering Research Council of Canada</funding-source></award-group></funding-group><counts><fig-count count="19"/><table-count count="0"/><page-count count="24"/><word-count count="16131"/></counts><custom-meta-group><custom-meta><meta-name>source-schema-version-number</meta-name><meta-value>2.0</meta-value></custom-meta><custom-meta><meta-name>cover-date</meta-name><meta-value>December 2019</meta-value></custom-meta><custom-meta><meta-name>details-of-publishers-convertor</meta-name><meta-value>Converter:WILEY_ML3GV2_TO_JATSPMC version:5.7.4 mode:remove_FC converted:10.01.2020</meta-value></custom-meta></custom-meta-group></article-meta><notes><p content-type="self-citation">
<mixed-citation publication-type="journal" id="ece35767-cit-1001">
<string-name>
<surname>Greenberg</surname>
<given-names>S</given-names>
</string-name>, <string-name>
<surname>Godin</surname>
<given-names>T</given-names>
</string-name>, <string-name>
<surname>Whittington</surname>
<given-names>J</given-names>
</string-name>. <article-title>Design patterns for wildlife&#x02010;related camera trap image analysis</article-title>. <source xml:lang="en">Ecol Evol</source>. <year>2019</year>;<volume>9</volume>:<fpage>13706</fpage>&#x02013;<lpage>13730</lpage>. <pub-id pub-id-type="doi">10.1002/ece3.5767</pub-id>
</mixed-citation>
</p></notes></front><body id="ece35767-body-0001"><sec id="ece35767-sec-0001"><label>1</label><title>INTRODUCTION</title><p>Camera traps&#x02014;also known as wildlife, remote, field, or trail cameras&#x02014;are increasingly used to address a broad range of ecological research and field monitoring applications (e.g., Steenweg et al., <xref rid="ece35767-bib-0043" ref-type="ref">2017</xref>; Swann, Kawanishi, &#x00026; Palmer, <xref rid="ece35767-bib-0044" ref-type="ref">2010</xref>). Their basic idea is deceptively simple. First, cameras are located at strategic stations within a geographic study site, where they are positioned to capture activities occurring within a particular field of view (e.g., Tobler, Z&#x000fa;&#x000f1;iga Hartley, Carrillo&#x02010;Percastegui, &#x00026; Powell, <xref rid="ece35767-bib-0048" ref-type="ref">2015</xref>). Second, cameras are set up to take images automatically in one of two ways: a <italic>Timelapse</italic> mode where images are taken at regular intervals, or a <italic>motion&#x02010;triggering</italic> mode where one or more images are taken whenever movement is detected in the scene. Third, cameras are serviced after a period of time (weeks or months), where field personnel change camera batteries and retrieve the image&#x02010;laden SD cards. Fourth, analysts review the set of retrieved images. The analyst examines each image for attributes of interest and encodes those as descriptive or quantitative data. This step is also known as <italic>tagging</italic>. Finally, that data&#x02014;usually managed and stored in a database or spreadsheet&#x02014;become the input for the data processing (including statistical analysis) particular to the project.</p><p>This paper is primarily concerned with the fourth tagging step described above: how analysts examine images and encode its attributes of interest as data. The number of images typically collected is voluminous: thousands or tens of thousands of images per retrieved camera card easily accumulate to hundreds of thousands and even millions of images per project. Consequently, image examination and data encoding are laborious, time&#x02010;intensive, error&#x02010;prone, and expensive. It is no wonder that international survey respondents identified image classification as the top challenge in camera trapping (Glover&#x02010;Kapfer, Soto&#x02010;Navarro, &#x00026; Wearn, <xref rid="ece35767-bib-0016" ref-type="ref">2019</xref>).</p><p>Recent research seeks to remedy this burden via automated image recognition, where promising wildlife species detection and identification rates have been reported (e.g., Norouzzadeha et al., <xref rid="ece35767-bib-0031" ref-type="ref">2018</xref>; Schneider, Taylor, &#x00026; Kramer, <xref rid="ece35767-bib-0039" ref-type="ref">2018</xref>; Tabak et al., <xref rid="ece35767-bib-0047" ref-type="ref">2018</xref>; Yousif, Yuan, Kays, &#x00026; He, <xref rid="ece35767-bib-0055" ref-type="ref">2019</xref>). Unfortunately, image recognition for camera traps is still in its formative stage. It is limited in what can be recognized. For example, somewhat easy to extremely difficult recognition challenges range from: detecting if wildlife is present, identifying the species, identifying individuals, determining animal health, to distinguishing animal behaviors. Image recognition also: requires a model trained on domain&#x02010;specific images; incurs varying rates of classification errors (false positives and false negatives); performs less well with new camera placements (due to different backgrounds); and is currently poorly integrated in the analyst's software and workflow. Even if it was available, analysts would still have to verify recognition predictions and correct erroneous ones. Manual methods will likely dominate for years to come, especially when additional attributes beyond simple species detection are required (but see Section <xref rid="ece35767-sec-0044" ref-type="sec">8</xref> below).</p><p>In the past, analysts resorted to off&#x02010;the&#x02010;shelf generic software when tagging images, such as a stock image viewer to view images, and a separate spreadsheet package to record data. More recently, researchers and corporations have developed specialized software packages to support camera trap analysis (e.g., Bubnicki, Churski, &#x00026; Kuijper, <xref rid="ece35767-bib-0005" ref-type="ref">2016</xref>; Ivan &#x00026; Newkirk, <xref rid="ece35767-bib-0022" ref-type="ref">2016</xref>; Krishnappa &#x00026; Turner, <xref rid="ece35767-bib-0026" ref-type="ref">2014</xref>; Reconyx Inc, <xref rid="ece35767-bib-0035" ref-type="ref">2016</xref>; Scotson et al., <xref rid="ece35767-bib-0040" ref-type="ref">2017</xref>; Swanson et al., <xref rid="ece35767-bib-0046" ref-type="ref">2015</xref>; WildTrax, <xref rid="ece35767-bib-0053" ref-type="ref">2019</xref>; Young, Rode&#x02010;Margono, &#x00026; Amin, <xref rid="ece35767-bib-0054" ref-type="ref">2018</xref>). Such camera trap software systems should, of course, include user interface features that encourage efficient human inspection of images and encoding of its data attributes. Those interface features should be based on a firm understanding of what analysts do when examining and encoding images, including mitigating human performance bottlenecks. However, most descriptions of these systems often provide only sparse details and discussions (if any) of the problems faced by analysts that the system purportedly solves, the benefits of the particular interface features provided, and how the particular solution offered can be generalized to other systems. Thus, there is a gap in how the lessons learnt from developing such systems could be applied to evaluate current software and/or developing next generation camera trap software interfaces. As a recent extensive World Wildlife Fund report on best practices for camera&#x02010;trapping summarizes:<disp-quote content-type="quotation" id="ece35767-blkfxd-0002"><p>Importantly, no single software package has emerged as a favourite amongst camera trappers, and lots of very different solutions to the problem of camera trap data management are currently being trialled&#x02026; For any given camera trap project&#x02026;, this makes it difficult to decide which software package to commit to. Many large camera trap projects &#x02026; have ended up designing their own systems from scratch. (p. 146)</p><p>The various software options available differ greatly in their approaches &#x02026;and you may need to test various options before deciding which one satisfies your requirements and most efficiently fits into your workflow (p. 150). <named-content content-type="attribution">(Wearn and Glover&#x02010;Kapfer, 2017)</named-content>
</p></disp-quote>
</p><p>Our goal in this paper is to describe and explain <italic>user interface design patterns</italic> for software supporting how wildlife biologists perform camera trap image analysis during the tagging step. By way of background, the notion of design patterns was first introduced by architect Christopher Alexander as a documented reusable and proven solution to commonly occurring architectural design problems (Alexander, <xref rid="ece35767-bib-0002" ref-type="ref">1977</xref>). Design patterns are typically derived by examining existing solutions to design problems (which may include &#x0201c;folk&#x0201d; solutions) and generalizing them. Design patterns were later advocated as a way of describing common solutions to typical software engineering problems (Gamma, Helm, Johnson, &#x00026; Vlissides, <xref rid="ece35767-bib-0014" ref-type="ref">1994</xref>) and to human&#x02013;computer interaction problems (Borchers, <xref rid="ece35767-bib-0004" ref-type="ref">2001</xref>). Design patterns are much more than a feature list, for that they provide the rationale behind a feature in a general and reusable manner. Design patterns are usually structured as a name, a problem that explains it, and a design approach to a solution that solves the problem. Importantly, a design pattern is not a finished design. Rather, it is a description or template for how to solve a problem that can be used in many different situations.</p><p>We base our design patterns on both our understanding of what biologists require and the user interface features that support their needs from over 8&#x000a0;years of developing and deploying our freely available Timelapse Image Analysis system<xref ref-type="fn" rid="ece35767-note-1001">1</xref> (Greenberg, <xref rid="ece35767-bib-0018" ref-type="ref">2019</xref>; Greenberg &#x00026; Godin, <xref rid="ece35767-bib-0019" ref-type="ref">2015</xref>, <xref ref-type="fn" rid="ece35767-note-1002">2</xref>). For each design pattern presented, we describe the problem faced by the image analyst, a design approach that solves that problem, and a concrete example of how Timelapse addresses that design pattern.<xref ref-type="fn" rid="ece35767-note-1003">3</xref>
</p><p>While this paper concerns the design of software, we stress that it is highly relevant to wildlife biologists. It is the biologist&#x02014;not programmers&#x02014;that should determine and decide upon what camera trap design features are important to their work. We also recommend that biologists should be part of any camera trap software design team, where they should be the ones motivating what requirements should be included, how requirements should appear in the software, and how software features should be considered in the workflow.</p></sec><sec id="ece35767-sec-0002"><label>2</label><title>THE DIVERSITY OF CAMERA TRAP RESEARCH PROJECTS, GOALS, AND IMAGES</title><p>Camera traps are used to address a wide diversity of ecological research and management objectives and associated taxa. This diversity leads to large differences in how cameras are situated and configured, the kinds of images collected, how analysts would examine those images, the attribute data recorded from images, and how the collected data would be subsequently analyzed. For example, Figure <xref rid="ece35767-fig-0001" ref-type="fig">1</xref> illustrates differences between two images of the same species with varying research objectives. In Figure <xref rid="ece35767-fig-0001" ref-type="fig">1</xref>a, the camera was set to motion&#x02010;triggering to capture close&#x02010;up views of mountain goats (<italic>Oreamnos americanus</italic>) as they passed by. Attributes of the goat could then be analyzed (e.g., species identification, sex, estimated age, individual identification, etc.). In Figure <xref rid="ece35767-fig-0001" ref-type="fig">1</xref>b, the camera was set to Timelapse mode that took an image every 5&#x000a0;min in order to monitor the presence and activity of a herd of goats over time on a distant pasture and mountain&#x02010;side. Attributes of herd activity could then be analyzed (e.g., counts, duration in the meadow, ratio of kids to adults, etc.). Figure <xref rid="ece35767-fig-0001" ref-type="fig">1</xref>b includes a small herd in the green pasture, each goat just visible as white dots.</p><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>The diversity of images produced by two camera traps. (a) A close&#x02010;up shot of a mountain goat. (b) Mountain goats are barely visible in the meadow as white dots.</p></caption><graphic id="nlm-graphic-3" xlink:href="ECE3-9-13706-g001"/></fig><p>Perhaps, the most familiar uses of camera traps and the subsequence analysis of its images are in some form of in situ wildlife monitoring (e.g., Burton et al., <xref rid="ece35767-bib-0006" ref-type="ref">2015</xref>; Steenweg et al., <xref rid="ece35767-bib-0043" ref-type="ref">2017</xref>). For example, in a review of 266 camera trap publications over a 6&#x02010;year period, Burton et al. counted a range of ecological objectives and responses metrics including: relative abundance (43.6%), presence&#x02013;absence (41.4%), behavior such as activity patterns and diet (32.3%), population density (15.8%), and occupancy (15.4%) (Burton et al., <xref rid="ece35767-bib-0006" ref-type="ref">2015</xref>, p. 678). Other examples illustrate further diversity of objectives:
<list list-type="bullet" id="ece35767-list-0001"><list-item><p>monitor species diversity and inventories (e.g., Ahumada, Hurtado, &#x00026; Lizcano, <xref rid="ece35767-bib-0001" ref-type="ref">2013</xref>; Glover&#x02010;Kapfer et al., <xref rid="ece35767-bib-0016" ref-type="ref">2019</xref>; O'Brien &#x00026; Kinnaird, <xref rid="ece35767-bib-0032" ref-type="ref">2010</xref>),</p></list-item><list-item><p>measure population abundance, density, and distribution of marked and unmarked populations (e.g., Goswami, Madhusudan, &#x00026; Ullas Karanth, <xref rid="ece35767-bib-0017" ref-type="ref">2007</xref>; Heilbrun, Silvy, Peterson, &#x00026; Tewes, <xref rid="ece35767-bib-0020" ref-type="ref">2006</xref>; Karanth &#x00026; Nichols, <xref rid="ece35767-bib-0024" ref-type="ref">1998</xref>; Rowcliffe &#x00026; Carbone, <xref rid="ece35767-bib-0037" ref-type="ref">2008</xref>; Royle, Fuller, &#x00026; Sutherland, <xref rid="ece35767-bib-0038" ref-type="ref">2018</xref>; Whittington, Low, &#x00026; Hunt, <xref rid="ece35767-bib-0052" ref-type="ref">2019</xref>; Whittington, Hebblewhite, &#x00026; Chandler, <xref rid="ece35767-bib-0051" ref-type="ref">2018</xref>)</p></list-item><list-item><p>examine multi&#x02010;species dynamics (Swanson et al., <xref rid="ece35767-bib-0046" ref-type="ref">2015</xref>),</p></list-item><list-item><p>estimate population trends (e.g., Karanth, Nichols, Samba Kumar, &#x00026; Hines, <xref rid="ece35767-bib-0025" ref-type="ref">2006</xref>),</p></list-item><list-item><p>correlating wildlife abundances to anthropological stressors (e.g., Fisher &#x00026; Burton, <xref rid="ece35767-bib-0013" ref-type="ref">2018</xref>), including human activity, and to mitigation efforts (e.g., Pollock, Nielsen, &#x00026; St. Clair, <xref rid="ece35767-bib-0034" ref-type="ref">2017</xref>; Whittington et al., <xref rid="ece35767-bib-0052" ref-type="ref">2019</xref>),</p></list-item><list-item><p>quantify animal behavior and success rates in DNA hair capture sites (Clevenger &#x00026; Sawaya, <xref rid="ece35767-bib-0010" ref-type="ref">2010</xref>),</p></list-item><list-item><p>linking seasonal plant phenology to climate change and wildlife distributions (e.g., Laskin et al., <xref rid="ece35767-bib-0027" ref-type="ref">2019</xref>; Mills et al., <xref rid="ece35767-bib-0029" ref-type="ref">2018</xref>), and</p></list-item><list-item><p>quantifying how seasonal changes in coat color are influenced by climate change (e.g., Mills et al., <xref rid="ece35767-bib-0029" ref-type="ref">2018</xref>).</p></list-item></list>
</p><p>Camera traps can also help answer very specific research questions. For example, Garc&#x000ed;a&#x02010;Salgado et al. (<xref rid="ece35767-bib-0015" ref-type="ref">2015</xref>) analyzed the diet of nesting raptors by examining images for prey deliveries to nests. Rollack, Wiebe, Stoffel, and Houston (<xref rid="ece35767-bib-0036" ref-type="ref">2013</xref>) similarly deployed cameras around nests, but in this case to study the breeding behaviors of turkey vultures. Jumeau, Petrod, and Handrich (<xref rid="ece35767-bib-0023" ref-type="ref">2017</xref>) used camera traps to estimate the effectiveness of wildlife crossing structures for small mammals.</p><p>Camera traps are also used to monitor and analyze human activity. Examples include: counting the number of anglers actively fishing in order to estimate angling effort in small lakes fisheries (Greenberg &#x00026; Godin, <xref rid="ece35767-bib-0019" ref-type="ref">2015</xref>); evaluating the influence of human disturbance on wildlife (e.g., Blake, Mosquera, Loiselle, Romo, &#x00026; Swing, <xref rid="ece35767-bib-0003" ref-type="ref">2017</xref>; Oberoslerab, Groff, Lemma, Pedrini, &#x00026; Rovero, <xref rid="ece35767-bib-0033" ref-type="ref">2017</xref>), quantifying levels of human use and type of human activities (e.g., Campbell, <xref rid="ece35767-bib-0007" ref-type="ref">2010</xref>; Fairfax, MacKenzie Dowling, &#x00026; Neldner, <xref rid="ece35767-bib-0012" ref-type="ref">2014</xref>); and detecting types and occurrences of illegal activity in wildlife sanctuaries including identifying perpetrators (e.g., Hossain et al., <xref rid="ece35767-bib-0021" ref-type="ref">2016</xref>).</p><p>The above is just a very small sampling to indicate the diverse use of camera traps (e.g., see Section 5 in Steenweg et al., <xref rid="ece35767-bib-0043" ref-type="ref">2017</xref>; Wearn &#x00026; Glover&#x02010;Kapfer, <xref rid="ece35767-bib-0050" ref-type="ref">2017</xref>). While there may be some overlap in the kinds of images gathered across projects, we can easily expect differences between projects in: the kinds of images that are captured, the data attributes that would be encoded from these images, and the ways analysts would have to examine those images to extract and record that data. Our design patterns reflect this diversity of project objectives. Some design patterns are broadly applicable and useful in all kinds of image classification, while others may be of use in narrower suites of projects. More generally, software based on these design patterns will include tools for examining images, will provide a flexible user interface for efficiently encoding project&#x02010;specific attributes, and will simplify data entry by automatically extracting available image features as data. Collectively, it can substantially increase project efficiency, increase data quality and improve the user experience.</p></sec><sec id="ece35767-sec-0003"><label>3</label><title>METHODOLOGY: DECONSTRUCTING THE DESIGN OF TIMELAPSE INTO DESIGN PATTERNS</title><p>Our methodology for understanding how analysts classify data and what user interface design patterns would be useful was derived from deconstructing our real&#x02010;world experiences designing, implementing, and refining the Timelapse open software system. Timelapse was specifically developed to help analysts inspect and classify camera trap images via tagging. Timelapse is conceptually simple: it displays images to analysts, along with a variety of project&#x02010;specific (and user generated) attribute fields that they can efficiently fill in to describe the image. Yet its design goes far beyond that, for it, includes many features addressing the subtleties of the analyst's workflow.</p><p>Timelapse evolved through many versions over its 8&#x000a0;years. Its capabilities were designed to meet a broad variety of ecological needs as requested from a diverse international user community comprising different agencies (government, industry, university, and independents) and biologists (researchers, practitioners, and students). Most had differing projects and goals (e.g., wildlife monitoring, angling effort in fisheries, environmental monitoring, human monitoring, etc.). Our requirements, analysis, and various redesigns of Timelapse were further informed by the following.
<list list-type="bullet" id="ece35767-list-0002"><list-item><p>We had ongoing discussions with both project managers and analysts about their camera trap image analysis needs and existing workflow.</p></list-item><list-item><p>We held observational studies of analysts using Timelapse as they did their work (e.g., Greenberg &#x00026; Godin, <xref rid="ece35767-bib-0019" ref-type="ref">2015</xref>), using standard techniques in the field of human&#x02013;computer interaction (e.g., Shneiderman et al., <xref rid="ece35767-bib-0041" ref-type="ref">2016</xref>). We observed and interviewed the technicians analyzing images, paying particular attention to their workflow, problems encountered, and bottlenecks;</p></list-item><list-item><p>We collected feedback from analysts who had used Timelapse to inspect millions of images (e.g., problems, feature requests, bottlenecks).</p></list-item></list>
</p><p>We emphasize that our methodology followed an iterative versus one&#x02010;time design process. We began with understanding the requirements of a single small&#x02010;lakes fisheries agency (Greenberg &#x00026; Godin, <xref rid="ece35767-bib-0019" ref-type="ref">2015</xref>). We repeated our methodology as other agencies from different domains and with different project needs came on board. Our ongoing discussions, observations, and feedback with those agencies helped us understand and articulate the subtleties and variations in the workflow that arose due to various factors. For example, the type of images captured at particular sites differed considerably, which often led to workflow alterations in how technicians analyzed images. As well, particular subprojects required analysts to examine and encode different image features as data, again resulting in workflow differences. We used this knowledge to update the Timelapse design, albeit with the constraint that it had to remain a tool usable by its broad community. That is, new interface features would be added only if they were potentially valuable to a broad range of projects and users, or&#x02014;at the very least&#x02014;could be ignored if they were not needed.</p><p>The remainder of this paper deconstructs and generalizes as design patterns the key workflow tasks and problems seen, and how they informed the corresponding Timelapse interface design solutions. Each section is organized around an issue that relates particular problems faced by analysts. Each problem is followed by a named design pattern. This includes a general description of an interface design solution that mitigates that problem and a concrete example of how Timelapse realized that design pattern. We also invite the reader to run Timelapse as they read the design patterns, as this can help them better understand the nuances of the proposed solution.</p><p>While we highlight how Timelapse instantiates a particular design pattern solution, we stress that the design pattern is more general than that, as it can also inform alternate software designs. For example, designers can selectively incorporate particular design solutions as seen in Timelapse into their own systems. Alternately, designers can use our design pattern problem descriptions and general solutions to create their own novel, alternate ways to solve those problems. Finally, project managers can match design patterns against their project needs, and then evaluate competing software solutions to see whether those solutions have interface features that support the design patterns relevant to their project.</p><p>We recognize that other camera trap software systems may offer similar or alternate solutions to our design patterns. However, we restrict our example solutions to Timelapse, as a comparative review and analysis are beyond the scope of this paper. Still, the design patterns supplied here should allow readers to reconsider whether and how other software solutions address particular design problems and may even help them identify problems and design patterns that are outside of what we provide below.</p></sec><sec id="ece35767-sec-0004"><label>4</label><title>ISSUE: DATA CONSISTENCY</title><p>The ultimate goal of the analyst is to enter attribute data that reflect the contents of the images. Statistical analysis of attribute data collected from those images typically occurs later as a separate step. The value of attribute data depends upon its consistency (explained below) and quality. Thus, data entry protocols for what attributes to collect must be developed prior to image classification.</p><sec id="ece35767-sec-0005"><label>4.1</label><title>Problem&#x02014;The data required and how they are named may be inconsistent between analysts</title><p>Projects typically involve multiple cameras located at multiple stations at one or more study sites. In turn, this can generate a large number of distinct <italic>image sets</italic>,<xref ref-type="fn" rid="ece35767-note-1004">4</xref> each containing many thousands of images. Multiple analysts may be involved (perhaps including volunteer citizen scientists), each analyzing the images within a particular image set. A key issue is maintaining data consistency across different image sets and different analysts, that is, where all analysts are inputting consistently formatted data into a set of consistently named data fields. Without data consistency (e.g., if each analyst idiosyncratically specified what data should be encoded from images, in what format, and under what name), it would be extremely difficult to make sense of the data across analysts and image sets.</p><sec id="ece35767-sec-0006"><label>4.1.1</label><title>Design pattern: Specify and deploy a common data schema</title><p>The project manager should initially decide what data should be collected from the image sets and communicate those needs to the analysts as a standardized computer&#x02010;readable <italic>schema</italic> that defines and specifies the data of interest. Analysis of image sets by all analysts should then be based on that schema. The image analysis software should enforce the schema, where deviations from that schema are discouraged unless absolutely necessary. The schema should define the required data fields, how they are named, their data type, their format, and constraints on what values they allow.</p></sec><sec id="ece35767-sec-0007"><label>4.1.2</label><title>Timelapse example</title><p>Project managers use a Timelapse utility called the <italic>template editor</italic> (Figure <xref rid="ece35767-fig-0002" ref-type="fig">2</xref>) to construct a template file that defines the data schema. Analysts place the data template file in the folder containing the image set. When the actual Timelapse application is opened (Figure <xref rid="ece35767-fig-0003" ref-type="fig">3</xref>), it uses the template schema to build the user interface and to construct the database table that will ultimately contain the data entered by the analyst. This enforces the data schema.</p><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>The Template Editor. The project manager defines the attribute data of interest (the schema) as well as the associated user interface specifications by form&#x02010;filling (top pane). The middle pane generates a preview of the user interface data entry controls that will be seen by the analyst. The bottom pane shows how the data of interest will be stored as columns within a database table. Interface controls and spreadsheet columns can be re&#x02010;ordered by dragging them to the desired location</p></caption><graphic id="nlm-graphic-5" xlink:href="ECE3-9-13706-g002"/></fig><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Timelapse Image Analyzer. The analyst uses Timelapse to navigate and inspect images (lower panel) and to enter data describing an image's features of interest (top panel). The data entry controls shown in the top panel were automatcially constructed from the schema information specified by the project manager through the template editor illustrated in Figure <xref rid="ece35767-fig-0001" ref-type="fig">1</xref>
</p></caption><graphic id="nlm-graphic-7" xlink:href="ECE3-9-13706-g003"/></fig><p>Figure <xref rid="ece35767-fig-0002" ref-type="fig">2</xref> illustrates a screenshot of the template editor in action. In this example, the project manager has specified a fairly simple schema used to count how many goats and hikers are seemed and to track environmental conditions and a few other attributes.<xref ref-type="fn" rid="ece35767-note-1005">5</xref> Generally, each row in the form specifies all attributes of a particular data field, while each column names the attribute. The template editor allows project managers to easily create project&#x02010;specific schemas. Each attribute field of the schema is developed with the following options:
<list list-type="bullet" id="ece35767-list-0003"><list-item><p>
<italic>Type </italic>indicates the data type and its format. For example, the type <italic>DateTime</italic> and <italic>UTCOffset</italic> follow the international standard for specifying dates. <italic>Counters</italic> are positive integers and are usually used for counting entities in an image. <italic>Flags</italic> can only contain true/false values. <italic>Notes</italic> are free&#x02010;form text fields. <italic>Choices</italic> are constrained to a limited number of possible values provided by the project manager. For example, a field labeled &#x0201c;Weather&#x0201d; could be constrained to the values &#x0201c;Sunny,&#x0201d; &#x0201c;Cloudy,&#x0201d; &#x0201c;Foggy&#x0201d; etc., while a field named &#x0201c;Species&#x0201d; could list&#x02014;and eventually allow an analyst to select from&#x02014;all possible species of interest in that ecology.</p></list-item><list-item><p>
<italic>Data Label </italic>names the data field containing that data, that is, the column name as it would appear in a database table or a spreadsheet.</p></list-item><list-item><p>
<italic>Default Value</italic> indicates the initial value of that data, which will be applied to every image seen by Timelapse.</p></list-item><list-item><p>
<italic>List </italic>defines the allowable entries for Choice data types. Selecting &#x0201c;Define list&#x0201d; raises a small window where the manager can type in those entries (see Figure <xref rid="ece35767-fig-0002" ref-type="fig">2</xref>, lower right side).</p></list-item></list>
</p></sec></sec><sec id="ece35767-sec-0008"><label>4.2</label><title>Problem&#x02014;Data schema terminology may not be in the analyst's language</title><p>Computer systems often ask users to enter data by either filling in rows in a table, or via data entry widgets (textboxes for text entry, menus for selecting choices, etc.). These are often labeled in some manner (e.g., column names in a table or a name associated with the data entry widget). Labels are important, as analysts need to understand what data they are expected to enter. The issue is that the terminology used may not be in the analyst's language. For example, systems may use the database field name to label a data entry widget, but these names may not be understandable (e.g., acronyms, abbreviations, technical terms, ambiguous meanings, etc.) or insufficient to describe what the analyst should enter. This can lead to inconsistencies between the project manager's expectations of the data required versus the analysts' interpretation of what data should be entered. A related issue is that the schema may include fields that are of little interest to the analyzer, such as fields they are not expected to review or fill in. Their inclusion in the analyst's user interface would add clutter and perhaps confusion, or have the analyst fill in fields unnecessarily. Finally, because the project manager and all analysts need to communicate to one another, the terminology used to identify that data should be common. These terminology problems are particularly endemic to analysts who inspect images only occasionally, as they may forget what particular terms mean, and to citizen scientists with minimal training.</p><sec id="ece35767-sec-0009"><label>4.2.1</label><title>Design pattern: Present the data schema in the analyst's language</title><p>The data entry interface presented to the analyst should be expressed clearly in familiar words, phrases, concepts, and explanations rather than in system&#x02010;oriented terms. Nonrelevant data items should be removed (Nielsen, <xref rid="ece35767-bib-0030" ref-type="ref">1993</xref>, Chapter 5). One way to do this is to associate the terms in the data schema with a more understandable terminology and descriptions of what data they are expected to enter, and whether or not particular data items should be displayed to the analyst. While the interface seen by the analyst would be constructed using that terminology, the data entered by the analyst would be stored under the data labels provided by the data schema, thereby maintaining data consistency.</p></sec><sec id="ece35767-sec-0010"><label>4.2.2</label><title>Timelapse example</title><p>Using the Timelapse <italic>Template Editor</italic>, the project manager specifies the terminology of data entry controls, that is, the interface controls associated with every data field used by the analyst to enter data. For example, consider the data field with the data label of &#x0201c;GoatCnt&#x0201d; in Figure <xref rid="ece35767-fig-0002" ref-type="fig">2</xref>. In the <italic>Label</italic> column, the manager specified &#x0201c;Goats&#x0201d; as a more human&#x02010;readable alternative to that data field. In the <italic>Tooltip</italic> column, the manager provided a brief help explanation of what should be entered: &#x0201c;A count of how many goats appear in this image.&#x0201d; By unchecking the &#x0201c;Visible&#x0201d; checkbox, the manager has indicated that the &#x0201c;UtcOffset&#x0201d; data field should not be displayed to the analyst. As the manager performs these actions, a live preview of the user interface (Figure <xref rid="ece35767-fig-0002" ref-type="fig">2</xref>, middle pane) reflects the actual user interface that will be seen by the analyst when using the Timelapse system (Figure <xref rid="ece35767-fig-0003" ref-type="fig">3</xref>, top pane). Both illustrate how the data controls adopt the analyst&#x02010;oriented terminology and explanations specified in the template (e.g., the &#x0201c;Goats&#x0201d; control and the displayed tooltip). When the analyst enters data in that control (Figure <xref rid="ece35767-fig-0003" ref-type="fig">3</xref>), it is stored in the corresponding data field (e.g., &#x0201c;GoatCnt&#x0201d;).</p></sec></sec><sec id="ece35767-sec-0011"><label>4.3</label><title>Issue: Data input errors are commonplace</title><p>When analysts enter data, they may inadvertently introduce errors into the stored data. For example, the entered data may be outside of what is expected (e.g., nonnumeric characters entered into an integer data field, Yes/No instead of True/False as expected by a boolean data field). As another example, the entered data may not be in the correct format (e.g., date may be incorrectly entered as &#x0201c;mm/dd/yyyy&#x0201d; order instead of the expected &#x0201c;dd//mm/yyyy&#x0201d;).</p><sec id="ece35767-sec-0012"><label>4.3.1</label><title>Design pattern: Data entry controls should minimize input errors by constraining input to the data field's type</title><p>Input controls should provide visual hints of what input is accepted, and should only accept input that matches the data field's type and format. This pattern is now common in many modern user interfaces, where myriads of input controls suggest and constrain what can be entered into them.</p></sec><sec id="ece35767-sec-0013"><label>4.3.2</label><title>Timelapse example</title><p>Timelapse generates its user interface from the description provided in the template editor (Figure <xref rid="ece35767-fig-0002" ref-type="fig">2</xref>), where each data entry control is based on its corresponding template specification (Figure <xref rid="ece35767-fig-0003" ref-type="fig">3</xref>, top pane). The data entry control constrains the input, where only valid data can be entered into it. For example, an analyst interacting with the DateTime data entry control (Figure <xref rid="ece35767-fig-0003" ref-type="fig">3</xref>, top pane, right side) can only enter or edit valid date and time values. Counter controls (e.g., Goats, Hikers) only accept positive integers (either by typing or by mouse interaction). Choice data controls (e.g., Image Quality, Weather) restrict the analyst to selecting from a pop&#x02010;up menu containing only valid text entries. Flags (e.g., Delete?, Publicity?) present themselves as a checkbox, where the control itself translates its &#x0201c;checked&#x0201d; state to the two allowable data storage values of &#x0201c;true&#x0201d; or &#x0201c;false.&#x0201d; In all cases, the analyst's data entry is constrained by the corresponding data control to allow only legal values, which reduces possible input errors significantly.</p></sec></sec></sec><sec id="ece35767-sec-0014"><label>5</label><title>ISSUE: INSPECTING IMAGE FEATURES</title><p>A major part of the analyst's tasks is to inspect the image to discover (and record as data) features of interest. Yet inspection can be problematic for some images, especially when the features of interest are not discernable at a glance.</p><sec id="ece35767-sec-0015"><label>5.1</label><title>Problem&#x02014;Relevant image details may be difficult to examine</title><p>Depending on what is captured in the image's field of view, analysts may have to inspect small image details in order to classify what is there. One example is camera images that capture large fields of view, where the items of interest are very small. Figures <xref rid="ece35767-fig-0001" ref-type="fig">1</xref>b, <xref rid="ece35767-fig-0003" ref-type="fig">3</xref> and <xref rid="ece35767-fig-0004" ref-type="fig">4</xref>a illustrate such an image, where the camera is oriented to capture distant goats as they wander through a mountain&#x02010;side and meadow. Other examples include cases where the analyst has to identify small animals, or details of that animal (e.g., sex, health), or where the image only displays a portion of that animal (e.g., because of occlusion by vegetation, or because the animal is only partly in the camera's field of view).</p><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Various approaches to examining details via magnification. (a) The unaltered view. (b) Pan and zoom. (c) Standard Magnifying lens. (d) Offset magnifying. (e) Fisheye lens.</p></caption><graphic id="nlm-graphic-9" xlink:href="ECE3-9-13706-g004"/></fig><sec id="ece35767-sec-0016"><label>5.1.1</label><title>Design pattern: Allow the analyst to examine image details through magnification</title><p>Image magnification can help the analyst examine small image features. However, due to the number of images inspected, the analyst's interaction with the provided magnification technique must be efficient to use. Various magnification interaction techniques are known. The most common is perhaps a <italic>pan and zoom</italic> facility (Figure <xref rid="ece35767-fig-0004" ref-type="fig">4</xref>b), which allows the analyst to zoom (magnify) a particular image region (called the <italic>focus</italic>). While powerful, zooming into local detail incurs the cost of losing <italic>global context</italic>, that is, only the zoomed in portion of the image is visible, which means informative details outside of the focus cannot be seen at the same time.</p><p>Another common approach mimics a <italic>magnifying lens</italic> (see Figure <xref rid="ece35767-fig-0004" ref-type="fig">4</xref>c), where the area under the cursor (the focus point) is overlaid by a small zoomed in region. The area being magnified is immediately updated as the cursor is moved. Again, there is a trade&#x02010;off: while the magnified areas show the focus area under the cursor, it occludes some of the surrounding global context as it overlays it. For example, Figure <xref rid="ece35767-fig-0004" ref-type="fig">4</xref>c magnifies 3 of the goats, but at the cost of occluding the other two nearby goats.</p><p>Far more sophisticated magnification approaches have been developed in the field of <italic>information visualization</italic> (e.g., Shneiderman et al., <xref rid="ece35767-bib-0041" ref-type="ref">2016</xref>; Spence, <xref rid="ece35767-bib-0042" ref-type="ref">2014</xref> Ch. 12). For example, an <italic>offset magnifying lens</italic> avoids occlusion by offsetting the magnified area away from the cursor's focus area (Ware &#x00026; Lewis, <xref rid="ece35767-bib-0049" ref-type="ref">1995</xref>). As Figure <xref rid="ece35767-fig-0004" ref-type="fig">4</xref>d illustrates, the small square by the cursor is the area to magnify, where that zoomed in area is shown offset in the larger square. Thus, the analyst can simultaneously see both the unzoomed and zoomed in area at the same time. Another approach is <italic>focus plus context magnification</italic>. For example, a <italic>fisheye lens</italic> distorts an image to provide magnification in place (Carpendale, Light, &#x00026; Pattison, <xref rid="ece35767-bib-0008" ref-type="ref">2004</xref>). As Figure <xref rid="ece35767-fig-0004" ref-type="fig">4</xref>e illustrates, the highest magnification is focused under the cursor, where a drop&#x02010;off function applies progressively less magnification away from the cursor. The advantage, as seen in Figure <xref rid="ece35767-fig-0004" ref-type="fig">4</xref>e, is that local detail is shown in place within the global context. This also avoids separation of the magnified versus unmagnified image as evident in Figure <xref rid="ece35767-fig-0004" ref-type="fig">4</xref>b,c.</p><p>Of course, the value of magnification is affected by image fidelity. Some systems may, for example, reduce an image's display resolution for performance purposes with the side effect of comprising image details. The fewer the pixels read in, the less memory required and the faster images can be displayed. The trade&#x02010;off is that magnification would then produce &#x0201c;fat pixels&#x0201d; (<italic>aka</italic> blurry images) rather than details. Fortunately, various image processing techniques are known that can efficiently display the whole image at low fidelity, while reading in high resolution details for only the magnified regions (Carpendale et al., <xref rid="ece35767-bib-0008" ref-type="ref">2004</xref>).</p></sec><sec id="ece35767-sec-0017"><label>5.1.2</label><title>Timelapse example</title><p>Timelapse contains several methods for rapidly examining image details through magnification.
<list list-type="order" id="ece35767-list-0004"><list-item><p>
<bold>Zoom and pan.</bold> The analyst can zoom and pan into any part of the image using the scroll wheel and the mouse. Zooming occurs at the cursor location, while panning to a particular image region is done by dragging the image with the mouse. Timelapse's zooming and panning features also include nuances that support how an analyst would use it over multiple images. First, the analyst can &#x0201c;bookmark&#x0201d; a particular zoomed in area, where she can flip between the zoomed and normal image with a single keypress, thus maintaining some sense of how the zoomed&#x02010;in focus relates to the global context. Bookmarks have other advantages. For example, if the analyst was interested in animal activity in the pasture seen in Figure <xref rid="ece35767-fig-0003" ref-type="fig">3</xref>, she could zoom into that pasture and bookmark it. When checking other images for activity in that pasture, she can use that bookmark to zoom into the same corresponding area. Second, zoom/pan levels are maintained when navigating between images. For example, the analyst could zoom into the pasture of Figure <xref rid="ece35767-fig-0003" ref-type="fig">3</xref> and then see how the goats have moved around that pasture by navigating to the next few images.</p></list-item><list-item><p>
<bold>Offset magnifying lens.</bold> Similar to Figure <xref rid="ece35767-fig-0004" ref-type="fig">4</xref>d but with different visuals, the analyst can turn on a magnifier that displays zoomed&#x02010;in image of the area around the cursor: the lens is offset to avoid occluding that area. Figure <xref rid="ece35767-fig-0003" ref-type="fig">3</xref> illustrates the Timelapse magnifier in action, where the analyst is using it to detect and examine a herd of goats. The analyzer can easily scan the image details for features of interest by dragging the magnifier, whose magnified content is instantly updated. The analyst can also quickly adjust the amount of magnification through a few keystrokes.</p></list-item></list>
</p></sec></sec><sec id="ece35767-sec-0018"><label>5.2</label><title>Problem&#x02014;The presence of small entities may be difficult to notice</title><p>Various projects use cameras in Timelapse mode, where periodically taken images capture a very wide field of view. We already saw how Figures <xref rid="ece35767-fig-0001" ref-type="fig">1</xref>b and <xref rid="ece35767-fig-0003" ref-type="fig">3</xref> illustrate one actual example, where the camera was located to capture distant goat activity in a pasture and on a mountain side. Another real example includes cameras positioned to capture distant anglers on and around a large lake area (Greenberg &#x00026; Godin, <xref rid="ece35767-bib-0019" ref-type="ref">2015</xref>). The issue is that analysts may not notice the presence of these small entities. This becomes more problematic when a run of images being examined have nothing in them, as analysts expect that pattern to continue. Magnification, while somewhat helpful, is best used to examine details <italic>after</italic> an entity has been noticed.</p><sec id="ece35767-sec-0019"><label>5.2.1</label><title>Design pattern: Enhance the noticeability of small entities within images</title><p>Various techniques can enhance how the analyst can notice small entities in a scene by making them visually distinctive.</p><p>
<italic>Animation of image sequences</italic> visually highlights changes that occur by rapidly switching between images. Because the background scene is reasonably constant, the appearance, disappearance, and movements of entities within the scene are often very noticeable.</p><p>
<italic>Image processing through image differencing</italic> compares, pixel by pixel, the current image against the previous and/or next image. A new image is generated from that comparison, where (for example) a white pixel is drawn if the compared pixels differ significantly in brightness and color (set by threshold values), and black otherwise. The resulting image visually highlights the differences in white, while removing the somewhat static background. Because entities appear, disappear, and move around a scene, the differenced image will display that entity as a white blob (usually in the shape of the desired entity) against a black background. Other image processing techniques may also help, such as motion tracking that track the position of an object over subsequent frames. We note that the effectiveness of image processing techniques can be compromised when large visual differences occur between the images, such as dramatic changes in image lighting, motion of nearby grass and branches affected by the wind, and even slight changes of a camera's position (e.g., due to wind effects on the tree, it is mounted on). As well, image differencing will not work for the few cases where the animal is completely still.</p><p>
<italic>Image enhancement</italic>. Many off&#x02010;the&#x02010;shelf photo viewing systems now include various ways to adjust an image. Examples include contrast adjustment, saturation and luminance of particular colors, dehazing, sharpening, edge detection, etc. In the image analysis context, an analyst could apply various adjustments on a test image and&#x02014;if effective at enhancing an entity's visibility&#x02014;have that setting automatically applied when viewing other images.</p></sec><sec id="ece35767-sec-0020"><label>5.2.2</label><title>Timelapse example</title><p>Timelapse contains the first two methods above for enhancing the noticeability of small entities, both based on analyzing the differences between images. Timelapse does not include other image enhancement methods, but they could be added easily.
<list list-type="order" id="ece35767-list-0005"><list-item><p>
<bold>Animation through rapid image switching.</bold> Timelapse lets the analyst rapidly switch between the current image of interest and the next or previous image (using the arrow keys), where images are displayed immediately and without flicker. The analyst perceives this as an animation, where the differences between images&#x02014;such as a small moving animal&#x02014;&#x0201c;pop out.&#x0201d; Furthermore, the magnifier and zoom/pan level are maintained at their current setting and position during image switching, which helps the analyst spot differences in a magnified region.</p></list-item><list-item><p>
<bold>Image processing through image differencing</bold> compares the original image to the previous image, the next image, or to both. The analyst toggles between the differenced and original image with a single key press. When blobs of interest appear, the analyst can use the magnifying glass (which displays that region in its original form) to investigate. Alternately, the analyst can use rapid image switching to see whether the blob has moved. Figure <xref rid="ece35767-fig-0005" ref-type="fig">5</xref> provides an example of how this appears in practice. Figure <xref rid="ece35767-fig-0005" ref-type="fig">5</xref>a displays the normal image: The small goat can be easily missed or mistaken for a rock. Figure <xref rid="ece35767-fig-0005" ref-type="fig">5</xref>b is the differenced image: not only is the goat highlighted as a blob, but another goat partly hidden in the trees on its left is revealed. Figure <xref rid="ece35767-fig-0005" ref-type="fig">5</xref>b also shows the analyst furthering inspecting a blob via the magnifier, which displays the goat as it appears in the original image.</p></list-item></list>
</p><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Image differencing. The analyst flips between the normal and differenced view of the image. (a) Normal image (cropped). Several goats are in the lower right corner, but the analyst may easily miss them. (b) Differenced image (same cropped region). The analyst investigates the white blobs with the magnifying glass, and sees that they are goats.</p></caption><graphic id="nlm-graphic-11" xlink:href="ECE3-9-13706-g005"/></fig></sec></sec><sec id="ece35767-sec-0021"><label>5.3</label><title>Problem&#x02014;Entities within images may be difficult to see due to poor image fidelity</title><p>Because cameras are positioned in the field, the quality of the images produced can be compromised by many factors. Weather is one factor, where fog, rain, and snow can limit what is visible, especially at a distance. Lighting is another fact, such as sharp shadows mixed with bright sunshine, or failing light due to dusk and night&#x02010;time shots. The camera itself can be compromised, such as by moisture on the lens, or by focus problems.</p><sec id="ece35767-sec-0022"><label>5.3.1</label><title>Design pattern: Enhance images whose fidelity is compromised</title><p>Various image processing techniques can enhance the clarity of compromised images, albeit with limits. Indeed, the previously described techniques used to enhance the noticeability of small entities could perhaps help here: contrast adjustment, color correction including saturation and luminance, sharpening, edge detection, etc. Dehazing will likely be of particular value in mitigating fog effects. As before, an analyst could apply various adjustments on one image and have that setting automatically applied to other similarly compromised images.</p></sec><sec id="ece35767-sec-0023"><label>5.3.2</label><title>Timelapse example</title><p>Timelapse does not yet include these image processing capabilities. Currently, the analyst would have to correct the image outside of Timelapse (e.g., using the many tools available in photograph editors such as Adobe Photoshop or Adobe Lightroom). The modified saved image would then be visible within Timelapse.</p></sec></sec></sec><sec id="ece35767-sec-0024"><label>6</label><title>ISSUE: NAVIGATING IMAGES</title><p>Analysts are often tasked with inspecting tens and even hundreds of thousands of images in an image set. Thus reviewing, searching, and navigating between images should be rapid.</p><sec id="ece35767-sec-0025"><label>6.1</label><title>Problem&#x02014;Tedious image navigation and review</title><p>An analyst may want to rapidly navigate and review a sequence of images for various reasons. She may want to scan all images quickly before coding them, in order to get a sense of what is in them. She may want to quickly move over &#x0201c;empty images&#x0201d; (e.g., scenes with no wildlife in it) until she spots an image containing something of interest. She may want to visually search the image set for a particular scene, for example, an image with wolves and cubs. She may also want to search for a particular image by its file name. The problem is that image packages often differ considerably in how they support navigation, where some navigational methods can interfere with the analyst's task. For example, image review would be severely impeded if each image has to be separately opened in its own window.</p><sec id="ece35767-sec-0026"><label>6.1.1</label><title>Design pattern: Provide tools that allow rapid navigation and review of images</title><p>Analyst often examines images sequentially to see how they unfold over time. Stepping forward and backward through them should be visually instantaneous and should require minimal effort, for example, via a single key press or mouse click. Because image sets can number in the hundreds of thousands, analysts should be able to move, scrub, and jump through images quickly, similar to how one can scrub through a video. To help the analyst visually review and compare images during navigation (such as to detect changes as discussed in the previous design pattern), display settings such as zoom levels and the image location on the screen should be kept constant.</p></sec><sec id="ece35767-sec-0027"><label>6.1.2</label><title>Timelapse example</title><p>Timelapse contains many navigation methods, each allowing rapid image review.
<list list-type="order" id="ece35767-list-0006"><list-item><p>
<bold>Forwards/Backwards controls.</bold> Timelapse lets the analyst rapidly move either backwards or forwards between images via the keyboard (the arrow keys) or by the File Player (described next). Holding down the arrow key scrubs through successive images. Settings on the current image&#x02014;the location of the magnifying glass, zoom and pan levels, image differencing (if any) are all retained during navigation, allowing the analyst to rapidly compare images for similarities and differences as he or she views them.</p></list-item><list-item><p>
<bold>File Player</bold> (seen at the upper right of Figure <xref rid="ece35767-fig-0003" ref-type="fig">3</xref> and annotated in Figure <xref rid="ece35767-fig-0006" ref-type="fig">6</xref>) provides an alternative mouse&#x02010;operated navigational control. Depending on the button pressed, the analyst can step through images, jump to the first or last image, or automatically play (and thus review) successive images at slow and fast speeds. These speeds are user&#x02010;configurable.</p></list-item><list-item><p>
<bold>Navigational Slider</bold> (next to the File Player, see top middle of Figure <xref rid="ece35767-fig-0003" ref-type="fig">3</xref>) allows the analyst to both scrubs through and to rapidly jump across many images. Intervening images are displayed as fast as possible as the analyst moves the slider.</p></list-item><list-item><p>
<bold>The Overview.</bold> Analysts can &#x0201c;zoom out&#x0201d; to see an overview containing multiple images, as illustrated in Figure <xref rid="ece35767-fig-0007" ref-type="fig">7</xref>. The more one zooms out, the more images are displayed, albeit at progressively smaller sizes. The analyst can navigate to a full&#x02010;sized view of a desired image (as in Figure <xref rid="ece35767-fig-0003" ref-type="fig">3</xref>) by clicking its thumbnail in the overview. The behavior of the navigational controls described above is also transformed to work with the overview. For example, the File Player controls now allow the analyst to navigate through successive images one by one, row by row, or page by page. Using the overview, the analyst can navigate and review collections of multiple images quickly.</p></list-item><list-item><p>
<bold>Find Search Bar,</bold> illustrated at the top left of Figure <xref rid="ece35767-fig-0007" ref-type="fig">7</xref>, is somewhat similar to search bars seen in text editors. The analyst uses it to find and display the next file in the image sequence whose file name partially matches the entered text. Figure <xref rid="ece35767-fig-0007" ref-type="fig">7</xref> illustrates a search for any file name containing &#x0201c;05.&#x0201d; Find works in both the single image view (Figure <xref rid="ece35767-fig-0003" ref-type="fig">3</xref>) and in the overview where the found image becomes the first image in the displayed image array (Figure <xref rid="ece35767-fig-0007" ref-type="fig">7</xref>). Find also works on suffixes. For example, if an image set is interspersed with video files, searching on &#x0201c;.avi&#x0201d; will step through all videos.</p></list-item><list-item><p>
<bold>Navigating via the data table.</bold> Analysts have the option of a database view, which displays all the data entered so far as a scrollable table. This is available through the &#x0201c;Data Table&#x0201d; tab as seen in Figure <xref rid="ece35767-fig-0008" ref-type="fig">8</xref>. Each row represents all the data currently associated for an image. The analyst can inspect the rows for data of interest and click on that row to navigate to and view the image associated with that row (akin to the display in Figure <xref rid="ece35767-fig-0003" ref-type="fig">3</xref>).</p></list-item></list>
</p><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>The Timelapse File Player</p></caption><graphic id="nlm-graphic-13" xlink:href="ECE3-9-13706-g006"/></fig><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>The overview showing selection and the Find feature</p></caption><graphic id="nlm-graphic-15" xlink:href="ECE3-9-13706-g007"/></fig><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0008" orientation="portrait" position="float"><label>Figure 8</label><caption><p>The Data Table view</p></caption><graphic id="nlm-graphic-17" xlink:href="ECE3-9-13706-g008"/></fig></sec></sec></sec><sec id="ece35767-sec-0028"><label>7</label><title>ISSUE: ENTERING DATA</title><p>Our example data schema illustrated in Figure <xref rid="ece35767-fig-0002" ref-type="fig">2</xref> and composed as a data entry interface in Figure <xref rid="ece35767-fig-0003" ref-type="fig">3</xref> has relatively few data entry fields. This contrasts with the actual number of data fields that analysts can encounter in practice. For example, one of the agencies using Timelapse composed and regularly used a template defining 30 separate data entry fields that analysts had to fill in. Even if only a subset of those fields relevant to a particular image had to be filled in, data entry can quickly become tedious, error prone, and very time&#x02010;consuming when done over hundreds of thousands of images.</p><sec id="ece35767-sec-0029"><label>7.1</label><title>Problem&#x02014;Typing is time&#x02010;consuming and error&#x02010;prone</title><p>Filling in data fields by typing is tedious. Fields have to be navigated, and typing takes time. Mis&#x02010;typing is common and introduces errors and inconsistencies in the data.</p><sec id="ece35767-sec-0030"><label>7.1.1</label><title>Design pattern: Data entry controls should minimize or eliminate typing when possible</title><p>Selection (via the mouse or via tab/select/enter) should replace typing whenever possible. Since much data entry is repetitious, previously typed&#x02010;in entries should be offered as candidates for selection rather than requiring re&#x02010;entry.</p></sec><sec id="ece35767-sec-0031"><label>7.1.2</label><title>Timelapse example</title><p>Several data controls available through Timelapse (e.g., see top of Figure <xref rid="ece35767-fig-0003" ref-type="fig">3</xref>) favor selection via the mouse or through the keyboard's tab and arrow keys. An analyst selects a Flag's true or false value by clicking on its checkbox. She selects from a Choice's limited possibilities via a pop&#x02010;up menu. She can fill in Counters by clicking its up/down arrow buttons, or by clicking an entity in the image to count it (discussed shortly). She can edit the dates and times in the DateTime control by its up/down arrow buttons, or by directly selecting a date from a calendar. She can accept text predictions in Notes instead of typing an entry in full. Each Note tracks all previously typed text entries and uses those to predict the rest of the text as the analyst types. For example, Figure <xref rid="ece35767-fig-0009" ref-type="fig">9</xref> illustrates the text prediction that appears after the analyst has typed the single letter &#x0201c;O.&#x0201d;</p><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0009" orientation="portrait" position="float"><label>Figure 9</label><caption><p>A note displaying a text prediction</p></caption><graphic id="nlm-graphic-19" xlink:href="ECE3-9-13706-g009"/></fig></sec></sec><sec id="ece35767-sec-0032"><label>7.2</label><title>Problem&#x02014;Counting is difficult and error&#x02010;prone when there are many countable entities present in an image</title><p>As previously discussed, some cameras are positioned to capture a wide field of view. In turn, the resulting images can contain many entities, perhaps of different types, that must be counted (e.g., Figure <xref rid="ece35767-fig-0001" ref-type="fig">1</xref>b). A wildlife monitoring example is a herd of animals present in the field of view, while a fisheries example is many anglers and nonanglers present on a popular lake's shoreline or in boats (Greenberg &#x00026; Godin, <xref rid="ece35767-bib-0019" ref-type="ref">2015</xref>). All entities must be categorized and counted. The problem is that mis&#x02010;counting is easy. Common errors include losing track of the current count number, double counting that counts an entity more than once, and omission errors where an entity is accidentally skipped.</p><sec id="ece35767-sec-0033"><label>7.2.1</label><title>Design pattern: The system should allow one to visually mark the entities present in an image that have been counted along with its type</title><p>Visually marking entities as the analyst counts them can mitigate common counting errors: the analyst can discern what has been counted and what is yet to be counted. If different entities are present and being counted, the visual mark could also indicate how that entity was identified. Visual marking also affords validation, where a (perhaps different) analyst can later review the image and its marked entities for counting or classification errors.</p></sec><sec id="ece35767-sec-0034"><label>7.2.2</label><title>Timelapse example</title><p>The Counter data entry control supports interactive counting and visual marking and is illustrated in Figure <xref rid="ece35767-fig-0010" ref-type="fig">10</xref>. Here, the analyst has activated the &#x0201c;Goats&#x0201d; interactive counting mode by clicking its radio button. The analyst then counts goats simply by clicking next to each one: each click increments the count and adds a colored marker at that spot. Markers also work with the magnifying glass, where the analyst can inspect entities before marking them. Finally, markers provide feedback as to which Counter button they are associated with. For example, hovering over a marker reveals that it was counted as a &#x0201c;Goat&#x0201d; (as in Figure <xref rid="ece35767-fig-0010" ref-type="fig">10</xref>). Conversely, hovering over the Goats Counter button will highlight only those marks in the image counted as a &#x0201c;Goat.&#x0201d;</p><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0010" orientation="portrait" position="float"><label>Figure 10</label><caption><p>An activated Count control showing the visual marks next to the counted goats</p></caption><graphic id="nlm-graphic-21" xlink:href="ECE3-9-13706-g010"/></fig></sec></sec><sec id="ece35767-sec-0035"><label>7.3</label><title>Problem&#x02014;The analyst has to manually re&#x02010;enter image data even when it is available in a computer&#x02010;readable form</title><p>Analysts find it particularly frustrating when they have to re&#x02010;enter information that is already available electronically. This problem usually arises when software does not try to read in that information, or cannot make sense of that information without some guidance.</p><sec id="ece35767-sec-0036"><label>7.3.1</label><title>Design pattern: The system should automatically fill in data fields if the information is available</title><p>The system should try to automatically fill in useful and readily available known information. This can include &#x0201c;standard&#x0201d; information such as file names, file location in folder, and the date and time the image was taken. As well, image files typically contain embedded metadata that describes attributes of the image, where some of these fields could be of interest and automatically imported. Yet metadata introduces its own problems. Most camera vendors embed a mix of standard and nonstandard (proprietary) metadata, which means that the information available is highly camera&#x02010;dependent. For example, some may include ambient temperature and GPS location of the station, but others may not. Another issue is that different venders may name fields differently, for example, the outside temperature may be recorded in one camera as &#x0201c;Ambient temperature,&#x0201d; and in another as &#x0201c;Temperature C.&#x0201d; Thus, the analyst should be able to specify what metadata fields of interest, if any, should be imported, and where that information should go.</p></sec><sec id="ece35767-sec-0037"><label>7.3.2</label><title>Timelapse example</title><p>Timelapse automatically fills in data fields in two ways.
<list list-type="order" id="ece35767-list-0007"><list-item><p>
<bold>Standard file information.</bold> Timelapse template schemas always include several default data fields representing standard file information: its name, its location (as a folder name and relative path), and the date and time that image was taken (as a combined Date/Time Field). Figure <xref rid="ece35767-fig-0002" ref-type="fig">2</xref>, top, shows these data fields in the top rows: The grayed out cells are not editable. When the analyst first invokes Timelapse on an image set, Timelapse scans every file for that information and fills in those corresponding data fields.</p></list-item><list-item><p>
<bold>Metadata.</bold> Timelapse includes a <italic>metadata viewer</italic>, which the analyst can invoke on one of the images being analyzed and specify what data should be imported. We explain how this works by the example illustrated in Figure <xref rid="ece35767-fig-0011" ref-type="fig">11</xref>. The metadata viewer displays all the metadata found in the image as a table. The analyst sees, in the first row of the table in Figure <xref rid="ece35767-fig-0011" ref-type="fig">11</xref>, that the camera has recorded some metadata of interest: the &#x0201c;Ambient Temperature&#x0201d; field that records the temperature at the time the image was taken. As annotated in red in Figure <xref rid="ece35767-fig-0011" ref-type="fig">11</xref>, the analyst can link the Ambient Temperature metadata field to a Timelapse data entry Note field&#x02014;in this case a field called &#x0201c;Temperature&#x0201d;&#x02014;simply by selecting both of them. When the analyst clicks the &#x0201c;Populate&#x0201d; button (bottom), the &#x0201c;Temperature&#x0201d; field for each and every image is automatically filled in with the Ambient Temperate metadata value recorded in each image. The process can be repeated for other metadata of interest.</p></list-item></list>
</p><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0011" orientation="portrait" position="float"><label>Figure 11</label><caption><p>The metadata inspector. The analyst can see what metadata fields are available, and link a particular metadata field to a Timelapse data field to import the metadata value into that field across all images</p></caption><graphic id="nlm-graphic-23" xlink:href="ECE3-9-13706-g011"/></fig></sec></sec><sec id="ece35767-sec-0038"><label>7.4</label><title>Problem&#x02014;The analyst has to enter information that the computer should be able to recognize by image analysis</title><p>Analysts usually have experience using a variety of other image&#x02010;based systems when doing day to day and recreational tasks. Many include capabilities that recognize aspects of an image, with perhaps face recognition, bar code reading, and text recognition being common examples. Analysts may find it frustrating to enter data that they believe could be detected through image analysis and automatically filled in.</p><sec id="ece35767-sec-0039"><label>7.4.1</label><title>Design Pattern. The system should, if plausible, use image analysis techniques to automatically fill in data fields</title><p>Generally speaking, image analysis is the extraction of meaningful data from a digital image. One form of image analysis is image recognition, where complex algorithms use models built upon prior human classification to identify features in an image, such as objects, people, text, faces, and so on. As previously discussed, various researchers are now applying image analysis, and in particular image recognition techniques, to classify images from camera traps. A typical objective is to see how well various recognition algorithms identify animal species (e.g., Norouzzadeha et al., <xref rid="ece35767-bib-0031" ref-type="ref">2018</xref>; Schneider et al., <xref rid="ece35767-bib-0039" ref-type="ref">2018</xref>; Tabak et al., <xref rid="ece35767-bib-0047" ref-type="ref">2018</xref>; Yousif et al., <xref rid="ece35767-bib-0055" ref-type="ref">2019</xref>), and even in recognizing individuals in particular species (e.g., Cheema &#x00026; Anand, <xref rid="ece35767-bib-0009" ref-type="ref">2017</xref>; Crouse et al., <xref rid="ece35767-bib-0011" ref-type="ref">2017</xref>). Simpler image analysis methods can also identify other image aspects, for example, differentiate between color versus monochrome images, light versus dark images, and so on. Because image analysis and recognition are not yet full proof, manual verification of the data will be required, at least for the near future. Thus, extracted data should be integrated into the analyst's workflow in a manner that allows the analysts to check and correct that data as needed.</p></sec><sec id="ece35767-sec-0040"><label>7.4.2</label><title>Timelapse example</title><p>
<list list-type="order" id="ece35767-list-0008"><list-item><p>
<bold>Dark images.</bold> Some of the agencies we worked with used cameras set in Timelapse mode that periodically took images over a day's 24&#x02010;hr period. A good number of those images proved of little value because they were too dark (e.g, shots taken at night time) and added clutter to the images being reviewed. To help identify overly dark images, Timelapse incorporates an image analyser that automatically classifies images against a user&#x02010;configurable darkness threshold. Its classification is recorded in the &#x0201c;Image Quality&#x0201d; data field of every image as either &#x0201c;Dark&#x0201d; or &#x0201c;Ok.&#x0201d; Timelapse also includes the ability to filter the displayed images by its data, which we will discuss shortly. Analysts could apply a &#x0201c;Dark&#x0201d; image filter to display only dark images, where the analyst can quickly review and correct the classification if needed, and perhaps discard those night&#x02010;time shots. Alternately, the analyst could apply an &#x0201c;Ok&#x0201d; filter, which displays only the nondark images.</p></list-item><list-item><p>
<bold>Animal detection and recognition.</bold> We are currently working with several vision researchers who specialize in automated animal detection (e.g., whether an animal is in an image) and species recognition (which species the animal is). (Microsoft, <xref rid="ece35767-bib-0028" ref-type="ref">2019</xref>; Schneider et al., <xref rid="ece35767-bib-0039" ref-type="ref">2018</xref>). Figure <xref rid="ece35767-fig-0012" ref-type="fig">12</xref> illustrates a Timelapse prototype that imports and displays animal detection data produced by Microsoft's &#x0201c;Megadetector&#x0201d; model (Microsoft, <xref rid="ece35767-bib-0028" ref-type="ref">2019</xref>). Basically, Megadetector scans all provided images and outputs data to a file. For each image, Megadetector detects whether an animal, person, or vehicle is in an image, its confidence of correctness, and the coordinates of a bounding box outlining each entity's location. Timelapse imports that data and draws a bounding box atop each identified entity above a detection confidence threshold (set by the analyst). The analyst then uses the standard Timelapse features to select detected entities and review predictions at given confidence levels and accepts or rejects those predictions as needed.</p></list-item></list>
</p><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0012" orientation="portrait" position="float"><label>Figure 12</label><caption><p>Timelapse prototype incorporating recognition data: bounding boxes are drawn around each suspected species in the image when its detection confidence exceeds a user&#x02010;defined threshold</p></caption><graphic id="nlm-graphic-25" xlink:href="ECE3-9-13706-g012"/></fig></sec></sec><sec id="ece35767-sec-0041"><label>7.5</label><title>Problem&#x02014;Cameras often record incorrect or ambiguous timestamps</title><p>We have observed many issues resulting from the way camera traps record date and time. While the software can automatically import and fill in date/time fields, analysts may have to correct those after the fact. The problem is that it is incredibly time&#x02010;consuming to manually correct every image's date and time. Common issues we have observed are as follows:
<list list-type="bullet" id="ece35767-list-0009"><list-item><p>The camera is not set to the correct date and time when deployed, meaning all date/times are off by a fixed amount.</p></list-item><list-item><p>The camera does not take into account changes in daylight saving time, which means a large subset of images are off by an hour.</p></list-item><list-item><p>The camera's internal clock drifts, for example, it runs slow or fast, which means that the date/time of successive images is increasingly inaccurate.</p></list-item><list-item><p>The camera records dates ambiguously. For example, consider a date recorded as 02/10/2019. This date can be interpreted as either October 2, 2019 in day/month order, or as February 10 in month/day order. Even worse are cameras that record the year as only the last two digits, for example, 02/10/10 could be interpreted many different ways. This issue is exacerbated by the way different countries set different format standards for encoding dates and times (e.g., see Wikipedia: Date format by country).</p></list-item></list>
</p><sec id="ece35767-sec-0042"><label>7.5.1</label><title>Design pattern: The system should provide facilities to bulk&#x02010;correct common date/time errors</title><p>All the above errors care amenable to bulk&#x02010;correction, albeit with some manual guidance. For example, if the camera was not set to the correct date and time, the analyst would only have to enter the correct date for the first image. The system could calculate the difference between the two, and then use that difference to time&#x02010;shift the date and time for all subsequent images. Similarly, the analyst can specify where the daylight savings time change should occur and time&#x02010;shift previous or subsequent images by plus or minus an hour. To correct for internal clock drifts, the analyst can specify the correct time for the last image, where the system would then calculate a drift factor and adjust the times across all images. When the software detects a possibility for ambiguous dates, it can notify the analyst who can then indicate which date format to apply.</p></sec><sec id="ece35767-sec-0043"><label>7.5.2</label><title>Timelapse example</title><p>Because we expect analysts to have to correct dates only infrequently, analysts can raise specialized dialogs for each type of date/time error mentioned above: each dialog includes full instructions and an easy to use interface for specifying how the date should be corrected. For example, Figure <xref rid="ece35767-fig-0013" ref-type="fig">13</xref> illustrates the Timelapse dialog for correcting standard/daylight savings time errors. The analyst navigates to the first image that should be corrected, and then specifies (via various checkboxes) how the correction should be applied. A preview of the corrected date and time is also displayed.</p><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0013" orientation="portrait" position="float"><label>Figure 13</label><caption><p>Dialog for correcting daylight savings time</p></caption><graphic id="nlm-graphic-27" xlink:href="ECE3-9-13706-g013"/></fig></sec></sec></sec><sec id="ece35767-sec-0044"><label>8</label><title>ISSUE: ENTERING REPETITIOUS DATA</title><sec id="ece35767-sec-0045"><label>8.1</label><title>Problem&#x02014;Similar data are often entered and re&#x02010;entered over many images</title><p>Image sets often comprise subsets of very similar images. For example, a motion&#x02010;triggered camera may capture a sequence of multiple images of an animal moving through a scene. As another example, an image set can comprise a small set of recurrent but interspersed images, for example, images containing goats, or elk, or deer, or nothing at all. The data entered that describe these images are often highly similar. Even when the analyst recognizes these similarities, she still has to manually enter the same data per similar image over and over again. This leads to highly repetitious and very time&#x02010;consuming data re&#x02010;entry.</p><sec id="ece35767-sec-0046"><label>8.1.1</label><title>Design pattern: It should be easy to re&#x02010;enter data previously entered elsewhere</title><p>Various general techniques are known in other domains for re&#x02010;entering the same data efficiently. Examples include history lists, copying and pasting, predictions based on previous entries, data propagation, and others.</p></sec><sec id="ece35767-sec-0047"><label>8.1.2</label><title>Timelapse example</title><p>Timelapse includes several techniques for easing the task of entering repetitive data across multiple images.
<list list-type="order" id="ece35767-list-0010"><list-item><p>
<bold>Text prediction in a single data field.</bold> As already discussed, Notes include auto&#x02010;completion capabilities. They maintain a history of previously typed text entries and use those to predict the rest of the text as the analyst types.</p></list-item><list-item><p>
<bold>Propagating data across a single data field.</bold> Every data field includes a pop&#x02010;up menu that allows the analyst to propagate data across a sequence of images (Figure <xref rid="ece35767-fig-0014" ref-type="fig">14</xref>). <italic>Propagate from the last nonempty value to here</italic> uses back&#x02010;filling. That is, it will copy the last nonempty value entered by the analyst in a data field (e.g., several images back in the sequence) to every intervening image up to the current image. For example, the analyst may enter an image's weather as &#x0201c;Sunny,&#x0201d; then navigate forward through the images until the weather changes, and then backfill the intervening empty fields with that value. <italic>Copy forward to end</italic> is somewhat similar, except it forward&#x02010;fills the current value to all remaining images in the sequence. It can be re&#x02010;applied at any time where it over&#x02010;writes existing values. For example, an analyst may Copy forward Cloudy (as in Figure <xref rid="ece35767-fig-0014" ref-type="fig">14</xref>), then move through the sequence until the next non&#x02010;Cloudy day is noted, enter the new value, and then Copy that forward as well. <italic>Copy to all</italic> copies the current value to all images. For example, the analyst may just enter their name once in the &#x0201c;Analyst&#x0201d; field and copy that to all images.</p></list-item><list-item><p>
<bold>Copy Previous Values.</bold> Image sets often contain runs of identical images, where some of the data entered over the next image are identical to what was entered in the previous image. Timelapse supplies a &#x0201c;Copy Previous Values&#x0201d; button, illustrated in Figure <xref rid="ece35767-fig-0015" ref-type="fig">15</xref>. Pressing this button copies the previous image's values from particular data fields (those set as &#x0201c;Copyable&#x0201d; in the template: See Figure <xref rid="ece35767-fig-0002" ref-type="fig">2</xref>) to the current image's data fields. As illustrated in Figure <xref rid="ece35767-fig-0015" ref-type="fig">15</xref>, previews of what fields are affected and the data that will be copied are displayed and highlighted in green when the analyst hovers the mouse over the Copy previous values button.</p></list-item><list-item><p>
<bold>Quickpaste: Copying and pasting multiple data fields.</bold> Analysts typically recognize when they entering a small set of similar data patterns over and over again. Timelapse provides QuickPaste as a way for the analyst to capture and name these data entry patterns, where the analyst can then paste that pattern into an image's data field via a single mouse click. Figure <xref rid="ece35767-fig-0016" ref-type="fig">16</xref> below illustrates this through a simple example. The analyst has raised the QuickPaste editor (left) to compose a QuickPaste entry: she has titled the entry &#x0201c;No goats, sunny&#x0201d; and has selected and filled in which data fields should be used (Goats, Weather, Analyst, Comments, Publicity) and the values to be pasted. This entry is then added to the list of other QuickPaste entries in the QuickPaste window (right side). The analyst can then use the QuickPaste window to select and paste particular patterns into the image's data entry fields. As illustrated in Figure <xref rid="ece35767-fig-0016" ref-type="fig">16</xref>, when she hovers over an entry, a preview of the values to be pasted appears in the affected data fields (highlighted in green). Clicking the entry pastes, those values into the field. While requiring some initial setup to create these custom entries, QuickPaste becomes a very effective and efficient way for entering common data patterns.</p></list-item></list>
</p><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0014" orientation="portrait" position="float"><label>Figure 14</label><caption><p>The data field's pop&#x02010;up menu for propagating data</p></caption><graphic id="nlm-graphic-29" xlink:href="ECE3-9-13706-g014"/></fig><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0015" orientation="portrait" position="float"><label>Figure 15</label><caption><p>Copy Previous Values button, showing previews of the data to be copied</p></caption><graphic id="nlm-graphic-31" xlink:href="ECE3-9-13706-g015"/></fig><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0016" orientation="portrait" position="float"><label>Figure 16</label><caption><p>The QuickPaste editor (left) and the QuickPaste window (right)</p></caption><graphic id="nlm-graphic-33" xlink:href="ECE3-9-13706-g016"/></fig></sec></sec><sec id="ece35767-sec-0048"><label>8.2</label><title>Problem&#x02014;Reviewing and entering repetitive data image by image can be inefficient</title><p>Most image packages display a single image at a time, where the analyst has to inspect and enter data for them individually. Bulk&#x02010;image inspection and data entry are not possible.</p><sec id="ece35767-sec-0049"><label>8.2.1</label><title>Design pattern: Allow the analyst to inspect and bulk&#x02010;enter data for multiple images at a time</title><p>The system should provide facilities for displaying multiple images at a time (e.g., a table of large thumbnails). The analyst should be able to select particular images with common features, and then bulk&#x02010;entering data for those selected images all at once. The analyst should also be able to choose the appropriate thumbnail size, as the features of interest need to be discernable.</p></sec><sec id="ece35767-sec-0050"><label>8.2.2</label><title>Timelapse example</title><p>The overview supplied in Timelapse, discussed above and previously illustrated in Figure <xref rid="ece35767-fig-0007" ref-type="fig">7</xref>, allows the analyst to review multiple images at the same time. The analyst can quickly trade&#x02010;off the number of images displayed versus the image size (to optimize just&#x02010;discernable features with the number of images shown) by zooming in or out of different overview levels with the scroll wheel. The analyst can then select and bulk&#x02010;edit data for one or more of those images. For example, Figure <xref rid="ece35767-fig-0007" ref-type="fig">7</xref> shows how the analyst selected only those images with a full view of a goat in it (the first five images), where she has entered a &#x0201c;1&#x0201d; in the Goats Counter field and &#x0201c;Full body view&#x0201d; in the Comment field. Those values are then applied to all the selected images. Interface subtleties are also addressed. As multiple selections are done, the data fields and their contents are adjusted to reflect that selection. For example, and as also shown in Figure <xref rid="ece35767-fig-0007" ref-type="fig">7</xref>, the DateTime data field is disabled as bulk&#x02010;editing that field makes little sense. If a data field in the selected images all share the same data value, that value is displayed. Otherwise a &#x0201c;&#x02026;&#x0201d; symbol is displayed to indicate that their values differ.</p></sec></sec></sec><sec id="ece35767-sec-0051"><label>9</label><title>ISSUE: SORTING AND FILTERING THE IMAGE SEQUENCE</title><sec id="ece35767-sec-0052"><label>9.1</label><title>Problem&#x02014;Images are often presented in a single sort order, usually based on their file name, which may not reflect how the analyst wants to view them</title><p>Analysts usually inspect images as a sequence, one after the other. Thus, the way images are ordered (sorted) can affect what they see and how they interpret images as events unfolding over time. Consider the example of a motion&#x02010;triggered camera taking images of one or more animals moving through the scene. If the presentation sequence is in time order, the analyst will recognize that those images relate to one another, as they are capturing a single event. As another example, the analyst may wish to review already classified images ordered by a combination of criteria. For example, the analyst may want to get a sense of whether the number of goats using the pasture in Figure <xref rid="ece35767-fig-0003" ref-type="fig">3</xref> is correlated to weather conditions. This can be done by ordering images by weather and then by the number of goats. The problem is that most systems typically order images only by its file name and do not allow any other sorting capabilities.</p><sec id="ece35767-sec-0053"><label>9.1.1</label><title>Design pattern: Allow the analyst to sort images by one or more criteria</title><p>Providing the ability to sort by date/time rather than file name is perhaps the most fundamental sort capability that should be included. While cameras typically add a sequence number to a file name as images files are created, there is no guarantee that they will be presented in time order for example, alphabetically sorted files named 1.jpg, 2.jpg&#x02026; 10.jpg would be presented as 1.jpg, 10.jpg, 2.jpg&#x02026;, which breaks time ordering. Ideally, the software will also allow the analyst to sort on any data field or combination of fields and their data values.</p></sec><sec id="ece35767-sec-0054"><label>9.1.2</label><title>Timelapse example</title><p>Timelapse provides a sorting capability based on one or two data fields of the analyst's choosing. The analyst can quickly select (via a menu) common sorting criteria including image load order, date/time order, how images are organized into folders, and by particular data entry field contents. The analyst can also raise a custom sort dialog (Figure <xref rid="ece35767-fig-0017" ref-type="fig">17</xref> below), where she can choose primary and secondary sorting criteria from a drop&#x02010;down menu that lists labels for the data fields. In this case, she is sorting by weather and then by Goats. Images are then presented in that sort order. The rows in the data table in Figure <xref rid="ece35767-fig-0008" ref-type="fig">8</xref> are also updated to that sort order.</p><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0017" orientation="portrait" position="float"><label>Figure 17</label><caption><p>The sorting dialog</p></caption><graphic id="nlm-graphic-35" xlink:href="ECE3-9-13706-g017"/></fig></sec></sec><sec id="ece35767-sec-0055"><label>9.2</label><title>Problem&#x02014;The analyst may need to view a particular subset of images</title><p>Analysts may, at times, be interested in only a subset of the available files. Yet finding and viewing the images in this subset can be problematic, especially with large image sets comprising tens of thousands of files. As one example, the analyst may want to verify and possibly correct prior image classification category, for example, that all system&#x02010;classified dark images are indeed dark, that images classified by another analyst as &#x0201c;Goats&#x0201d; all contain goats, and so on. As another example, the analyst may be interested in only those files taken at a certain site and between particular dates. As yet another example, the analyst may want to review a particular image classification in order to choose an archetypical image, for example, an excellent image of a goat to be used for publicity purposes.</p><sec id="ece35767-sec-0056"><label>9.2.1</label><title>Design pattern: Allow the analyst to specify criteria that filters which images are displayed</title><p>The system should provide the analyst with a query facility and search engine. The analyst should be able to specify a search query, where the system filters images so that it only displays images matching that query. Query criteria should include queries against the values recorded in the image data fields.</p></sec><sec id="ece35767-sec-0057"><label>9.2.2</label><title>Timelapse example</title><p>Timelapse incorporates a free database (SQLite: <ext-link ext-link-type="uri" xlink:href="http://www.sqlite.org">http://www.sqlite.org</ext-link>) to store the data entered by the analyst. SQLite includes a query language for searching for matching records. Thus, Timelapse can perform any standard database search against that data, where search results are returned as records describing the matching images. Those images are then displayed. However, it is unrealistic to expect analysts to compose cryptic SQL query expressions. As a better alternative, Timelapse displays a dialog box listing all data fields, as illustrated in Figure <xref rid="ece35767-fig-0018" ref-type="fig">18</xref>. The analyst then composes a query by selecting the data fields of interest, and then specifying the values that should be matched. The system translates that into an SQL query and returns only those images that match the query. For example, in Figure <xref rid="ece35767-fig-0018" ref-type="fig">18</xref>, the analyst is interested in the interaction between goats and hikers and wishes to see only those images that have both a goat and a hiker in it. The analyst selects the Goats and Hikers data fields for use (the &#x0201c;Select&#x0201d; column on the left) and has specified that both have values greater than 0 (the &#x0201c;Expression column&#x0201d;). The &#x0201c;AND&#x0201d; checkbox at the top indicates that both those constraints must be satisfied. Feedback (bottom right) indicates that three files match that query. After clicking Okay, only those three images will be available for navigation and display. Had the analyst had clicked the &#x0201c;OR&#x0201d; checkbox instead, then all returned images would contain either one or more goats, or one or more hikers, or both. The Timelapse Sort function can also be applied to the results, for example, to show all images with both goats and hikers, but sorted by the number of goats and then by the number of hikers.</p><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0018" orientation="portrait" position="float"><label>Figure 18</label><caption><p>The query dialog for filtering images from view</p></caption><graphic id="nlm-graphic-37" xlink:href="ECE3-9-13706-g018"/></fig></sec></sec><sec id="ece35767-sec-0058"><label>9.3</label><title>Problem&#x02014;The analyst may need to consider images taken over a short time period as a unit</title><p>As previously mentioned, camera traps set in motion&#x02010;capture mode are often triggered when an animal or herd is moving through a scene. This can result in a burst of images that capture that activity, which we define as an <italic>episode</italic>. Episodes are sometimes treated differently than individual images. For example, we saw analysts manually determine which images fall into an episode (e.g., by examining their timestamp), count the unique wildlife seen in that episode, and enter that data into only a single image. They do this to avoid inflating the number of wildlife present. To illustrate, consider the analyst who has to count the number of hikers using a trail. A single hiker may appear on several images over time, perhaps due to motion triggering, or because the hiker is milling about in the camera's field of view. To avoid double counting, the analyst would only count the hiker once in this series. The problem is that it is laborious for the analyst to recognize which images belong together in an episode.</p><sec id="ece35767-sec-0059"><label>9.3.1</label><title>Design pattern: The system should identify and group episodes of time&#x02010;related images</title><p>Various strategies can be used to identify episodes. For example, some cameras include metadata that indicate whether an image is part of a motion&#x02010;capture sequence, as well its position in that sequence (e.g.,1/5, 2/5, etc.). While useful, it is limited as an episode can easily comprise two or more back to back motion&#x02010;capture sequences. Alternately, a reasonable heuristic is to have the system examine the time interval between time&#x02010;ordered images. If the interval is small, the system would group them together as part of an episode.</p></sec><sec id="ece35767-sec-0060"><label>9.3.2</label><title>Timelapse example</title><p>Timelapse uses the heuristic above, where the analyst can ask it to group together images separated by a small user&#x02010;configurable time interval. Timelapse then annotates each image to indicate how images relate to one another as an episode. Figure <xref rid="ece35767-fig-0019" ref-type="fig">19</xref> is similar to Figure <xref rid="ece35767-fig-0007" ref-type="fig">7</xref>, except that it now illustrates how episode annotations appear in the overview. The first image in an episode is colored red (top left) so that the analyst can visually identify the start of the episode. That and subsequent images in the episode are given a sequence number (e.g., 1/3, 2/3, and 3/3). A timestamp is also overlaid atop the image, so that the analyst can examine the time differences between those images if needed. If an image does not belong to an episode, it is marked as &#x0201c;Single&#x0201d; (not shown). In Figure <xref rid="ece35767-fig-0019" ref-type="fig">19</xref>, the first ten and the last five images are identified as two different episodes of a goat walking through the scene. In this case, the analyst does not want to double count the same goat. Consequently, she selects the best image in each episode (Img04 and Img15), and increments the Goats counter of only that image.</p><fig fig-type="Figure" xml:lang="en" id="ece35767-fig-0019" orientation="portrait" position="float"><label>Figure 19</label><caption><p>Episodes. Here, the analyst is using a strategy of entering data on only one image per episode</p></caption><graphic id="nlm-graphic-39" xlink:href="ECE3-9-13706-g019"/></fig></sec></sec></sec><sec sec-type="discussion" id="ece35767-sec-0061"><label>10</label><title>DISCUSSION</title><p>Decisions on what software is used to inspect and encode image data have consequences on how well an analyst can perform their job. Yet, we question how some agencies make their decision. We have seen some consider only the stock software available on typical computers: For example, Microsoft Photo Viewer to view images, and an Excel spreadsheet for data entry. This is inefficient. For example, we previously studied how analysts entered data using spreadsheets versus an earlier version of Timelapse. Timelapse provided time improvements of ~200% or more, which translates into significant cost savings (Greenberg &#x00026; Godin, <xref rid="ece35767-bib-0019" ref-type="ref">2015</xref>). We saw other agencies use either researcher&#x02010;based software or the stock software that came bundled with their cameras without considering the consequences of that choice on the analyst. Some agencies may also make their choice based on other factors, such as how the software stores data in a format amenable to standardization or later analysis versus how that data are actually entered by analysts. We advocate that decisions on which software is used should deeply consider how well they support the analysts' tasks. The design patterns described earlier should be part of that consideration. Poor system choices imply tedious data entry, are error&#x02010;prone (which affects the validity of the collected data), are morale&#x02010;sucking, and&#x02014;in the long run&#x02014;are very expensive in terms of analyst time.</p><p>While our design patterns mitigate various problems faced by analysts, we recognize that these problems range in seriousness, in frequency of occurrence, in applicability to particular projects, and in consequences if they are not addressed. We also recognize that our catalog of design patterns is just a starting point and future work is required: There are surely other problems and design patterns that could and should be articulated and considered in camera trap analysis design. For example, if image analysis is done through crowdsourcing and citizen science (Swanson, Kosmala, Lintott, &#x00026; Packer, <xref rid="ece35767-bib-0045" ref-type="ref">2016</xref>), design patterns specific to that audience would likely emerge. Design patterns can also extend beyond interface features. For example, they can recognize and address the problems related to data management issues (e.g., Ivan &#x00026; Newkirk, <xref rid="ece35767-bib-0022" ref-type="ref">2016</xref>), data validation, and data standardization and scaling across the field (e.g., Steenweg et al., <xref rid="ece35767-bib-0043" ref-type="ref">2017</xref>).</p><p>We also stress that design patterns are not &#x0201c;feature list.&#x0201d; Rather, each design pattern suggests a design approach that can be adapted, refined, and specialized to best fit the project, the background and needs of the analysts, and the equipment available. Each design pattern can also inform decision&#x02010;making. If the problem and design approach is relevant, that should become a factor influencing the requirements analysis of the software being developed or for a manager deciding between available software systems.</p><p>We also show how our own Timelapse system implements the design pattern. These are intended to serve as concrete examples rather than prescriptions. Of course, the specific techniques used by Timelapse could be implemented &#x0201c;as is&#x0201d; in other camera trap systems. However, we recognize&#x02014;and indeed encourage&#x02014;future system designers to see beyond our own solutions, where they should seek solutions that implement the design pattern in even better ways. For example, Timelapse was intentionally designed to work on lowest common denominator computers typically available to analysts: Microsoft Windows running a keyboard and mouse on a conventional low&#x02010;cost computer as found in many agencies. Thus its design eschewed more modern interaction techniques, such as touch interaction, as we felt it would limit its deployment. If a system such as Timelapse was redesigned to run on (say) a touch&#x02010;based tablet, we would expect different design solutions that still follow the above design pattern recommendations. Similarly, Timelapse was designed to work off&#x02010;line so analysts could work in the field on disconnected laptops. If Timelapse was redesigned to work as a networked client or over the web, design solutions would have to account for performance aspects such as network bandwidth and latency that could affect responsiveness and rapid image display.</p><p>As mentioned, we recognize that our list of design patterns is incomplete, where future work should elicit other design patterns to produce a comprehensive catalog. Researchers should continually conduct interviews and observation of analysts as they work to gain an even more nuanced understanding of their core tasks and problems. Because camera traps are broadly used for many quite different purposes, domain&#x02010;specific design patterns should be developed. Other software systems should be reviewed and compared for how they address problems and deliver solutions not covered by Timelapse, and whether those can be encapsulated as useful design patterns. As well, our design patterns are limited to only the analyst's interface for inspecting images and entering data. Future work should consider design patterns for other related tasks. One example concerns interface patterns that suggest how a project manager can view and manage data within and across projects. To illustrate, the Reconyx MapView software (Reconyx Inc, <xref rid="ece35767-bib-0035" ref-type="ref">2016</xref>) includes a map interface that lets the project manager or analyst geo&#x02010;locate study sites and stations onto it, and which lets them drill down into the captured data.</p><p>Finally, we recognize that elements of various design patterns are based on aspects well&#x02010;known within the field of human&#x02013;computer interaction, information visualization, and experience design. These fields have a rich literature of research, practitioner's guides, and texts relating to the design of systems for human use, including methodologies that describe how to test how well a person can use that system and its features (e.g., Shneiderman et al., <xref rid="ece35767-bib-0041" ref-type="ref">2016</xref>; Spence, <xref rid="ece35767-bib-0042" ref-type="ref">2014</xref>). As well, various stock components and interaction techniques are readily available in software development tools, where most are based upon best practices of user interaction. The catch is that decisions of what is relevant must still be made on the needs of the domain being considered. This is the purpose of this paper, where it identifies problems and solutions as design patterns relevant to the domain of camera trap image analysis.</p></sec><sec sec-type="COI-statement" id="ece35767-sec-0062"><title>CONFLICT OF INTEREST</title><p>None declared.</p></sec><sec id="ece35767-sec-0063"><title>AUTHOR CONTRIBUTIONS</title><p>See the Section <xref rid="ece35767-sec-0003" ref-type="sec">3</xref>, which describes the various roles played by the authors in more detail. Greenberg developed the Timelapse software and was the primary author of the design patterns listed here. Godin and Whittington contributed regularly to the Timelapse software design via on&#x02010;going discussions of its features and its use by their team of analysts. They also reviewed and contributed to various drafts of this paper.</p></sec><sec id="ece35767-sec-0064"><sec disp-level="2" id="ece35797-sec-0026"><title>OPEN RESEARCH BADGES</title><p>This article has earned an <ext-link ext-link-type="uri" xlink:href="https://openscience.com">https://openscience.com</ext-link> for making publicly available the components of the research methodology needed to reproduce the reported procedure and analysis. All materials are available at <ext-link ext-link-type="uri" xlink:href="https://Github.com/saulgreenberg/Timelapse">https://Github.com/saulgreenberg/Timelapse</ext-link> and <ext-link ext-link-type="uri" xlink:href="http://saul.cpsc.ucalgary.ca/timelapse/">http://saul.cpsc.ucalgary.ca/timelapse/</ext-link>.</p></sec></sec></body><back><ack id="ece35767-sec-0067"><title>ACKNOWLEDGEMENTS</title><p>This research was partially funded by the National Science and Engineering Research Council. We interacted with a large number of Timelapse users over the years. We listened to their needs, their feature requests, heard their feedback of using Timelapse, and watched them use Timelapse on their own images. That information was invaluable for designing the successive improved versions of the system over time. We also thank Todd West for his contributions to earlier versions of Timelapse. Images are courtesy of Parks Canada.</p></ack><fn-group id="ece35767-ntgp-0001"><title>ENDNOTES</title><fn id="ece35767-note-1001"><label>1</label><p>Timelapse availability is described at the end of this paper.</p></fn><fn id="ece35767-note-1002"><label>2</label><p>The cited paper concerns an earlier version of Timelapse. While it includes descriptions of a subset of interface features explored in this paper, those descriptions tend to be more superficial, are not provided as design patterns, or discussed in that context. The current paper also covers design patterns of features not present in that earlier system or paper.</p></fn><fn id="ece35767-note-1003"><label>3</label><p>A narrated video illustrating many of the Timelapse features described in this paper can be found at: <ext-link ext-link-type="uri" xlink:href="http://grouplab.cpsc.ucalgary.ca/grouplab/uploads/Publications/Publications/2019-DesigningCameraTrapSoftware.TechSymp.mp4">http://grouplab.cpsc.ucalgary.ca/grouplab/uploads/Publications/Publications/2019-DesigningCameraTrapSoftware.TechSymp.mp4</ext-link>
</p></fn><fn id="ece35767-note-1004"><label>4</label><p>An <italic>image set</italic> as the <italic>collection of images</italic> being analyzed by an analyst. The images that comprise an image set depends on how the project manager conceptualizes and manages images as a set. As one example, an image set can comprise only the images just extracted from the SD card at a particular camera station at the end of a servicing period. Alternately, an image set can comprise all images at a particular station, where new images extracted from a servicing period are added to the collection of older (perhaps already analyzed) images. As a further example, an image set can comprise all images taken from multiple camera stations at a particular study site.</p></fn><fn id="ece35767-note-1005"><label>5</label><p>This paper uses a deliberately simple schema/template for illustration. In practice, schemas can be quite rich and complex.</p></fn></fn-group><sec sec-type="data-availability" id="ece35767-sec-3005"><title>DATA AVAILABILITY STATEMENT</title><p>Timelapse is an open source project written in C#/WPF and available on the Github repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/saulgreenberg/Timelapse" specific-use="software is-supplemented-by">https://github.com/saulgreenberg/Timelapse</ext-link>. For even easier access, Timelapse software, installation instructions, tutorial documentation (describing all its functions and including example image and template files), and mailing list information are freely available at <ext-link ext-link-type="uri" xlink:href="http://saul.cpsc.ucalgary.ca/timelapse" specific-use="software is-supplemented-by">http://saul.cpsc.ucalgary.ca/timelapse</ext-link>. Project managers and analysts are invited to download Timelapse, and developers are invited to modify or enhance the software as needed. Finally, Timelapse is actively maintained and supported by the first author of this paper. Contact <email>saul@ucalgary.ca</email> for more information.</p></sec><ref-list content-type="cited-references" id="ece35767-bibl-0001"><title>REFERENCES</title><ref id="ece35767-bib-0001"><mixed-citation publication-type="journal" id="ece35767-cit-0001">
<string-name>
<surname>Ahumada</surname>, <given-names>J. A.</given-names>
</string-name>, <string-name>
<surname>Hurtado</surname>, <given-names>J.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Lizcano</surname>, <given-names>D.</given-names>
</string-name> (<year>2013</year>). <article-title>Monitoring the status and trends of tropical forest terrestrial vertebrate communities from camera trap data: A tool for conservation</article-title>. <source xml:lang="en">PLoS ONE</source>, <volume>8</volume>(<issue>9</issue>), <elocation-id>e73707</elocation-id>
<pub-id pub-id-type="doi">10.1371/journal.pone.0073707</pub-id>
<pub-id pub-id-type="pmid">24023898</pub-id></mixed-citation></ref><ref id="ece35767-bib-0002"><mixed-citation publication-type="book" id="ece35767-cit-0002">
<string-name>
<surname>Alexander</surname>, <given-names>C.</given-names>
</string-name> (<year>1977</year>). <source xml:lang="en">A pattern language: Towns, buildings, construction</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref><ref id="ece35767-bib-0003"><mixed-citation publication-type="journal" id="ece35767-cit-0003">
<string-name>
<surname>Blake</surname>, <given-names>J. G.</given-names>
</string-name>, <string-name>
<surname>Mosquera</surname>, <given-names>D.</given-names>
</string-name>, <string-name>
<surname>Loiselle</surname>, <given-names>B. A.</given-names>
</string-name>, <string-name>
<surname>Romo</surname>, <given-names>D.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Swing</surname>, <given-names>K.</given-names>
</string-name> (<year>2017</year>). <article-title>Effects of human traffic on use of trails by mammals in lowland forest of eastern Ecuador</article-title>. <source xml:lang="en">Neotropical Biodiversity</source>, <volume>3</volume>(<issue>1</issue>), <fpage>57</fpage>&#x02013;<lpage>64</lpage>. <pub-id pub-id-type="doi">10.1080/23766808.2017.1292756</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0004"><mixed-citation publication-type="book" id="ece35767-cit-0004">
<string-name>
<surname>Borchers</surname>, <given-names>J.</given-names>
</string-name> (<year>2001</year>). <source xml:lang="en">A pattern approach to interaction design</source> (vol. <volume>12</volume>, pp. <fpage>359</fpage>&#x02013;<lpage>376</lpage>). <publisher-name>AI &#x00026; Society, Springer</publisher-name>.</mixed-citation></ref><ref id="ece35767-bib-0005"><mixed-citation publication-type="journal" id="ece35767-cit-0005">
<string-name>
<surname>Bubnicki</surname>, <given-names>J. W.</given-names>
</string-name>, <string-name>
<surname>Churski</surname>, <given-names>M.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Kuijper</surname>, <given-names>D. P. J.</given-names>
</string-name> (<year>2016</year>). <article-title>Trapper: An open source web&#x02010;based application to manage camera trapping projects</article-title>. <source xml:lang="en">Methods in Ecology and Evolution</source>, <volume>7</volume>(<issue>10</issue>), <fpage>1209</fpage>&#x02013;<lpage>1216</lpage>. <pub-id pub-id-type="doi">10.1111/2041-210x.12571</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0006"><mixed-citation publication-type="journal" id="ece35767-cit-0006">
<string-name>
<surname>Burton</surname>, <given-names>C. A.</given-names>
</string-name>, <string-name>
<surname>Neilson</surname>, <given-names>E.</given-names>
</string-name>, <string-name>
<surname>Moreira</surname>, <given-names>D.</given-names>
</string-name>, <string-name>
<surname>Ladle</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Steenwag</surname>, <given-names>R.</given-names>
</string-name>, <string-name>
<surname>Fisher</surname>, <given-names>J. T.</given-names>
</string-name>, &#x02026; <string-name>
<surname>Boutin</surname>, <given-names>S.</given-names>
</string-name> (<year>2015</year>). <article-title>Wildlife camera trapping: A review and recommendations for linking surveys to ecological processes</article-title>. <source xml:lang="en">Journal of Applied Ecology</source>, <volume>52</volume>, <fpage>675</fpage>&#x02013;<lpage>685</lpage>. <pub-id pub-id-type="doi">10.1111/1365-2664.12432</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0007"><mixed-citation publication-type="book" id="ece35767-cit-0007">
<string-name>
<surname>Campbell</surname>, <given-names>J. M.</given-names>
</string-name> (<year>2010</year>). <chapter-title>Seeing is believing: Using digital cameras to monitor trail use in Riding Mountain National Park</chapter-title> In <person-group person-group-type="editor"><name name-style="western"><surname>Bondrup&#x02010;Nielsen</surname><given-names>S.</given-names></name></person-group>, <person-group person-group-type="editor"><name name-style="western"><surname>Beazley</surname><given-names>K.</given-names></name></person-group>, <person-group person-group-type="editor"><name name-style="western"><surname>Bissix</surname><given-names>G.</given-names></name></person-group>, <person-group person-group-type="editor"><name name-style="western"><surname>Colville</surname><given-names>D.</given-names></name></person-group>, <person-group person-group-type="editor"><name name-style="western"><surname>Flemming</surname><given-names>S.</given-names></name></person-group>, <person-group person-group-type="editor"><name name-style="western"><surname>Herman</surname><given-names>T.</given-names></name></person-group>, <person-group person-group-type="editor"><name name-style="western"><surname>McPherson</surname><given-names>M.</given-names></name></person-group>, <person-group person-group-type="editor"><name name-style="western"><surname>Mockford</surname><given-names>S.</given-names></name></person-group>, &#x00026; <person-group person-group-type="editor"><name name-style="western"><surname>O'Grady</surname><given-names>S.</given-names></name></person-group> (Eds), <source xml:lang="en">Ecosystem based management: Beyond boundaries. Proceedings of the Sixth International Conference of Science and the Management of Protected Areas</source>. <publisher-loc>Wolfville, NS</publisher-loc>: <publisher-name>Science and Management of Protected Areas Association</publisher-name>.</mixed-citation></ref><ref id="ece35767-bib-0008"><mixed-citation publication-type="book" id="ece35767-cit-0008">
<string-name>
<surname>Carpendale</surname>, <given-names>S.</given-names>
</string-name>, <string-name>
<surname>Light</surname>, <given-names>J.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Pattison</surname>, <given-names>E.</given-names>
</string-name> (<year>2004</year>). <chapter-title>Achieving higher magnification in context</chapter-title> In <source xml:lang="en">Proceedings of the 17th annual ACM symposium on User interface software and technology (UIST '04)</source> (pp. <fpage>71</fpage>&#x02013;<lpage>80</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>.</mixed-citation></ref><ref id="ece35767-bib-0009"><mixed-citation publication-type="book" id="ece35767-cit-0009">
<string-name>
<surname>Cheema</surname>, <given-names>G. S.</given-names>
</string-name>, <string-name>
<surname>Anand</surname>, <given-names>S.</given-names>
</string-name> (<year>2017</year>). <chapter-title>Automatic detection and recognition of individuals in patterned species</chapter-title> In <person-group person-group-type="editor"><name name-style="western"><surname>Altun</surname><given-names>Y.</given-names></name></person-group> (Ed.), <source xml:lang="en">Machine learning and knowledge discovery in databases. ECML PKDD 2017. Lecture notes in computer science</source> (vol. <volume>10536</volume>, pp. <fpage>27</fpage>&#x02013;<lpage>38</lpage>). <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>.</mixed-citation></ref><ref id="ece35767-bib-0010"><mixed-citation publication-type="journal" id="ece35767-cit-0010">
<string-name>
<surname>Clevenger</surname>, <given-names>A. P.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Sawaya</surname>, <given-names>M. A.</given-names>
</string-name> (<year>2010</year>). <article-title>Piloting a non&#x02010;invasive genetic sampling method for evaluating population&#x02010;level benefits of wildlife crossing structures</article-title>. <source xml:lang="en">Ecology and Society</source>, <volume>15</volume>(<issue>1</issue>).</mixed-citation></ref><ref id="ece35767-bib-0011"><mixed-citation publication-type="journal" id="ece35767-cit-0011">
<string-name>
<surname>Crouse</surname>, <given-names>D.</given-names>
</string-name>, <string-name>
<surname>Jacobs</surname>, <given-names>R. L.</given-names>
</string-name>, <string-name>
<surname>Richardson</surname>, <given-names>Z.</given-names>
</string-name>, <string-name>
<surname>Klum</surname>, <given-names>S.</given-names>
</string-name>, <string-name>
<surname>Jain</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Baden</surname>, <given-names>A. L.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Teco</surname>, <given-names>S. R.</given-names>
</string-name> (<year>2017</year>). <article-title>LemurFaceID: A face recognition system to facilitate individual identification of lemurs</article-title>. <source xml:lang="en">BioMed Central Zoology</source>, <volume>2</volume>, <fpage>2</fpage>
<pub-id pub-id-type="doi">10.1186/s40850-016-0011-9</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0012"><mixed-citation publication-type="journal" id="ece35767-cit-0012">
<string-name>
<surname>Fairfax</surname>, <given-names>R. J.</given-names>
</string-name>, <string-name>
<surname>MacKenzie Dowling</surname>, <given-names>R.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Neldner</surname>, <given-names>V. J.</given-names>
</string-name> (<year>2014</year>). <article-title>The use of infrared sensors and digital cameras for documenting visitor use patterns: A case study from D'Aguilar National Park, south&#x02010;east Queensland, Australia</article-title>. <source xml:lang="en">Journal of Current Issues in Tourism</source>, <volume>17</volume>(<issue>1</issue>), <fpage>72</fpage>&#x02013;<lpage>83</lpage>. <pub-id pub-id-type="doi">10.1080/13683500.2012.714749</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0013"><mixed-citation publication-type="journal" id="ece35767-cit-0013">
<string-name>
<surname>Fisher</surname>, <given-names>J. T.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Burton</surname>, <given-names>A. C.</given-names>
</string-name> (<year>2018</year>). <article-title>Wildlife winners and losers in an oil sands landscape</article-title>. <source xml:lang="en">The Ecological Survey of America</source>, <volume>16</volume>(<issue>5</issue>), <fpage>323</fpage>&#x02013;<lpage>328</lpage>. <pub-id pub-id-type="doi">10.1002/fee.1807</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0014"><mixed-citation publication-type="book" id="ece35767-cit-0014">
<string-name>
<surname>Gamma</surname>, <given-names>E.</given-names>
</string-name>, <string-name>
<surname>Helm</surname>, <given-names>R.</given-names>
</string-name>, <string-name>
<surname>Johnson</surname>, <given-names>R.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Vlissides</surname>, <given-names>J.</given-names>
</string-name> (<year>1994</year>). <source xml:lang="en">Design patterns: Elements of reusable object&#x02010;oriented software</source>. <publisher-name>Pearson</publisher-name>.</mixed-citation></ref><ref id="ece35767-bib-0015"><mixed-citation publication-type="journal" id="ece35767-cit-0015">
<string-name>
<surname>Garc&#x000ed;a&#x02010;Salgado</surname>, <given-names>G.</given-names>
</string-name>, <string-name>
<surname>Rebollo</surname>, <given-names>S.</given-names>
</string-name>, <string-name>
<surname>P&#x000e9;rezCamacho</surname>, <given-names>L.</given-names>
</string-name>, <string-name>
<surname>Mart&#x000ed;nez&#x02010;Hesterkamp</surname>, <given-names>S.</given-names>
</string-name>, <string-name>
<surname>Navarro</surname>, <given-names>A.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Fern&#x000e1;ndez&#x02010;Pereira</surname>, <given-names>J.&#x02010;M.</given-names>
</string-name> (<year>2015</year>). <article-title>Evaluation of TrailCameras for analyzing the diet of nesting raptors using the northern goshawk as a model</article-title>. <source xml:lang="en">PLoS ONE</source>, <volume>10</volume>(<issue>5</issue>), <elocation-id>e0127585</elocation-id>
<pub-id pub-id-type="doi">10.1371/journal.pone.0127585</pub-id>
<pub-id pub-id-type="pmid">25992956</pub-id></mixed-citation></ref><ref id="ece35767-bib-0016"><mixed-citation publication-type="journal" id="ece35767-cit-0016">
<string-name>
<surname>Glover&#x02010;Kapfer</surname>, <given-names>P.</given-names>
</string-name>, <string-name>
<surname>Soto&#x02010;Navarro</surname>, <given-names>C. A.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Wearn</surname>, <given-names>O. R.</given-names>
</string-name> (<year>2019</year>). <article-title>Camera&#x02010;trapping version 3.0: Current constraints and future priorities for development</article-title>. <source xml:lang="en">Remote Sensing in Ecology and Conservation</source>, <volume>5</volume>(<issue>3</issue>), <fpage>209</fpage>&#x02013;<lpage>223</lpage>. <pub-id pub-id-type="doi">10.1002/rse2.106</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0017"><mixed-citation publication-type="journal" id="ece35767-cit-0017">
<string-name>
<surname>Goswami</surname>, <given-names>V. R.</given-names>
</string-name>, <string-name>
<surname>Madhusudan</surname>, <given-names>M. D.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Ullas Karanth</surname>, <given-names>K.</given-names>
</string-name> (<year>2007</year>). <article-title>Application of photographic capture&#x02013;recapture modelling to estimate demographic parameters for male Asian elephants</article-title>. <source xml:lang="en">Animal Conservation</source>, <volume>10</volume>, <fpage>391</fpage>&#x02013;<lpage>399</lpage>. <pub-id pub-id-type="doi">10.1111/j.1469-1795.2007.00124.x</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0018"><mixed-citation publication-type="book" id="ece35767-cit-0018">
<string-name>
<surname>Greenberg</surname>, <given-names>S.</given-names>
</string-name> (<year>2019</year>). <source xml:lang="en">The Timelapse user guide version 2.2.2.4</source>. Updated versions of this manual available at Retrieved from <ext-link ext-link-type="uri" xlink:href="http://saul.cpsc.ucalgary.ca/timelapse/pmwiki.php?n=Main.UserGuide">http://saul.cpsc.ucalgary.ca/timelapse/pmwiki.php?n=Main.UserGuide</ext-link>
</mixed-citation></ref><ref id="ece35767-bib-0019"><mixed-citation publication-type="journal" id="ece35767-cit-0019">
<string-name>
<surname>Greenberg</surname>, <given-names>S.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Godin</surname>, <given-names>T.</given-names>
</string-name> (<year>2015</year>). <article-title>A tool supporting the extraction of angling effort data from remote camera image (feature article)</article-title>. <source xml:lang="en">Fisheries Magazine</source>, <volume>40</volume>(<issue>6</issue>), <fpage>276</fpage>&#x02013;<lpage>287</lpage>. American Fisheries Society, June. <pub-id pub-id-type="doi">10.1080/03632415.2015.1038380</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0020"><mixed-citation publication-type="journal" id="ece35767-cit-0020">
<string-name>
<surname>Heilbrun</surname>, <given-names>R. D.</given-names>
</string-name>, <string-name>
<surname>Silvy</surname>, <given-names>N. J.</given-names>
</string-name>, <string-name>
<surname>Peterson</surname>, <given-names>M. J.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Tewes</surname>, <given-names>M. E.</given-names>
</string-name> (<year>2006</year>). <article-title>Estimating bobcat abundance using automatically triggered cameras</article-title>. <source xml:lang="en">Wildlife Society Bulletin</source>, <volume>34</volume>, <fpage>69</fpage>&#x02013;<lpage>73</lpage>. <pub-id pub-id-type="doi">10.2193/0091-7648(2006)34(69:EBAUAT)2.0.CO;2</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0021"><mixed-citation publication-type="journal" id="ece35767-cit-0021">
<string-name>
<surname>Hossain</surname>, <given-names>A. N. M.</given-names>
</string-name>, <string-name>
<surname>Barlow</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Greenwood Barlow</surname>, <given-names>C.</given-names>
</string-name>, <string-name>
<surname>Lynam</surname>, <given-names>A. J.</given-names>
</string-name>, <string-name>
<surname>Chakma</surname>, <given-names>S.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Savini</surname>, <given-names>T.</given-names>
</string-name> (<year>2016</year>). <article-title>Assessing the efficacy of camera trapping as a tool for increasing detection rates of wildlife crime in tropical protected areas</article-title>. <source xml:lang="en">Biological Conservation</source>, <volume>201</volume>, <fpage>314</fpage>&#x02013;<lpage>319</lpage>. <pub-id pub-id-type="doi">10.1016/j.biocon.2016.07.023</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0022"><mixed-citation publication-type="journal" id="ece35767-cit-0022">
<string-name>
<surname>Ivan</surname>, <given-names>J. S.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Newkirk</surname>, <given-names>E. S.</given-names>
</string-name> (<year>2016</year>). <article-title>Cpw Photo Warehouse: A custom database to facilitate archiving, identifying, summarizing and managing photo data collected from camera traps</article-title>. <source xml:lang="en">Methods in Ecology and Evolution</source>, <volume>7</volume>(<issue>4</issue>), <fpage>499</fpage>&#x02013;<lpage>504</lpage>. <pub-id pub-id-type="doi">10.1111/2041-210X.12503</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0023"><mixed-citation publication-type="journal" id="ece35767-cit-0023">
<string-name>
<surname>Jumeau</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Petrod</surname>, <given-names>L.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Handrich</surname>, <given-names>Y.</given-names>
</string-name> (<year>2017</year>). <article-title>A comparison of camera trap and permanent recording video camera efficiency in wildlife underpasses</article-title>. <source xml:lang="en">Ecology and Evolution</source>, <volume>7</volume>(<issue>18</issue>), <fpage>7399</fpage>&#x02013;<lpage>7407</lpage>. <pub-id pub-id-type="doi">10.1002/ece3.3149</pub-id>
<pub-id pub-id-type="pmid">28944025</pub-id></mixed-citation></ref><ref id="ece35767-bib-0024"><mixed-citation publication-type="journal" id="ece35767-cit-0024">
<string-name>
<surname>Karanth</surname>, <given-names>K. U.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Nichols</surname>, <given-names>J. D.</given-names>
</string-name> (<year>1998</year>). <article-title>Estimation of tiger densities in India using photographic captures and recaptures</article-title>. <source xml:lang="en">Ecology</source>, <volume>79</volume>, <fpage>2852</fpage>&#x02013;<lpage>2862</lpage>. <pub-id pub-id-type="doi">10.1890/0012-9658(1998)079(2852:EOTDII)2.0.CO;2</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0025"><mixed-citation publication-type="journal" id="ece35767-cit-0025">
<string-name>
<surname>Karanth</surname>, <given-names>K. U.</given-names>
</string-name>, <string-name>
<surname>Nichols</surname>, <given-names>J. D.</given-names>
</string-name>, <string-name>
<surname>Samba Kumar</surname>, <given-names>N.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Hines</surname>, <given-names>J. E.</given-names>
</string-name> (<year>2006</year>). <article-title>Assessing tiger population dynamics using photographic capture&#x02010;recapture sampling</article-title>. <source xml:lang="en">Ecology</source>, <volume>87</volume>(<issue>11</issue>), <fpage>2925</fpage>&#x02013;<lpage>2937</lpage>. <pub-id pub-id-type="doi">10.1890/0012-9658(2006)87(2925:ATPDUP)2.0.CO;2</pub-id>
<pub-id pub-id-type="pmid">17168036</pub-id></mixed-citation></ref><ref id="ece35767-bib-0026"><mixed-citation publication-type="journal" id="ece35767-cit-0026">
<string-name>
<surname>Krishnappa</surname>, <given-names>Y. S.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Turner</surname>, <given-names>W. C.</given-names>
</string-name> (<year>2014</year>). <article-title>Software for minimalistic data management in large camera trap studies</article-title>. <source xml:lang="en">Ecological Information</source>, <volume>24</volume>, <fpage>11</fpage>&#x02013;<lpage>16</lpage>. <pub-id pub-id-type="doi">10.1016/j.ecoinf.2014.06.004</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0027"><mixed-citation publication-type="journal" id="ece35767-cit-0027">
<string-name>
<surname>Laskin</surname>, <given-names>D. N.</given-names>
</string-name>, <string-name>
<surname>McDermid</surname>, <given-names>G. J.</given-names>
</string-name>, <string-name>
<surname>Nielsen</surname>, <given-names>S. E.</given-names>
</string-name>, <string-name>
<surname>Marshall</surname>, <given-names>S. J.</given-names>
</string-name>, <string-name>
<surname>Roberts</surname>, <given-names>D. R.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Montaghi</surname>, <given-names>A.</given-names>
</string-name> (<year>2019</year>). <article-title>Advances in phenology are conserved across scale in present and future climates</article-title>. <source xml:lang="en">Nature Climate Change</source>, <volume>9</volume>, <fpage>419</fpage>&#x02013;<lpage>425</lpage>. <pub-id pub-id-type="doi">10.1038/s41558-019-0454-4</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0028"><mixed-citation publication-type="book" id="ece35767-cit-0028">
<collab collab-type="authors">Microsoft</collab>
(<year>2019</year>). <source xml:lang="en">AI for Earth camera trap image processing API. Github repository of its Megadetector recognizer</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="https://github.com/Microsoft/CameraTraps">https://github.com/Microsoft/CameraTraps</ext-link>
</mixed-citation></ref><ref id="ece35767-bib-0029"><mixed-citation publication-type="journal" id="ece35767-cit-0029">
<string-name>
<surname>Mills</surname>, <given-names>L. S.</given-names>
</string-name>, <string-name>
<surname>Bragina</surname>, <given-names>E. V.</given-names>
</string-name>, <string-name>
<surname>Kumar</surname>, <given-names>A. V.</given-names>
</string-name>, <string-name>
<surname>Zimova</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<surname>Lafferty</surname>, <given-names>D. J. R.</given-names>
</string-name>, <string-name>
<surname>Feltner</surname>, <given-names>J.</given-names>
</string-name>, &#x02026; <string-name>
<surname>Fay</surname>, <given-names>K.</given-names>
</string-name> (<year>2018</year>). <article-title>Winter color polymorphisms identify global hot spots for evolutionary rescue from climate change</article-title>. <source xml:lang="en">Science</source>, <volume>359</volume>(<issue>6379</issue>), <fpage>1033</fpage>&#x02013;<lpage>1066</lpage>. <pub-id pub-id-type="doi">10.1126/science.aan8097</pub-id>
<pub-id pub-id-type="pmid">29449510</pub-id></mixed-citation></ref><ref id="ece35767-bib-0030"><mixed-citation publication-type="book" id="ece35767-cit-0030">
<string-name>
<surname>Nielsen</surname>, <given-names>J.</given-names>
</string-name> (<year>1993</year>). <source xml:lang="en">Usability engineering</source>. <publisher-loc>San Diego, CA</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</mixed-citation></ref><ref id="ece35767-bib-0031"><mixed-citation publication-type="journal" id="ece35767-cit-0031">
<string-name>
<surname>Norouzzadeha</surname>, <given-names>M. S.</given-names>
</string-name>, <string-name>
<surname>Nguyenb</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Kosmalac</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<surname>Swanson</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Palmere</surname>, <given-names>M. S.</given-names>
</string-name>, <string-name>
<surname>Packere</surname>, <given-names>C.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Clunea</surname>, <given-names>J.</given-names>
</string-name> (<year>2018</year>). <article-title>Automatically identifying, counting, and describing wild animals in camera&#x02010;trap images with deep learning</article-title>. <source xml:lang="en">Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>115</volume>(<issue>25</issue>), <fpage>E5716</fpage>&#x02013;<lpage>E5725</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1719367115</pub-id>
<pub-id pub-id-type="pmid">29871948</pub-id></mixed-citation></ref><ref id="ece35767-bib-0032"><mixed-citation publication-type="book" id="ece35767-cit-0032">
<string-name>
<surname>O'Brien</surname>, <given-names>T. G.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Kinnaird</surname>, <given-names>M. F.</given-names>
</string-name> (<year>2010</year>). <chapter-title>Estimation of species richness of large vertebrates using camera traps: An example from an indonesian rainforest</chapter-title> In <person-group person-group-type="editor"><name name-style="western"><surname>O'Connell</surname><given-names>A. F.</given-names></name></person-group>, <person-group person-group-type="editor"><name name-style="western"><surname>Nichols</surname><given-names>J. D.</given-names></name></person-group>, &#x00026; <person-group person-group-type="editor"><name name-style="western"><surname>Karanth</surname><given-names>K.</given-names></name></person-group> (Eds.), <source xml:lang="en">Camera traps in animal ecology: Methods and analysis</source> (pp. <fpage>233</fpage>&#x02013;<lpage>252</lpage>). <publisher-loc>Tokyo, Japan</publisher-loc>: <publisher-name>Springer</publisher-name> ISBN 978&#x02010;4&#x02010;431&#x02010;99495&#x02010;4. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.springer.com/us/book/9784431994947">http://www.springer.com/us/book/9784431994947</ext-link>
</mixed-citation></ref><ref id="ece35767-bib-0033"><mixed-citation publication-type="journal" id="ece35767-cit-0033">
<string-name>
<surname>Oberoslerab</surname>, <given-names>V.</given-names>
</string-name>, <string-name>
<surname>Groff</surname>, <given-names>C.</given-names>
</string-name>, <string-name>
<surname>Lemma</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Pedrini</surname>, <given-names>P.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Rovero</surname>, <given-names>F.</given-names>
</string-name> (<year>2017</year>). <article-title>The influence of human disturbance on occupancy and activity patterns of mammals in the Italian Alps from systematic camera trapping</article-title>. <source xml:lang="en">Mammalian Biology</source>, <volume>87</volume>, <fpage>50</fpage>&#x02013;<lpage>61</lpage>. <pub-id pub-id-type="doi">10.1016/j.mambio.2017.05.005</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0034"><mixed-citation publication-type="journal" id="ece35767-cit-0034">
<string-name>
<surname>Pollock</surname>, <given-names>S. Z.</given-names>
</string-name>, <string-name>
<surname>Nielsen</surname>, <given-names>S. E.</given-names>
</string-name>, &#x00026; <string-name>
<surname>St. Clair</surname>, <given-names>C. C.</given-names>
</string-name> (<year>2017</year>). <article-title>A railway increases the abundance and accelerates the phenology of bear&#x02010;attracting plants in a forested, mountain park</article-title>. <source xml:lang="en">Ecosphere</source>, <volume>8</volume>(<issue>10</issue>), <elocation-id>e01985</elocation-id>
<pub-id pub-id-type="doi">10.1002/ecs2.1985</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0035"><mixed-citation publication-type="book" id="ece35767-cit-0035">
<collab collab-type="authors">Reconyx, Inc</collab>
(<year>2016</year>). <source xml:lang="en">Reconyx MapView professional software (2016) Version 3.7.2.2, including user guide</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="https://www.reconyx.com/software/mapview">https://www.reconyx.com/software/mapview</ext-link>
</mixed-citation></ref><ref id="ece35767-bib-0036"><mixed-citation publication-type="journal" id="ece35767-cit-0036">
<string-name>
<surname>Rollack</surname>, <given-names>C.</given-names>
</string-name>, <string-name>
<surname>Wiebe</surname>, <given-names>K.</given-names>
</string-name>, <string-name>
<surname>Stoffel</surname>, <given-names>M. J.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Houston</surname>, <given-names>S.</given-names>
</string-name> (<year>2013</year>). <article-title>Turkey vulture breeding behavior studied with trail cameras</article-title>. <source xml:lang="en">Journal of Raptor Research</source>, <volume>47</volume>(<issue>2</issue>), <fpage>153</fpage>&#x02013;<lpage>160</lpage>. <pub-id pub-id-type="doi">10.3356/JRR-12-40.1</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0037"><mixed-citation publication-type="journal" id="ece35767-cit-0037">
<string-name>
<surname>Rowcliffe</surname>, <given-names>J. M.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Carbone</surname>, <given-names>C.</given-names>
</string-name> (<year>2008</year>). <article-title>Surveys using camera traps: Are we looking to a brighter future?</article-title>
<source xml:lang="en">Animal Conservation</source>, <volume>11</volume>(<issue>3</issue>), <fpage>185</fpage>&#x02013;<lpage>186</lpage>. <pub-id pub-id-type="doi">10.1111/j.1469-1795.2008.00180.x</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0038"><mixed-citation publication-type="journal" id="ece35767-cit-0038">
<string-name>
<surname>Royle</surname>, <given-names>J. A.</given-names>
</string-name>, <string-name>
<surname>Fuller</surname>, <given-names>A. K.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Sutherland</surname>, <given-names>C.</given-names>
</string-name> (<year>2018</year>). <article-title>Unifying population and landscape ecology with spatial capture&#x02010;recapture</article-title>. <source xml:lang="en">Ecography</source>, <volume>41</volume>, <fpage>444</fpage>&#x02013;<lpage>456</lpage>. <pub-id pub-id-type="doi">10.1111/ecog.03170</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0039"><mixed-citation publication-type="book" id="ece35767-cit-0039">
<string-name>
<surname>Schneider</surname>, <given-names>S.</given-names>
</string-name>, <string-name>
<surname>Taylor</surname>, <given-names>G. W.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Kramer</surname>, <given-names>S. C.</given-names>
</string-name> (<year>2018</year>). <chapter-title>Deep learning object detection methods for ecological camera trap data</chapter-title> In <source xml:lang="en">Proceedings IEEE 15th Conference on Computer and Robot Vision</source> (pp. <fpage>321</fpage>&#x02013;<lpage>328</lpage>). May 8&#x02013;10. <publisher-loc>Toronto, ON, Canada</publisher-loc>.</mixed-citation></ref><ref id="ece35767-bib-0040"><mixed-citation publication-type="journal" id="ece35767-cit-0040">
<string-name>
<surname>Scotson</surname>, <given-names>L.</given-names>
</string-name>, <string-name>
<surname>Johnston</surname>, <given-names>L. R.</given-names>
</string-name>, <string-name>
<surname>Iannarilli</surname>, <given-names>F.</given-names>
</string-name>, <string-name>
<surname>Wearn</surname>, <given-names>O. R.</given-names>
</string-name>, <string-name>
<surname>Mohd&#x02010;Azlan</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Wong</surname>, <given-names>W.</given-names>
</string-name>, &#x02026; <string-name>
<surname>Fieberg</surname>, <given-names>J.</given-names>
</string-name> (<year>2017</year>). <article-title>Best practices and software for the management and sharing of camera trap data for small and large scales studies</article-title>. <source xml:lang="en">Remote Sensing in Ecology and Conservation</source>, <volume>3</volume>, <fpage>158</fpage>&#x02013;<lpage>172</lpage>. <pub-id pub-id-type="doi">10.1002/rse2.54</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0041"><mixed-citation publication-type="book" id="ece35767-cit-0041">
<string-name>
<surname>Shneiderman</surname>, <given-names>B.</given-names>
</string-name>, <string-name>
<surname>Plaisant</surname>, <given-names>C.</given-names>
</string-name>, <string-name>
<surname>Cohen</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<surname>Jacobs</surname>, <given-names>S.</given-names>
</string-name>, <string-name>
<surname>Elmqvist</surname>, <given-names>N.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Diakopoulos</surname>, <given-names>N.</given-names>
</string-name> (<year>2016</year>). <source xml:lang="en">Designing the user interface: Strategies for effective human&#x02010;computer interaction</source> (<edition>6th ed.</edition>). <publisher-loc>Essex, England</publisher-loc>: <publisher-name>Pearson</publisher-name>.</mixed-citation></ref><ref id="ece35767-bib-0042"><mixed-citation publication-type="book" id="ece35767-cit-0042">
<string-name>
<surname>Spence</surname>, <given-names>R.</given-names>
</string-name> (<year>2014</year>). <source xml:lang="en">Information visualization: An introduction</source> (<edition>3rd ed.</edition>). <publisher-loc>Heidelberg, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>.</mixed-citation></ref><ref id="ece35767-bib-0043"><mixed-citation publication-type="journal" id="ece35767-cit-0043">
<string-name>
<surname>Steenweg</surname>, <given-names>R.</given-names>
</string-name>, <string-name>
<surname>Hebblewhite</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<surname>Kays</surname>, <given-names>R.</given-names>
</string-name>, <string-name>
<surname>Ahumada</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Fisher</surname>, <given-names>J. T.</given-names>
</string-name>, <string-name>
<surname>Burton</surname>, <given-names>C.</given-names>
</string-name>, &#x02026; <string-name>
<surname>Rich</surname>, <given-names>L. N.</given-names>
</string-name> (<year>2017</year>). <article-title>Scaling&#x02010;up camera traps: Monitoring the planet's biodiversity with networks of remote sensors</article-title>. <source xml:lang="en">Frontiers in Ecology and the Environment</source>, <volume>15</volume>(<issue>1</issue>), <fpage>26</fpage>&#x02013;<lpage>34</lpage>. <pub-id pub-id-type="doi">10.1002/fee.1448</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0044"><mixed-citation publication-type="book" id="ece35767-cit-0044">
<string-name>
<surname>Swann</surname>, <given-names>D. E.</given-names>
</string-name>, <string-name>
<surname>Kawanishi</surname>, <given-names>K.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Palmer</surname>, <given-names>J.</given-names>
</string-name> (<year>2010</year>). <chapter-title>Evaluating types and features of camera traps in ecological studies: A guide for researchers</chapter-title> In <person-group person-group-type="editor"><name name-style="western"><surname>O'Connell</surname><given-names>A. F.</given-names></name></person-group>, <person-group person-group-type="editor"><name name-style="western"><surname>Nichols</surname><given-names>J. D.</given-names></name></person-group>, &#x00026; <person-group person-group-type="editor"><name name-style="western"><surname>Karanth</surname><given-names>K.</given-names></name></person-group> (Eds.), <source xml:lang="en">Camera traps in animal ecology: Methods and analysis</source> (pp. <fpage>27</fpage>&#x02013;<lpage>43</lpage>). <publisher-loc>Tokyo, Japan</publisher-loc>: <publisher-name>Springer</publisher-name> ISBN 978&#x02010;4&#x02010;431&#x02010;99495&#x02010;4. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.springer.com/us/book/9784431994947">http://www.springer.com/us/book/9784431994947</ext-link>
</mixed-citation></ref><ref id="ece35767-bib-0045"><mixed-citation publication-type="journal" id="ece35767-cit-0045">
<string-name>
<surname>Swanson</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Kosmala</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<surname>Lintott</surname>, <given-names>C.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Packer</surname>, <given-names>C.</given-names>
</string-name> (<year>2016</year>). <article-title>A generalized approach for producing, quantifying, and validating citizen science data from wildlife images</article-title>. <source xml:lang="en">Conservation Biology</source>, <volume>30</volume>(<issue>3</issue>), <fpage>520</fpage>&#x02013;<lpage>531</lpage>. <pub-id pub-id-type="doi">10.1111/cobi.12695</pub-id>
<pub-id pub-id-type="pmid">27111678</pub-id></mixed-citation></ref><ref id="ece35767-bib-0046"><mixed-citation publication-type="journal" id="ece35767-cit-0046">
<string-name>
<surname>Swanson</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Kosmala</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<surname>Lintott</surname>, <given-names>C.</given-names>
</string-name>, <string-name>
<surname>Simpson</surname>, <given-names>R.</given-names>
</string-name>, <string-name>
<surname>Smith</surname>, <given-names>A.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Packer</surname>, <given-names>C.</given-names>
</string-name> (<year>2015</year>). <article-title>Snapshot serengeti, high frequency annotated camera trap images of 40 mammalian species in an African Savanna</article-title>. <source xml:lang="en">Scientific Data</source>, <volume>2</volume>, <fpage>150026</fpage>
<pub-id pub-id-type="doi">10.1038/sdata.2015.26</pub-id>
<pub-id pub-id-type="pmid">26097743</pub-id></mixed-citation></ref><ref id="ece35767-bib-0047"><mixed-citation publication-type="journal" id="ece35767-cit-0047">
<string-name>
<surname>Tabak</surname>, <given-names>M. A.</given-names>
</string-name>, <string-name>
<surname>Norouzzadeh</surname>, <given-names>M. S.</given-names>
</string-name>, <string-name>
<surname>Wolfson</surname>, <given-names>D. W.</given-names>
</string-name>, <string-name>
<surname>Sweeney</surname>, <given-names>S. J.</given-names>
</string-name>, <string-name>
<surname>Vercauteren</surname>, <given-names>K. C.</given-names>
</string-name>, <string-name>
<surname>Snow</surname>, <given-names>N. P.</given-names>
</string-name>, &#x02026; <string-name>
<surname>Miller</surname>, <given-names>R. S.</given-names>
</string-name> (<year>2018</year>). <article-title>Machine learning to classify animal species in camera trap images: Applications in Ecology</article-title>. <source xml:lang="en">Methods in Ecology and Evolution</source>, <volume>10</volume>(<issue>4</issue>), <fpage>585</fpage>&#x02013;<lpage>590</lpage>. <pub-id pub-id-type="doi">10.1111/2041-210X.13120</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0048"><mixed-citation publication-type="journal" id="ece35767-cit-0048">
<string-name>
<surname>Tobler</surname>, <given-names>M. W.</given-names>
</string-name>, <string-name>
<surname>Z&#x000fa;&#x000f1;iga Hartley</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Carrillo&#x02010;Percastegui</surname>, <given-names>S. E.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Powell</surname>, <given-names>G. V.</given-names>
</string-name> (<year>2015</year>). <article-title>Spatiotemporal hierarchical modelling of species richness and occupancy using camera trap data</article-title>. <source xml:lang="en">Journal of Applied Ecology</source>, <volume>52</volume>, <fpage>413</fpage>&#x02013;<lpage>421</lpage>. <pub-id pub-id-type="doi">10.1111/1365-2664.12399</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0049"><mixed-citation publication-type="book" id="ece35767-cit-0049">
<string-name>
<surname>Ware</surname>, <given-names>C.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Lewis</surname>, <given-names>M.</given-names>
</string-name> (<year>1995</year>). <source xml:lang="en">The DragMag image magnifier. Chi '95 Video Program</source>. From ACM Conference on Human Factors in Computing System, May 7&#x02013;11. <publisher-loc>Denver, CO</publisher-loc>: <publisher-name>ACM Press</publisher-name>. Videotape.</mixed-citation></ref><ref id="ece35767-bib-0050"><mixed-citation publication-type="book" id="ece35767-cit-0050">
<string-name>
<surname>Wearn</surname>, <given-names>O. R.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Glover&#x02010;Kapfer</surname>, <given-names>P. G.</given-names>
</string-name> (<year>2017</year>). <source xml:lang="en">Camera&#x02010;trapping for conservation: A guide to best&#x02010;practices</source>. WWF Conservation Technology Series 1(1). <publisher-name>WWF&#x02010;UK</publisher-name>, <publisher-loc>Woking, UK</publisher-loc> Retrieved from <ext-link ext-link-type="uri" xlink:href="https://www.wwf.org.uk/conservationtechnology/documents/CameraTraps-WWF-guidelines.pdf">https://www.wwf.org.uk/conservationtechnology/documents/CameraTraps-WWF-guidelines.pdf</ext-link>
</mixed-citation></ref><ref id="ece35767-bib-0051"><mixed-citation publication-type="journal" id="ece35767-cit-0051">
<string-name>
<surname>Whittington</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Hebblewhite</surname>, <given-names>M.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Chandler</surname>, <given-names>R. B.</given-names>
</string-name> (<year>2018</year>). <article-title>Generalized spatial mark&#x02010;resight models with an application to grizzly bears</article-title>. <source xml:lang="en">Journal of Applied Ecology</source>, <volume>55</volume>, <fpage>157</fpage>&#x02013;<lpage>168</lpage>. <pub-id pub-id-type="doi">10.1111/1365-2664.12954</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0052"><mixed-citation publication-type="journal" id="ece35767-cit-0052">
<string-name>
<surname>Whittington</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Low</surname>, <given-names>P.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Hunt</surname>, <given-names>B.</given-names>
</string-name> (<year>2019</year>) <article-title>Temporal road closures improve habitat quality for wildlife</article-title>. <source xml:lang="en">Nature: Scientific Reports</source>, <volume>9</volume>, <fpage>3772</fpage>
<pub-id pub-id-type="doi">10.1038/s41598-019-40581-y</pub-id>
</mixed-citation></ref><ref id="ece35767-bib-0053"><mixed-citation publication-type="book" id="ece35767-cit-0053">
<collab collab-type="authors">WildTrax</collab>
(<year>2019</year>). <source xml:lang="en">Website of the Alberta Biodiversity Monitoring Institute</source>. <publisher-name>Biological Sciences Centre, University of Alberta</publisher-name>, <publisher-loc>Edmonton, AB, Canada</publisher-loc> Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.wildtrax.ca/home.html">http://www.wildtrax.ca/home.html</ext-link>
</mixed-citation></ref><ref id="ece35767-bib-0054"><mixed-citation publication-type="journal" id="ece35767-cit-0054">
<string-name>
<surname>Young</surname>, <given-names>S.</given-names>
</string-name>, <string-name>
<surname>Rode&#x02010;Margono</surname>, <given-names>J.</given-names>
</string-name>, &#x00026; <string-name>
<surname>Amin</surname>, <given-names>R.</given-names>
</string-name> (<year>2018</year>). <article-title>Software to facilitate and streamline camera trap data management: A review</article-title>. <source xml:lang="en">Ecology and Evolution</source>, <volume>8</volume>(<issue>19</issue>), <fpage>9947</fpage>&#x02013;<lpage>9957</lpage>. <pub-id pub-id-type="doi">10.1002/ece3.4464</pub-id>
<pub-id pub-id-type="pmid">30386588</pub-id></mixed-citation></ref><ref id="ece35767-bib-0055"><mixed-citation publication-type="journal" id="ece35767-cit-0055">
<string-name>
<surname>Yousif</surname>, <given-names>H.</given-names>
</string-name>, <string-name>
<surname>Yuan</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Kays</surname>, <given-names>R.</given-names>
</string-name>, &#x00026; <string-name>
<surname>He</surname>, <given-names>Z.</given-names>
</string-name> (<year>2019</year>). <article-title>Animal Scanner: Software for classifying humans, animals, and empty frames in camera trap images</article-title>. <source xml:lang="en">Ecology and Evolution</source>, <volume>9</volume>(<issue>4</issue>), <fpage>1578</fpage>&#x02013;<lpage>1589</lpage>. <pub-id pub-id-type="doi">10.1002/ece3.4747</pub-id>
<pub-id pub-id-type="pmid">30847057</pub-id></mixed-citation></ref></ref-list></back></article>