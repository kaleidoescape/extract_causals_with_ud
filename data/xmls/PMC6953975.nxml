<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Int J Comput Vis</journal-id><journal-id journal-id-type="iso-abbrev">Int J Comput Vis</journal-id><journal-title-group><journal-title>International Journal of Computer Vision</journal-title></journal-title-group><issn pub-type="ppub">0920-5691</issn><publisher><publisher-name>Springer US</publisher-name><publisher-loc>New York</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">31983805</article-id><article-id pub-id-type="pmc">6953975</article-id><article-id pub-id-type="publisher-id">999</article-id><article-id pub-id-type="doi">10.1007/s11263-017-0999-5</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Comprehensive Performance Evaluation of Deformable Face Tracking &#x0201c;In-the-Wild&#x0201d;</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0650-1856</contrib-id><name><surname>Chrysos</surname><given-names>Grigorios G.</given-names></name><address><email>g.chrysos@imperial.ac.uk</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Antonakos</surname><given-names>Epameinondas</given-names></name><address><email>e.antonakos@imperial.ac.uk</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Snape</surname><given-names>Patrick</given-names></name><address><email>p.snape@imperial.ac.uk</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Asthana</surname><given-names>Akshay</given-names></name><address><email>akshay.asthana@seeingmachines.com</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Zafeiriou</surname><given-names>Stefanos</given-names></name><address><email>s.zafeiriou@imperial.ac.uk</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2113 8111</institution-id><institution-id institution-id-type="GRID">grid.7445.2</institution-id><institution>Department of Computing, </institution><institution>Imperial College London, </institution></institution-wrap>180 Queen&#x02019;s Gate, London, SW7 2AZ UK </aff><aff id="Aff2"><label>2</label>Seeing Machines Ltd., Level 1, 11 Lonsdale St, Braddon, ACT 2612 Australia </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0941 4873</institution-id><institution-id institution-id-type="GRID">grid.10858.34</institution-id><institution>Center for Machine Vision and Signal Analysis, </institution><institution>University of Oulu, </institution></institution-wrap>Oulu, Finland </aff></contrib-group><author-notes><fn fn-type="com"><p>Communicated by Lourdes Agapito.</p></fn></author-notes><pub-date pub-type="epub"><day>25</day><month>2</month><year>2017</year></pub-date><pub-date pub-type="pmc-release"><day>25</day><month>2</month><year>2017</year></pub-date><pub-date pub-type="ppub"><year>2018</year></pub-date><volume>126</volume><issue>2</issue><fpage>198</fpage><lpage>232</lpage><history><date date-type="received"><day>15</day><month>3</month><year>2016</year></date><date date-type="accepted"><day>10</day><month>2</month><year>2017</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2017</copyright-statement><license license-type="OpenAccess"><license-p>
<bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0">http://creativecommons.org/licenses/by/4.0</ext-link>/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Recently, technologies such as face detection, facial landmark localisation and face recognition and verification have matured enough to provide effective and efficient solutions for imagery captured under arbitrary conditions (referred to as &#x0201c;in-the-wild&#x0201d;). This is partially attributed to the fact that comprehensive &#x0201c;in-the-wild&#x0201d; benchmarks have been developed for face detection, landmark localisation and recognition/verification. A very important technology that has not been thoroughly evaluated yet is deformable face tracking &#x0201c;in-the-wild&#x0201d;. Until now, the performance has mainly been assessed qualitatively by visually assessing the result of a deformable face tracking technology on short videos. In this paper, we perform the first, to the best of our knowledge, thorough evaluation of state-of-the-art deformable face tracking pipelines using the recently introduced 300&#x000a0;VW benchmark. We evaluate many different architectures focusing mainly on the task of on-line deformable face tracking. In particular, we compare the following general strategies: (a)&#x000a0;generic face detection plus generic facial landmark localisation, (b)&#x000a0;generic model free tracking plus generic facial landmark localisation, as well as (c)&#x000a0;hybrid approaches using state-of-the-art face detection, model free tracking and facial landmark localisation technologies. Our evaluation reveals future avenues for further research on the topic.</p><sec><title>Electronic supplementary material</title><p>The online version of this article (doi:10.1007/s11263-017-0999-5) contains supplementary material, which is available to authorized users.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Deformable face tracking</kwd><kwd>Face detection</kwd><kwd>Model free tracking</kwd><kwd>Facial landmark localisation</kwd><kwd>Long-term tracking</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/J017787/1</award-id><principal-award-recipient><name><surname>Chrysos</surname><given-names>Grigorios G.</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>Engineering and Physical Sciences Research Council (GB)</institution></funding-source><award-id>EP/L026813/1</award-id><principal-award-recipient><name><surname>Zafeiriou</surname><given-names>Stefanos</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Science+Business Media, LLC, part of Springer Nature 2018</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">The human face is arguably among the most well-studied deformable objects in the field of Computer Vision. This is due to the many roles it has in numerous applications. For example, accurate detection of faces is an essential step for tasks such as controller-free gaming, surveillance, digital photo album organization, image tagging, etc. Additionally, detection of facial features plays a crucial role for facial behaviour analysis, facial attributes analysis (e.g., gender and age recognition, etc.), facial image editing (e.g., digital make-up, etc.), surveillance, sign language recognition, lip reading, human-computer and human-robot interaction. In this work, we study the deformable face tracking task and we develop the first, to the best of our knowledge, comprehensive evaluation of multiple deformable face tracking pipelines.</p><p id="Par3">Current research has been monopolised by the tasks of <italic>face detection</italic>, <italic>facial landmark localisation</italic> and <italic>face recognition or verification</italic>. Firstly, face detection, despite having permeated many forms of modern technology such as digital cameras and social networking, is still a challenging problem and a popular line of research, as shown by the recent surveys of Jain and Learned-Miller (<xref ref-type="bibr" rid="CR78">2010</xref>), Zhang and Zhang (<xref ref-type="bibr" rid="CR218">2010</xref>), Zafeiriou et&#x000a0;al. (<xref ref-type="bibr" rid="CR217">2015</xref>). Although face detection on well-lit frontal facial images can be performed reliably on an embedded device, face detection on arbitrary images of people is still extremely challenging (Jain and Learned-Miller <xref ref-type="bibr" rid="CR78">2010</xref>). Images of faces under these unconstrained conditions are commonly referred to as &#x0201c;in-the-wild&#x0201d; and may include scenarios such as extreme facial pose, defocus, faces occupying a very small number of pixels or occlusions. Given the fact that face detection is still regarded as a challenging task, many generic object detection architectures such as Yan et&#x000a0;al. (<xref ref-type="bibr" rid="CR209">2014</xref>), King (<xref ref-type="bibr" rid="CR89">2015</xref>) are either directly assessed on in-the-wild facial data, or are appropriately modified in order to explicitly perform face detection as done by Zhu and Ramanan (<xref ref-type="bibr" rid="CR230">2012</xref>), Felzenszwalb and Huttenlocher (<xref ref-type="bibr" rid="CR60">2005</xref>). The interested reader may refer to the most recent survey by Zafeiriou et&#x000a0;al. (<xref ref-type="bibr" rid="CR217">2015</xref>) for more information on in-the-wild face detection. The problem of localising facial landmarks that correspond to fiducial facial parts (e.g., eyes, mouth, etc.) is still extremely challenging and has only been possible to perform reliably relatively recently. Although the history of facial landmark localisation spans back many decades (Cootes et&#x000a0;al. <xref ref-type="bibr" rid="CR40">1995</xref>, <xref ref-type="bibr" rid="CR41">2001</xref>), the ability to accurately recover facial landmarks on in-the-wild images has only become possible in recent years (Matthews and Baker <xref ref-type="bibr" rid="CR128">2004</xref>; Papandreou and Maragos <xref ref-type="bibr" rid="CR139">2008</xref>; Saragih et&#x000a0;al. <xref ref-type="bibr" rid="CR165">2011</xref>; Cao et&#x000a0;al. <xref ref-type="bibr" rid="CR33">2014</xref>). Much of this progress can be attributed to the release of large annotated datasets of facial landmarks (Sagonas et&#x000a0;al. <xref ref-type="bibr" rid="CR160">2013b</xref>, <xref ref-type="bibr" rid="CR159">a</xref>; Zhu and Ramanan <xref ref-type="bibr" rid="CR230">2012</xref>; Le et&#x000a0;al. <xref ref-type="bibr" rid="CR103">2012</xref>; Belhumeur et&#x000a0;al. <xref ref-type="bibr" rid="CR20">2013</xref>; K&#x000f6;stinger et&#x000a0;al. <xref ref-type="bibr" rid="CR93">2011</xref>) and very recently the area of facial landmark localisation has become extremely competitive with recent works including Xiong and De&#x000a0;la Torre (<xref ref-type="bibr" rid="CR205">2013</xref>), Ren et&#x000a0;al. (<xref ref-type="bibr" rid="CR154">2014</xref>), Kazemi and Sullivan (<xref ref-type="bibr" rid="CR86">2014</xref>), Zhu et&#x000a0;al. (<xref ref-type="bibr" rid="CR229">2015</xref>), Tzimiropoulos (<xref ref-type="bibr" rid="CR182">2015</xref>). For a recent evaluation of facial landmark localisation methods the interested reader may refer to the survey by Wang et&#x000a0;al. (<xref ref-type="bibr" rid="CR191">2014</xref>) and to the results of the 300&#x000a0;W competition by Sagonas et&#x000a0;al. (<xref ref-type="bibr" rid="CR162">2015</xref>). Finally, face recognition and verification are extremely popular lines of research. For the past two decades, the majority of statistical machine learning algorithms spanning from linear/non-linear subspace learning techniques (De&#x000a0;la Torre <xref ref-type="bibr" rid="CR49">2012</xref>; Kokiopoulou et&#x000a0;al. <xref ref-type="bibr" rid="CR92">2011</xref>) to deep convolutional neural networks (DCNNs) (Taigman et&#x000a0;al. <xref ref-type="bibr" rid="CR178">2014</xref>; Schroff et&#x000a0;al. <xref ref-type="bibr" rid="CR167">2015</xref>; Parkhi et&#x000a0;al. <xref ref-type="bibr" rid="CR140">2015</xref>) have been applied to the problem of face recognition and verification. Recently, due to the revival of DCNNs, as well as the development of graphics processing units (GPUs), remarkable face verification performance has been reported (Taigman et&#x000a0;al. <xref ref-type="bibr" rid="CR178">2014</xref>). The interested reader may refer to the recent survey by Learned-Miller et&#x000a0;al. (<xref ref-type="bibr" rid="CR104">2016</xref>) as well as the most popular benchmark for face verification in-the-wild in Huang et&#x000a0;al. (<xref ref-type="bibr" rid="CR75">2007</xref>).</p><p id="Par4">In all of the aforementioned fields, significant progress has been reported in recent years. The primary reasons behind these advances are:<list list-type="bullet"><list-item><p id="Par5">
<italic>The collection and annotation of large databases</italic> Given the abundance of facial images available primarily through the Internet via services such as Flickr, Google Images and Facebook, the collection of facial images is extremely simple. Some examples of large databases for face detection are FDDB (Jain and Learned-Miller <xref ref-type="bibr" rid="CR78">2010</xref>), AFW (Zhu and Ramanan <xref ref-type="bibr" rid="CR230">2012</xref>) and LFW (Huang et&#x000a0;al. <xref ref-type="bibr" rid="CR75">2007</xref>). Similar large-scale databases for facial landmark localisation include 300&#x000a0;W (Sagonas et&#x000a0;al. <xref ref-type="bibr" rid="CR160">2013b</xref>) LFPW (Belhumeur et&#x000a0;al. <xref ref-type="bibr" rid="CR20">2013</xref>), AFLW (K&#x000f6;stinger et&#x000a0;al. <xref ref-type="bibr" rid="CR93">2011</xref>) and HELEN (Le et&#x000a0;al. <xref ref-type="bibr" rid="CR103">2012</xref>). Similarly, for face recognition there exists LFW (Huang et&#x000a0;al. <xref ref-type="bibr" rid="CR75">2007</xref>), FRVT (Phillips et&#x000a0;al. <xref ref-type="bibr" rid="CR146">2000</xref>) and the recently introduced Janus database (IJB-A) (Klare et&#x000a0;al. <xref ref-type="bibr" rid="CR90">2015</xref>).</p></list-item><list-item><p id="Par6">
<italic>The establishment of in-the-wild benchmarks and challenges</italic> that provide a fair comparison between state of the art techniques. FDDB (Jain and Learned-Miller <xref ref-type="bibr" rid="CR78">2010</xref>), 300&#x000a0;W (Sagonas et&#x000a0;al. <xref ref-type="bibr" rid="CR159">2013a</xref>, <xref ref-type="bibr" rid="CR162">2015</xref>) and Janus (Klare et&#x000a0;al. <xref ref-type="bibr" rid="CR90">2015</xref>) are the most characteristic examples for face detection, facial landmark localisation and face recognition, respectively.</p></list-item></list>Contrary to face detection, facial landmark localisation and face recognition, the problem of <italic>deformable face tracking</italic> across long-term sequences has yet to attract much attention, despite its crucial role in numerous applications. Given the fact that cameras are embedded in many common electronic devices, it is surprising that current research has not yet focused towards providing robust and accurate solutions for long-term deformable tracking. Almost all face-based applications, including facial behaviour analysis, lip reading, surveillance, human-computer and human-robot interaction etc., require accurate <italic>continuous tracking</italic> of the facial landmarks. The facial landmarks are commonly used as input signals of higher-level methodologies to compute motion dynamics and deformations. The performance of currently available technologies for facial deformable tracking has not been properly assessed (Yacoob and Davis <xref ref-type="bibr" rid="CR207">1996</xref>; Essa et&#x000a0;al. <xref ref-type="bibr" rid="CR56">1996</xref>, <xref ref-type="bibr" rid="CR57">1997</xref>; Decarlo and Metaxas <xref ref-type="bibr" rid="CR47">2000</xref>; Koelstra et&#x000a0;al. <xref ref-type="bibr" rid="CR91">2010</xref>; Snape et&#x000a0;al. <xref ref-type="bibr" rid="CR173">2015</xref>). This is attributed to the fact that, until recently, there was no established benchmark for the task. At ICCV 2015, the first benchmark for facial landmark tracking (so-called 300&#x000a0;VW) was presented by Shen et&#x000a0;al. (<xref ref-type="bibr" rid="CR170">2015</xref>), providing a large number of annotated videos captured in-the-wild.<xref ref-type="fn" rid="Fn1">1</xref> In particular, the benchmark provides 114 videos with average duration around 1 minute, split into three categories of increasing difficulty. The frames of all videos (218595 in total) were annotated by applying semi-automatic procedures, as shown in Chrysos et&#x000a0;al. (<xref ref-type="bibr" rid="CR35">2015</xref>). Five different facial tracking methodologies were evaluated in the benchmark (Rajamanoharan and Cootes <xref ref-type="bibr" rid="CR151">2015</xref>; Yang et&#x000a0;al. <xref ref-type="bibr" rid="CR213">2015a</xref>; Wu and Ji <xref ref-type="bibr" rid="CR198">2015</xref>; Uricar and Franc <xref ref-type="bibr" rid="CR187">2015</xref>; Xiao et&#x000a0;al. <xref ref-type="bibr" rid="CR203">2015</xref>) and the results are indicative of the current state-of-the-art performance.</p><p id="Par8">In this paper, we make a significant step further and develop the first, to the best of our knowledge, comprehensive evaluation of multiple deformable face tracking pipelines. In particular, we assess:<list list-type="bullet"><list-item><p id="Par9">A pipeline which combines a generic face detection algorithm with a facial landmark localisation method. This pipeline is typically assumed in the related tracking papers, e.g. Wolf et&#x000a0;al. (<xref ref-type="bibr" rid="CR196">2011</xref>), Best-Rowden et&#x000a0;al. (<xref ref-type="bibr" rid="CR23">2013</xref>), Chrysos et&#x000a0;al. (<xref ref-type="bibr" rid="CR35">2015</xref>), as well as in various implementations that are (publicly) available, e.g. King (<xref ref-type="bibr" rid="CR88">2009</xref>), Asthana et&#x000a0;al. (<xref ref-type="bibr" rid="CR12">2014</xref>), Chrysos et&#x000a0;al. (<xref ref-type="bibr" rid="CR35">2015</xref>), and the demos given in various conferences. The pipeline is fairly robust since the probability of drifting is reduced due to the application of the face detector at each frame. Nevertheless, it does not exploit the dynamic characteristics of the tracked face. Several state-of-the-art face detectors as well as facial landmark localisation methodologies are evaluated in this pipeline.</p></list-item><list-item><p id="Par10">A pipeline which combines a model free tracking system with a facial landmark localisation method. This approach takes into account the dynamic nature of the tracked face, but is susceptible to drifting and thus losing the tracked object. We evaluate the combinations of multiple state-of-the-art model free trackers, as well as landmark localisation techniques.</p></list-item><list-item><p id="Par11">Hybrid pipelines that include mechanisms for detecting tracking failures and performing re-initialisation, as well as using models for ensuring robust tracking.</p></list-item></list>Some of the above pipelines were used extensively by practitioners, especially the first one. Nevertheless, to the best of our knowledge, this is the first paper that explicitly refers to the various alternatives and provides a thorough examination of the different components of the pipelines (i.e., detectors, trackers, smoothing, landmark localisation etc.).</p><p id="Par12">Summarising, the findings of our evaluation show that current face detection and model free tracking technologies are advanced enough so that even a naive combination with landmark localisation techniques is adequate to achieve state-of-the-art performance on deformable face tracking. Specifically, we experimentally show that model free tracking based pipelines are very accurate when applied on videos with moderate lighting and pose circumstances. Furthermore, the combination of state-of-the-art face detectors with landmark localisation systems demonstrates excellent performance with surprisingly high true positive rate on videos captured under arbitrary conditions (extreme lighting, pose, occlusions, etc.). Moreover, we show that hybrid approaches provide only a marginal improvement, which is not worth their complexity and computational cost. Finally, we compare these approaches with the systems that participated in the 300&#x000a0;VW competition of Shen et&#x000a0;al. (<xref ref-type="bibr" rid="CR170">2015</xref>).</p><p id="Par13">The rest of the paper is organised as follows. Sect.&#x000a0;<xref rid="Sec2" ref-type="sec">2</xref> presents a survey of the current literature on both rigid and deformable face tracking. In Sect.&#x000a0;<xref rid="Sec5" ref-type="sec">3</xref>, we present the current state-of-the-art methodologies for deformable face tracking. Since, modern face tracking consists of various modules, including face detection, model free tracking and facial landmark localisation, Sects.&#x000a0;<xref rid="Sec6" ref-type="sec">3.1</xref>&#x02013;<xref rid="Sec8" ref-type="sec">3.3</xref> briefly outline the state-of-the-art in each of these domains. Experimental results are presented in Sect.&#x000a0;<xref rid="Sec9" ref-type="sec">4</xref>. Finally, in Sect.&#x000a0;<xref rid="Sec20" ref-type="sec">5</xref> we discuss the challenges that still remain to be addressed, provide future research directions and draw conclusions.</p></sec><sec id="Sec2"><title>Related Work</title><p id="Par14">Rigid and deformable tracking of faces and facial features have been a very popular topic of research over the past twenty years (Black and Yacoob <xref ref-type="bibr" rid="CR25">1995</xref>; Lanitis et&#x000a0;al. <xref ref-type="bibr" rid="CR102">1995</xref>; Sobottka and Pitas <xref ref-type="bibr" rid="CR174">1996</xref>; Essa et&#x000a0;al. <xref ref-type="bibr" rid="CR56">1996</xref>, <xref ref-type="bibr" rid="CR57">1997</xref>; Oliver et&#x000a0;al. <xref ref-type="bibr" rid="CR136">1997</xref>; Decarlo and Metaxas <xref ref-type="bibr" rid="CR47">2000</xref>; Jepson et&#x000a0;al. <xref ref-type="bibr" rid="CR79">2003</xref>; Matthews and Baker <xref ref-type="bibr" rid="CR128">2004</xref>; Matthews et&#x000a0;al. <xref ref-type="bibr" rid="CR129">2004</xref>; Xiao et&#x000a0;al. <xref ref-type="bibr" rid="CR202">2004</xref>; Patras and Pantic <xref ref-type="bibr" rid="CR141">2004</xref>; Kim et&#x000a0;al. <xref ref-type="bibr" rid="CR87">2008</xref>; Ross et&#x000a0;al. <xref ref-type="bibr" rid="CR157">2008</xref>; Papandreou and Maragos <xref ref-type="bibr" rid="CR139">2008</xref>; Amberg et&#x000a0;al. <xref ref-type="bibr" rid="CR7">2009</xref>; Kalal et&#x000a0;al. <xref ref-type="bibr" rid="CR82">2010a</xref>; Koelstra et&#x000a0;al. <xref ref-type="bibr" rid="CR91">2010</xref>; Tresadern et&#x000a0;al. <xref ref-type="bibr" rid="CR181">2012</xref>; Tzimiropoulos and Pantic <xref ref-type="bibr" rid="CR183">2013</xref>; Xiong and De&#x000a0;la Torre <xref ref-type="bibr" rid="CR205">2013</xref>; Liwicki et&#x000a0;al. <xref ref-type="bibr" rid="CR121">2013</xref>; Smeulders et&#x000a0;al. <xref ref-type="bibr" rid="CR172">2014</xref>; Asthana et&#x000a0;al. <xref ref-type="bibr" rid="CR12">2014</xref>; Tzimiropoulos and Pantic <xref ref-type="bibr" rid="CR184">2014</xref>; Li et&#x000a0;al. <xref ref-type="bibr" rid="CR106">2016a</xref>; Xiong and De&#x000a0;la Torre <xref ref-type="bibr" rid="CR206">2015</xref>; Snape et&#x000a0;al. <xref ref-type="bibr" rid="CR173">2015</xref>; Wu et&#x000a0;al. <xref ref-type="bibr" rid="CR201">2015</xref>; Tzimiropoulos <xref ref-type="bibr" rid="CR182">2015</xref>). In this section we provide an overview of face tracking spanning over the past twenty years up to the present day. In particular, we will outline the methodologies regarding rigid 2D/3D face tracking, as well as deformable 2D/3D face tracking using a monocular camera.<xref ref-type="fn" rid="Fn2">2</xref> Finally, we outline the benchmarks for both rigid and deformable face tracking.</p><sec id="Sec3"><title>Prior Art</title><p id="Par16">The first methods for rigid 2D tracking generally revolved around the use of various features or transformations and mainly explored various color-spaces for robust tracking (Crowley and Berard <xref ref-type="bibr" rid="CR42">1997</xref>; Bradski <xref ref-type="bibr" rid="CR29">1998b</xref>; Qian et&#x000a0;al. <xref ref-type="bibr" rid="CR150">1998</xref>; Toyama <xref ref-type="bibr" rid="CR180">1998</xref>; Jurie <xref ref-type="bibr" rid="CR81">1999</xref>; Schwerdt and Crowley <xref ref-type="bibr" rid="CR168">2000</xref>; Stern and Efros <xref ref-type="bibr" rid="CR175">2002</xref>; Vadakkepat et&#x000a0;al. <xref ref-type="bibr" rid="CR188">2008</xref>). The general methods of choice for tracking were Mean Shift and variations such as the Continuously Adaptive Mean Shift (Camshift) algorithm (Bradski <xref ref-type="bibr" rid="CR28">1998a</xref>; Allen et&#x000a0;al. <xref ref-type="bibr" rid="CR5">2004</xref>). The Mean Shift algorithm is a non-parametric technique that climbs the gradient of a probability distribution to find the nearest dominant mode (peak) (Comaniciu and Meer <xref ref-type="bibr" rid="CR37">1999</xref>; Comaniciu et&#x000a0;al. <xref ref-type="bibr" rid="CR38">2000</xref>). Camshift is an adaptation of the Mean Shift algorithm for object tracking. The primary difference between CamShift and Mean Shift is that the former uses continuously adaptive probability distributions (i.e., distributions that may be recomputed for each frame) while the latter is based on static distributions, which are not updated unless the target experiences significant changes in shape, size or color. Other popular methods of choice for tracking are linear and non-linear filtering techniques including Kalman filters, as well as methodologies that fall in the general category of particle filters (Del&#x000a0;Moral <xref ref-type="bibr" rid="CR50">1996</xref>; Gordon et&#x000a0;al. <xref ref-type="bibr" rid="CR66">1993</xref>), such as the popular Condensation algorithm by Isard and Blake (<xref ref-type="bibr" rid="CR77">1998</xref>). Condensation is the application of Sampling Importance Resampling (SIR) estimation by Gordon et&#x000a0;al. (<xref ref-type="bibr" rid="CR66">1993</xref>) to contour tracking. A recent successful 2D rigid tracker that updates the appearance model of the tracked face was proposed in Ross et&#x000a0;al. (<xref ref-type="bibr" rid="CR157">2008</xref>). The algorithm uses incremental Principal Component Analysis (PCA) (Levey and Lindenbaum <xref ref-type="bibr" rid="CR105">2000</xref>) to learn a statistical model of the appearance in an on-line manner and contrary to other eigentrackers, such as Black and Jepson (<xref ref-type="bibr" rid="CR24">1998</xref>), it does not contain any training phase. The method in Ross et&#x000a0;al. (<xref ref-type="bibr" rid="CR157">2008</xref>) uses a variant of the Condensation algorithm to model the distribution over the objects location as it evolves over time. The method has initiated a line of research on robust incremental object tracking including the works of Liwicki et&#x000a0;al. (<xref ref-type="bibr" rid="CR120">2012b</xref>, <xref ref-type="bibr" rid="CR121">2013</xref>, <xref ref-type="bibr" rid="CR119">2012a</xref>, <xref ref-type="bibr" rid="CR122">2015</xref>). Rigid 3D tracking has also been studied by using generic 3D models of the face (Malciu and Pr&#x0011b;teux <xref ref-type="bibr" rid="CR126">2000</xref>; La&#x000a0;Cascia et&#x000a0;al. <xref ref-type="bibr" rid="CR101">2000</xref>). For example, La&#x000a0;Cascia et&#x000a0;al. (<xref ref-type="bibr" rid="CR101">2000</xref>) formulate the tracking task as an image registration problem in the cylindrically unwrapped texture space and Sung et&#x000a0;al. (<xref ref-type="bibr" rid="CR177">2008</xref>) combine active appearance models (AAMs) with a cylindrical head model for robust recovery of the global rigid motion. Currently, rigid face tracking is generally treated along the same lines as general model free object tracking (Jepson et&#x000a0;al. <xref ref-type="bibr" rid="CR79">2003</xref>; Smeulders et&#x000a0;al. <xref ref-type="bibr" rid="CR172">2014</xref>; Liwicki et&#x000a0;al. <xref ref-type="bibr" rid="CR121">2013</xref>, <xref ref-type="bibr" rid="CR120">2012b</xref>; Ross et&#x000a0;al. <xref ref-type="bibr" rid="CR157">2008</xref>; Wu et&#x000a0;al. <xref ref-type="bibr" rid="CR201">2015</xref>; Li et&#x000a0;al. <xref ref-type="bibr" rid="CR106">2016a</xref>). An overview of model free object tracking is given in Sect.&#x000a0;<xref rid="Sec7" ref-type="sec">3.2</xref>.</p><p id="Par17">Non-rigid (deformable) tracking of faces is important in many applications, spanning from facial expression analysis to motion capture for graphics and game design. Deformable tracking of faces can be further subdivided into i) tracking of certain facial landmarks (Lanitis et&#x000a0;al. <xref ref-type="bibr" rid="CR102">1995</xref>; Black and Yacoob <xref ref-type="bibr" rid="CR25">1995</xref>; Sobottka and Pitas <xref ref-type="bibr" rid="CR174">1996</xref>; Xiao et&#x000a0;al. <xref ref-type="bibr" rid="CR202">2004</xref>; Matthews and Baker <xref ref-type="bibr" rid="CR128">2004</xref>; Matthews et&#x000a0;al. <xref ref-type="bibr" rid="CR129">2004</xref>; Patras and Pantic <xref ref-type="bibr" rid="CR141">2004</xref>; Papandreou and Maragos <xref ref-type="bibr" rid="CR139">2008</xref>; Amberg et&#x000a0;al. <xref ref-type="bibr" rid="CR7">2009</xref>; Tresadern et&#x000a0;al. <xref ref-type="bibr" rid="CR181">2012</xref>; Xiong and De&#x000a0;la Torre <xref ref-type="bibr" rid="CR205">2013</xref>; Asthana et&#x000a0;al. <xref ref-type="bibr" rid="CR12">2014</xref>; Xiong and De&#x000a0;la Torre <xref ref-type="bibr" rid="CR206">2015</xref>) or ii) tracking/estimation of dense facial motion (Essa et&#x000a0;al. <xref ref-type="bibr" rid="CR56">1996</xref>; Yacoob and Davis <xref ref-type="bibr" rid="CR207">1996</xref>; Essa et&#x000a0;al. <xref ref-type="bibr" rid="CR57">1997</xref>; Decarlo and Metaxas <xref ref-type="bibr" rid="CR47">2000</xref>; Koelstra et&#x000a0;al. <xref ref-type="bibr" rid="CR91">2010</xref>; Snape et&#x000a0;al. <xref ref-type="bibr" rid="CR173">2015</xref>). The latter category of estimating a dense facial motion through a model-based system was proposed by MIT Media lab in mid 1990&#x02019;s (Essa et&#x000a0;al. <xref ref-type="bibr" rid="CR57">1997</xref>, <xref ref-type="bibr" rid="CR56">1996</xref>, <xref ref-type="bibr" rid="CR59">1994</xref>; Basu et&#x000a0;al. <xref ref-type="bibr" rid="CR18">1996</xref>). In particular, the method by Essa and Pentland (<xref ref-type="bibr" rid="CR58">1994</xref>) tracks facial motion using optical flow computation coupled with a geometric and a physical (muscle) model describing the facial structure. This modeling results in a time-varying spatial patterning of facial shape and a parametric representation of the independent muscle action groups which is responsible for the observed facial motions. In Essa et&#x000a0;al. (<xref ref-type="bibr" rid="CR59">1994</xref>) the physically-based face model of Essa and Pentland (<xref ref-type="bibr" rid="CR58">1994</xref>) is driven by a set of responses from a set of templates that characterise facial regions. Model generated flow has been used by the same group in Basu et&#x000a0;al. (<xref ref-type="bibr" rid="CR18">1996</xref>) for motion regularisation. 3D motion estimation using sparse 3D models and optical flow estimation has also been proposed by Li et&#x000a0;al. (<xref ref-type="bibr" rid="CR108">1993</xref>), Bozda&#x0011f;i et&#x000a0;al. (<xref ref-type="bibr" rid="CR26">1994</xref>). Dense facial motion tracking is performed in Decarlo and Metaxas (<xref ref-type="bibr" rid="CR47">2000</xref>) by solving a model-based (using a facial deformable model) least-squares optical flow problem. The constraints are relaxed by the use of a Kalman filter, which permits controlled constraint violations based on the noise present in the optical flow information, and enables optical flow and edge information to be combined more robustly and efficiently. Free-form deformations (Rueckert et&#x000a0;al. <xref ref-type="bibr" rid="CR158">1999</xref>) are used in Koelstra et&#x000a0;al. (<xref ref-type="bibr" rid="CR91">2010</xref>) for extraction of dense facial motion for facial action unit recognition. Recently, Snape et&#x000a0;al. (<xref ref-type="bibr" rid="CR173">2015</xref>) proposed a statistical model of the facial flow for fast and robust dense facial motion extraction.</p><p id="Par18">Arguably, the category of deformable tracking that has received the majority of attention is that of tracking a set of sparse facial landmarks. The landmarks are either associated to a particular sparse facial model, i.e. the popular Candide facial model by Li et&#x000a0;al. (<xref ref-type="bibr" rid="CR108">1993</xref>), or correspond to fiducial facial regions/parts (e.g., mouth, eyes, nose etc.) (Cootes et&#x000a0;al. <xref ref-type="bibr" rid="CR41">2001</xref>). Even earlier attempts such as Essa and Pentland (<xref ref-type="bibr" rid="CR58">1994</xref>) understood the usefulness of tracking facial regions/landmarks in order to perform robust fitting of complex facial models (currently the vast majority of dense 3D facial model tracking techniques, such as Wei et&#x000a0;al. (<xref ref-type="bibr" rid="CR194">2004</xref>), Zhang et&#x000a0;al. (<xref ref-type="bibr" rid="CR228">2008</xref>), Amberg (<xref ref-type="bibr" rid="CR6">2011</xref>), rely on the robust tracking of a set of facial landmarks). Early approaches for tracking facial landmarks/regions included: (i)&#x000a0;the use of templates built around certain facial regions (Essa and Pentland <xref ref-type="bibr" rid="CR58">1994</xref>), (ii)&#x000a0;the use of facial classifiers to detect landmarks (Colmenarez et&#x000a0;al. <xref ref-type="bibr" rid="CR36">1999</xref>) where tracking is performed using modal analysis (Tao and Huang <xref ref-type="bibr" rid="CR179">1998</xref>) or (iii)&#x000a0;the use of face and facial region segmentation to detect the features where tracking is performed using block matching (Sobottka and Pitas <xref ref-type="bibr" rid="CR174">1996</xref>). Currently, deformable face tracking has converged with the problem of facial landmark localisation on static images. That is, the methods generally rely on fitting generative or discriminative statistical models of appearance and 2D/3D sparse facial shape at each frame. Arguably, the most popular methods are generative and discriminative variations of Active Appearance Models (AAMs) and Active Shape Models (ASMs) (Pighin et&#x000a0;al. <xref ref-type="bibr" rid="CR147">1999</xref>; Cootes et&#x000a0;al. <xref ref-type="bibr" rid="CR41">2001</xref>; Dornaika and Ahlberg <xref ref-type="bibr" rid="CR53">2004</xref>; Xiao et&#x000a0;al. <xref ref-type="bibr" rid="CR202">2004</xref>; Matthews and Baker <xref ref-type="bibr" rid="CR128">2004</xref>; Dedeo&#x0011f;lu et&#x000a0;al. <xref ref-type="bibr" rid="CR48">2007</xref>; Papandreou and Maragos <xref ref-type="bibr" rid="CR139">2008</xref>; Amberg et&#x000a0;al. <xref ref-type="bibr" rid="CR7">2009</xref>; Saragih et&#x000a0;al. <xref ref-type="bibr" rid="CR165">2011</xref>; Xiong and De&#x000a0;la Torre <xref ref-type="bibr" rid="CR205">2013</xref>, <xref ref-type="bibr" rid="CR206">2015</xref>). The statistical models of appearance and shape can either be generic as in Cootes et&#x000a0;al. (<xref ref-type="bibr" rid="CR41">2001</xref>), Matthews and Baker (<xref ref-type="bibr" rid="CR128">2004</xref>), Xiong and De&#x000a0;la Torre (<xref ref-type="bibr" rid="CR205">2013</xref>) or incrementally updated in order to better capture the face at hand, as in Sung and Kim (<xref ref-type="bibr" rid="CR176">2009</xref>), Asthana et&#x000a0;al. (<xref ref-type="bibr" rid="CR12">2014</xref>). The vast majority of the facial landmark localisation methodologies require an initialisation provided by a face detector. More details regarding current state-of-the-art in facial landmark localisation can be found in Sect.&#x000a0;<xref rid="Sec8" ref-type="sec">3.3</xref>.</p><p id="Par19">Arguably, the current practise regarding deformable face tracking includes the combination of a generic face detection and generic facial landmark localisation technique (Saragih et&#x000a0;al. <xref ref-type="bibr" rid="CR165">2011</xref>; Xiong and De&#x000a0;la Torre <xref ref-type="bibr" rid="CR205">2013</xref>, <xref ref-type="bibr" rid="CR206">2015</xref>; Alabort-i-Medina and Zafeiriou <xref ref-type="bibr" rid="CR3">2015</xref>; Asthana et&#x000a0;al. <xref ref-type="bibr" rid="CR13">2015</xref>). For example, popular approaches include successive application of the face detection and facial landmark localisation procedure at each frame. Another approach performs face detection in the first frame and then applies facial landmark localisation at each consecutive frame using the fitting result of the previous frame as initialisation. Face detection can be re-applied in case of failure. This is the approach that is used by popular packages such as Asthana et&#x000a0;al. (<xref ref-type="bibr" rid="CR12">2014</xref>). In this paper, we thoroughly evaluate variations of the above approaches. Furthermore, we consider the use of modern model free state-of-the-art trackers for rigid 2D tracking in order to be used as initialisation for the facial landmark localisation procedure. This is pictorially described in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>.</p></sec><sec id="Sec4"><title>Face Tracking Benchmarking</title><p id="Par20">For assessing the performance of rigid 2D face tracking several short face sequences have been annotated with regards to the facial region (using a bounding box style annotation). One of the first sequences that has been annotated for this task is the so-called Dudek sequence by Ross et&#x000a0;al. (<xref ref-type="bibr" rid="CR156">2015</xref>).<xref ref-type="fn" rid="Fn3">3</xref> Nowadays, several such sequences have been annotated and are publicly available, such as the ones by Liwicki et&#x000a0;al. (<xref ref-type="bibr" rid="CR123">2016</xref>), Li et&#x000a0;al. (<xref ref-type="bibr" rid="CR107">2016b</xref>), Wu et&#x000a0;al. (<xref ref-type="bibr" rid="CR201">2015</xref>).<fig id="Fig1"><label>Fig. 1</label><caption><p>Overview of the standard approaches for deformable face tracking. <italic>(Top)</italic> face detection is applied independently at each frame of the video followed by facial landmark localisation. <italic>(Bottom)</italic> model free tracking is employed, initialised with the bounding box of the face at the first frame, followed by facial landmark localisation</p></caption><graphic xlink:href="11263_2017_999_Fig1_HTML" id="MO1"/></fig>
</p><p id="Par22">The performance of deformable dense facial tracking methodologies was usually assessed by using markers (Decarlo and Metaxas <xref ref-type="bibr" rid="CR47">2000</xref>), simulated data (Snape et&#x000a0;al. <xref ref-type="bibr" rid="CR173">2015</xref>), visual inspection (Decarlo and Metaxas <xref ref-type="bibr" rid="CR47">2000</xref>; Essa et&#x000a0;al. <xref ref-type="bibr" rid="CR57">1997</xref>, <xref ref-type="bibr" rid="CR56">1996</xref>; Yacoob and Davis <xref ref-type="bibr" rid="CR207">1996</xref>; Snape et&#x000a0;al. <xref ref-type="bibr" rid="CR173">2015</xref>; Koelstra et&#x000a0;al. <xref ref-type="bibr" rid="CR91">2010</xref>) or indirectly by the use of the dense facial motion for certain tasks, such as expression analysis (Essa et&#x000a0;al. <xref ref-type="bibr" rid="CR56">1996</xref>; Yacoob and Davis <xref ref-type="bibr" rid="CR207">1996</xref>; Koelstra et&#x000a0;al. <xref ref-type="bibr" rid="CR91">2010</xref>). Regarding tracking of facial landmarks, up until recently, the preferred method for assessing the performance was visual inspection in a number of selected facial videos (Xiong and De&#x000a0;la Torre <xref ref-type="bibr" rid="CR205">2013</xref>; Tresadern et&#x000a0;al. <xref ref-type="bibr" rid="CR181">2012</xref>). Other methods were assessed on a small number of short (a few seconds in length) annotated facial videos (Sagonas et&#x000a0;al. <xref ref-type="bibr" rid="CR161">2014</xref>; Asthana et&#x000a0;al. <xref ref-type="bibr" rid="CR12">2014</xref>). Until recently the longest annotated facial video sequence was the so-called talking face of Cootes (<xref ref-type="bibr" rid="CR39">2016</xref>) which was used to evaluate many tracking methods including Orozco et&#x000a0;al. (<xref ref-type="bibr" rid="CR137">2013</xref>), Amberg et&#x000a0;al. (<xref ref-type="bibr" rid="CR7">2009</xref>). The talking face video comprises of 5000 frames (around 200 seconds) taken from a video of a person engaged in a conversation. The talking face video was initially tracked using an Active Appearance Model (AAM) that had a shape model and a total of 68 landmarks are provided. The tracked landmarks were visually checked and manually corrected where necessary.</p><p id="Par23">Recently, Xiong and De&#x000a0;la Torre (<xref ref-type="bibr" rid="CR206">2015</xref>) introduced a benchmark for facial landmark tracking using videos from the Distracted Driver Face (DDF) and Naturalistic Driving Study (NDS) in Campbell (<xref ref-type="bibr" rid="CR32">2016</xref>).<xref ref-type="fn" rid="Fn4">4</xref> The DDF dataset contains 15 sequences with a total of 10,882 frames. Each sequence displays a single subject posing as the distracted driver in a stationary vehicle or indoor environment. 12 out of 15 videos were recorded with subjects sitting inside of a vehicle. Five of them were recorded during the night under infrared (IR) light and the rest were recorded during the daytime under natural lighting. The remaining three were recorded indoors. The NDS database contains 20 sub-sequences of driver faces recorded during a drive conducted between the Blacksburg, VA and Washington, DC areas (NDS is more challenging than DDF since its videos are of lower spatial and temporal resolution). Each video of the NDS database has one minute duration recorded at 15 frames per second (fps) with a <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$360 \times 240$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mn>360</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>240</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq1.gif"/></alternatives></inline-formula> resolution. For both datasets one in every ten frames was annotated using either 49 landmarks for near-frontal faces or 31 landmarks for profile faces. The database contains many extreme facial poses (90<inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^\circ $$\end{document}</tex-math><mml:math id="M4"><mml:msup><mml:mrow/><mml:mo>&#x02218;</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq2.gif"/></alternatives></inline-formula> yaw, 50<inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\circ $$\end{document}</tex-math><mml:math id="M6"><mml:mo>&#x02218;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq3.gif"/></alternatives></inline-formula> pitch) as well as many faces under extreme lighting condition (e.g., IR). In total the dataset presented in Xiong and De&#x000a0;la Torre (<xref ref-type="bibr" rid="CR206">2015</xref>) contains between 2000 to 3000 annotated faces (please refer to Xiong and De&#x000a0;la Torre (<xref ref-type="bibr" rid="CR206">2015</xref>) for exemplar annotations).</p><p id="Par25">The only existing large in-the-wild benchmark for facial landmark tracking was recently introduced by Shen et&#x000a0;al. (<xref ref-type="bibr" rid="CR170">2015</xref>). The benchmark consists of 114 videos with varying difficulty and provides annotations generated in a semi-automatic manner (Chrysos et&#x000a0;al. <xref ref-type="bibr" rid="CR35">2015</xref>; Shen et&#x000a0;al. <xref ref-type="bibr" rid="CR170">2015</xref>; Tzimiropoulos <xref ref-type="bibr" rid="CR182">2015</xref>). This challenge, called 300&#x000a0;VW, is the only existing large-scale comprehensive benchmark for deformable model tracking. More details regarding the dataset of the 300&#x000a0;VW benchmark can be found in Sect.&#x000a0;<xref rid="Sec10" ref-type="sec">4.1</xref>. The performance of the pipelines considered in this paper are compared with the participating methods of the 300&#x000a0;VW challenge in Sect.&#x000a0;<xref rid="Sec19" ref-type="sec">4.8</xref>.</p></sec></sec><sec id="Sec5"><title>Deformable Face Tracking</title><p id="Par26">In this paper, we focus on the problem of performing deformable face tracking across long-term sequences within unconstrained videos. The problem of tracking across long-term sequences is particularly challenging as the appearance of the face may change significantly during the sequence due to occlusions, illumination variation, motion artifacts and head pose. For the problem of deformable tracking, however, the problem is further complicated by the expectation of recovering a set of accurate fiducial points in conjunction with successfully tracking the object. As described in Sect.&#x000a0;<xref rid="Sec2" ref-type="sec">2</xref>, current deformable facial tracking methods mainly concentrate on performing face detection per frame and then performing facial landmark localisation. However, we consider the most important metric for measuring the success of deformable face tracking as the facial landmark localisation accuracy. Given this, there are a number of strategies that could feasibly be employed in order to attempt to minimise the total facial landmark localisation error across the entire sequence. Therefore, we take advantage of current advances in face detection, model free tracking and facial landmark localisation techniques in order to perform deformable face tracking. Specifically, we investigate three strategies for deformable tracking:<list list-type="order"><list-item><p id="Par27">
<bold>Detection&#x000a0;+&#x000a0;landmark localisation</bold> Face Detection per frame, followed by facial landmark localisation initialised within the facial bounding boxes. This scenario is visualised in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> (top).</p></list-item><list-item><p id="Par28">
<bold>Model free tracking&#x000a0;+&#x000a0;landmark localisation</bold> Model free tracking, initialised around the interior of the face within the first frame, followed by facial landmark localisation within the tracked box. This scenario is visualised in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> (bottom).</p></list-item><list-item><p id="Par29">
<bold>Hybrid systems</bold> Hybrid methods that attempt to improve the robustness of the placement of the bounding box for landmark localisation. Namely, we investigate methods for failure detection, trajectory smoothness and reinitialisation. Examples of such methods are pictorially demonstrated in Figs.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> and&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>.</p></list-item></list>Note that we focus on combinations of methods that provide bounding boxes of the facial region followed by landmark localisation. This is due to the fact that the current set of state-of-the-art landmark localisation methods are all local methods and require initialisation within the facial region. Although joint face detection and landmark localisation methods have been proposed (Zhu and Ramanan <xref ref-type="bibr" rid="CR230">2012</xref>; Chen et&#x000a0;al. <xref ref-type="bibr" rid="CR34">2014</xref>), they are not competitive with the most recent set of landmark localisation methods. For this reason, in this paper we focus on the combination of bounding box estimators with state-of-the-art local landmark localisation techniques.</p><p id="Par30">The remainder of this Section will give a brief overview of the literature concerning face detection, model free tracking and facial landmark localisation.</p><sec id="Sec6"><title>Face Detection</title><p id="Par31">Face detection is among the most important and popular tasks in Computer Vision and an essential step for applications such as face recognition and face analysis. Although it is one of the oldest tasks undertaken by researchers (the early works appeared about 45 years ago (Sakai et&#x000a0;al. <xref ref-type="bibr" rid="CR163">1972</xref>; Fischler and Elschlager <xref ref-type="bibr" rid="CR62">1973</xref>)), it is still an open and challenging problem. Recent advances can achieve reliable performance under moderate illumination and pose conditions, which led to the installation of simple face detection technologies in everyday devices such as digital cameras and mobile phones. However, recent benchmarks (Jain and Learned-Miller <xref ref-type="bibr" rid="CR78">2010</xref>) show that the detection of faces on arbitrary images is still a very challenging problem.</p><p id="Par32">Since face detection has been a research topic for so many decades, the existing literature is, naturally, extremely extensive. The fact that all recent face detection surveys (Hjelm&#x000e5;s and Low <xref ref-type="bibr" rid="CR73">2001</xref>; Yang et&#x000a0;al. <xref ref-type="bibr" rid="CR214">2002</xref>; Zhang and Zhang <xref ref-type="bibr" rid="CR218">2010</xref>; Zafeiriou et&#x000a0;al. <xref ref-type="bibr" rid="CR217">2015</xref>) provide different categorisations of the relative literature is indicative of the huge range of existing techniques. Consequently, herein, we only present a basic outline of the face detection literature. For an extended review, the interested reader may refer to the most recent face detection survey in Zafeiriou et&#x000a0;al. (<xref ref-type="bibr" rid="CR217">2015</xref>).</p><p id="Par33">According to the most recent literature review Zafeiriou et&#x000a0;al. (<xref ref-type="bibr" rid="CR217">2015</xref>), existing methods can be separated in two major categories. The first one includes methodologies that learn a set of rigid templates, which can be further split in the following groups: (i)&#x000a0;boosting-based methods, (ii)&#x000a0;approaches that utilise SVM classifiers, (ii)&#x000a0;exemplar-based techniques, and (iv)&#x000a0;frameworks based on Neural Networks. The second major category includes deformable part models, i.e. methodologies that learn a set of templates per part as well as the deformations between them.</p><p id="Par34">
<italic>Boosting Methods</italic> Boosting combines multiple &#x0201c;weak&#x0201d; hypotheses of moderate accuracy in order to determine a highly accurate hypothesis. The most characteristic example is Adaptive Boosting (AdaBoost) which is utilised by the most popular face detection methodology, i.e. the Viola&#x02013;Jones (VJ) detector of Viola and Jones (<xref ref-type="bibr" rid="CR189">2001</xref>, <xref ref-type="bibr" rid="CR190">2004</xref>). Characteristic examples of other methods that employ variations of AdaBoost include&#x000a0;Li et&#x000a0;al. (<xref ref-type="bibr" rid="CR114">2002</xref>), Wu et&#x000a0;al. (<xref ref-type="bibr" rid="CR197">2004</xref>), Mita et&#x000a0;al. (<xref ref-type="bibr" rid="CR131">2005</xref>). The original VJ algorithm used Haar features, however boosting (or cascade of classifiers methodologies in general) have been shown to greatly benefit from robust features (K&#x000f6;stinger et&#x000a0;al. <xref ref-type="bibr" rid="CR94">2012</xref>; Jun et&#x000a0;al. <xref ref-type="bibr" rid="CR80">2013</xref>; Li et&#x000a0;al. <xref ref-type="bibr" rid="CR113">2011</xref>; Li and Zhang <xref ref-type="bibr" rid="CR112">2013</xref>; Mathias et&#x000a0;al. <xref ref-type="bibr" rid="CR127">2014</xref>; Yang et&#x000a0;al. <xref ref-type="bibr" rid="CR210">2014a</xref>), such as HOG (Dalal and Triggs <xref ref-type="bibr" rid="CR43">2005</xref>), SIFT (Lowe <xref ref-type="bibr" rid="CR124">1999</xref>), SURF (Bay et&#x000a0;al. <xref ref-type="bibr" rid="CR19">2008</xref>) and LBP (Ojala et&#x000a0;al. <xref ref-type="bibr" rid="CR135">2002</xref>). For example, SURF features have been successfully combined with a cascade of weak classifiers in Li et&#x000a0;al. (<xref ref-type="bibr" rid="CR113">2011</xref>), Li and Zhang (<xref ref-type="bibr" rid="CR112">2013</xref>), achieving faster convergence. Additionally, Jun et&#x000a0;al. (<xref ref-type="bibr" rid="CR80">2013</xref>) propose robust face specific features that combine both LBP and HOG. Mathias et&#x000a0;al. (<xref ref-type="bibr" rid="CR127">2014</xref>) recently proposed an approach (so called HeadHunter) with state-of-the-art performance that employs various robust features with boosting. Specifically, they propose the adaptation of Integral Channel Features (ICF) (Doll&#x000e1;r et&#x000a0;al. <xref ref-type="bibr" rid="CR51">2009</xref>) with HOG and LUV colour channels, combined with global feature normalisation. A similar approach is followed by Yang et&#x000a0;al. (<xref ref-type="bibr" rid="CR210">2014a</xref>), in which they combine gray-scale, RGB, HSV, LUV, gradient magnitude and histograms within a cascade of weak classifiers.</p><p id="Par35">
<italic>SVM Classifiers</italic> Maximum margin classifiers, such as Support Vector Machines (SVMs), have become popular for face detection (Romdhani et&#x000a0;al. <xref ref-type="bibr" rid="CR155">2001</xref>; Heisele et&#x000a0;al. <xref ref-type="bibr" rid="CR71">2003</xref>; R&#x000e4;tsch et&#x000a0;al. <xref ref-type="bibr" rid="CR153">2004</xref>; King <xref ref-type="bibr" rid="CR89">2015</xref>). Even though their detection speed was initially slow, various schemes have been proposed to speed up the process. Romdhani et&#x000a0;al. (<xref ref-type="bibr" rid="CR155">2001</xref>) propose a method that computes a reduced set of vectors from the original support vectors that are used sequentially in order to make early rejections. A similar approach is adopted by R&#x000e4;tsch et&#x000a0;al. (<xref ref-type="bibr" rid="CR153">2004</xref>). A hierarchy of SVM classifiers trained on different resolutions is applied in Heisele et&#x000a0;al. (<xref ref-type="bibr" rid="CR71">2003</xref>). King (<xref ref-type="bibr" rid="CR89">2015</xref>) proposes an algorithm for efficient learning of a max-margin classifier using all the sub-windows of the training images, without applying any sub-sampling, and formulates a convex optimisation that finds the global optimum. Moreover, SVM classifiers have also been used for multi-view face detection (Li et&#x000a0;al. <xref ref-type="bibr" rid="CR116">2000</xref>; Wang and Ji <xref ref-type="bibr" rid="CR192">2004</xref>). For example, Li et&#x000a0;al. (<xref ref-type="bibr" rid="CR116">2000</xref>) first apply a face pose estimator based on support vector regression (SVR), followed by an SVM face detector for each pose.</p><p id="Par36">
<italic>Exemplar-Based Techniques</italic> These methods aim to match a test image against a large set of facial images. This approach is inspired by principles used in image retrieval and requires that the exemplar set covers the large appearance variation of human face. Shen et&#x000a0;al. (<xref ref-type="bibr" rid="CR171">2013</xref>) employ bag-of-word image retrieval methods to extract features from each exemplar, which creates a voting map for each exemplar that functions as a weak classifier. Thus, the final detection is performed by combining the voting maps. A similar methodology is applied in Li et&#x000a0;al. (<xref ref-type="bibr" rid="CR110">2014</xref>), with the difference that specific exemplars are used as weak classifiers based on a boosting strategy. Recently, Kumar et&#x000a0;al. (<xref ref-type="bibr" rid="CR100">2015</xref>) proposed an approach that enhances the voting procedure by using semantically related visual words as well as weighted occurrence of visual words based on their spatial distributions.<table-wrap id="Tab1"><label>Table 1</label><caption><p>The set of detectors used in this paper</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Citation(s)</th><th align="left">Rigid template</th><th align="left">DPM</th><th align="left">Implementation</th></tr></thead><tbody><tr><td align="left" rowspan="3">DPM</td><td align="left">
Felzenszwalb et&#x000a0;al. (<xref ref-type="bibr" rid="CR61">2010</xref>)</td><td align="left"/><td align="left" rowspan="3">
<inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M8"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq4.gif"/></alternatives></inline-formula>
</td><td align="left" rowspan="3">
<ext-link ext-link-type="uri" xlink:href="https://github.com/menpo/ffld2">https://github.com/menpo/ffld2</ext-link>
</td></tr><tr><td align="left">
Mathias et&#x000a0;al. (<xref ref-type="bibr" rid="CR127">2014</xref>)</td><td align="left"/></tr><tr><td align="left">
Alabort-i-Medina et&#x000a0;al. (<xref ref-type="bibr" rid="CR4">2014</xref>)</td><td align="left"/></tr><tr><td align="left">HR-TF</td><td align="left">
Hu and Ramanan (<xref ref-type="bibr" rid="CR74">2016</xref>)</td><td align="left">
<inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M10"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq5.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://www.cs.cmu.edu/%7epeiyunh/tiny/">https://www.cs.cmu.edu/~peiyunh/tiny/</ext-link>
</td></tr><tr><td align="left">MTCNN</td><td align="left">
Zhang et&#x000a0;al. (<xref ref-type="bibr" rid="CR223">2016</xref>)</td><td align="left">
<inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M12"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq6.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://goo.gl/4BMGeR">https://goo.gl/4BMGeR</ext-link>
</td></tr><tr><td align="left">NPD</td><td align="left">
Liao et&#x000a0;al. (<xref ref-type="bibr" rid="CR118">2016</xref>)</td><td align="left">
<inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M14"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq7.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://goo.gl/dRXp8d">https://goo.gl/dRXp8d</ext-link>
</td></tr><tr><td align="left">SS-DPM</td><td align="left">
Zhu and Ramanan (<xref ref-type="bibr" rid="CR230">2012</xref>)</td><td align="left"/><td align="left">
<inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M16"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq8.gif"/></alternatives></inline-formula>
</td><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://www.ics.uci.edu/%7exzhu/face">https://www.ics.uci.edu/~xzhu/face</ext-link>
</td></tr><tr><td align="left" rowspan="2">SVM+HOG</td><td align="left">
King (<xref ref-type="bibr" rid="CR89">2015</xref>)</td><td align="left" rowspan="2">
<inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M18"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq9.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left" rowspan="2">
<ext-link ext-link-type="uri" xlink:href="https://github.com/davisking/dlib">https://github.com/davisking/dlib</ext-link>
</td></tr><tr><td align="left">
King (<xref ref-type="bibr" rid="CR88">2009</xref>)</td><td align="left"/></tr><tr><td align="left" rowspan="2">VJ</td><td align="left">
Viola and Jones (<xref ref-type="bibr" rid="CR190">2004</xref>)</td><td align="left" rowspan="2">
<inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M20"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq10.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left" rowspan="2">
<ext-link ext-link-type="uri" xlink:href="http://opencv.org">http://opencv.org</ext-link>
</td></tr><tr><td align="left">
Bradski (<xref ref-type="bibr" rid="CR27">2000</xref>)</td><td align="left"/></tr><tr><td align="left">VPHR</td><td align="left">
Kumar et&#x000a0;al. (<xref ref-type="bibr" rid="CR100">2015</xref>)</td><td align="left">
<inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M22"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq11.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="http://cvit.iiit.ac.in/projects/exemplar/">http://cvit.iiit.ac.in/projects/exemplar/</ext-link>
</td></tr></tbody></table><table-wrap-foot><p>The table reports the short name of the method, the relevant citation(s) as well as the link to the implementation used</p></table-wrap-foot></table-wrap>
</p><p id="Par37">
<italic>Convolutional Neural Networks</italic> Another category, similar to the previous rigid template-based ones, includes the employment of Convolutional Neural Networks (CNNs) and Deep CNNs (DCNNs) (Osadchy et&#x000a0;al. <xref ref-type="bibr" rid="CR138">2007</xref>; Zhang and Zhang <xref ref-type="bibr" rid="CR219">2014</xref>; Ranjan et&#x000a0;al. <xref ref-type="bibr" rid="CR152">2015</xref>; Li et&#x000a0;al. <xref ref-type="bibr" rid="CR111">2015a</xref>; Yang et&#x000a0;al. <xref ref-type="bibr" rid="CR215">2015b</xref>). Osadchy et&#x000a0;al. (<xref ref-type="bibr" rid="CR138">2007</xref>) use a network with four convolution layers and one fully connected layer that rejects the non-face hypotheses and estimates the pose of the correct face hypothesis. Zhang and Zhang (<xref ref-type="bibr" rid="CR219">2014</xref>) propose a multi-view face detection framework by employing a multi-task DCNN for face pose estimation and landmark localization in order to obtain better features for face detection. Ranjan et&#x000a0;al. (<xref ref-type="bibr" rid="CR152">2015</xref>) combine deep pyramidal features with Deformable Part Models. Recently, Yang et&#x000a0;al. (<xref ref-type="bibr" rid="CR215">2015b</xref>) proposed a DCNN architecture that is able to discover facial parts responses from arbitrary uncropped facial images without any part supervision and report state-of-the-art performance on current face detection benchmarks.</p><p id="Par38">
<italic>Deformable Part Models</italic> DPMs (Schneiderman and Kanade <xref ref-type="bibr" rid="CR166">2004</xref>; Felzenszwalb and Huttenlocher <xref ref-type="bibr" rid="CR60">2005</xref>; Felzenszwalb et&#x000a0;al. <xref ref-type="bibr" rid="CR61">2010</xref>; Zhu and Ramanan <xref ref-type="bibr" rid="CR230">2012</xref>; Yan et&#x000a0;al. <xref ref-type="bibr" rid="CR208">2013</xref>; Li et&#x000a0;al. <xref ref-type="bibr" rid="CR109">2013a</xref>; Yan et&#x000a0;al. <xref ref-type="bibr" rid="CR209">2014</xref>; Mathias et&#x000a0;al. <xref ref-type="bibr" rid="CR127">2014</xref>; Ghiasi and Fowlkes <xref ref-type="bibr" rid="CR64">2014</xref>; Barbu et&#x000a0;al. <xref ref-type="bibr" rid="CR17">2014</xref>) learn a patch expert for each part of an object and model the deformations between parts using spring-like connections based on a tree structure. Consequently, they perform joint facial landmark localisation and face detection. Even though they are not the best performing methods for landmark localisation, they are highly accurate for face detection in-the-wild. However, their main disadvantage is their high computational cost. Pictorial Structures (PS) (Fischler and Elschlager <xref ref-type="bibr" rid="CR62">1973</xref>; Felzenszwalb and Huttenlocher <xref ref-type="bibr" rid="CR60">2005</xref>) are the first family of DPMs that appeared. They are generative DPMs that assume Gaussian distributions to model the appearance of each part, as well as the deformations. They became a very popular line of research after the influential work in Felzenszwalb and Huttenlocher (<xref ref-type="bibr" rid="CR60">2005</xref>) that proposed a very efficient dynamic programming algorithm for finding the global optimum based on Generalized Distance Transform. Many discriminatively trained DPMs (Felzenszwalb et&#x000a0;al. <xref ref-type="bibr" rid="CR61">2010</xref>; Zhu and Ramanan <xref ref-type="bibr" rid="CR230">2012</xref>; Yan et&#x000a0;al. <xref ref-type="bibr" rid="CR208">2013</xref>, <xref ref-type="bibr" rid="CR209">2014</xref>) appeared afterwards, which learn the patch experts and deformation parameters using discriminative classifiers, such as latent SVM.</p><p id="Par39">DPMs can be further separated with respect to their training scenario into: (i) weakly supervised and (ii) strongly supervised. Weakly-supervised DPMs (Felzenszwalb et&#x000a0;al. <xref ref-type="bibr" rid="CR61">2010</xref>; Yan et&#x000a0;al. <xref ref-type="bibr" rid="CR209">2014</xref>) are trained using only the bounding boxes of the positive examples and a set of negative examples. The most representative example is the work by Felzenszwalb et&#x000a0;al. (<xref ref-type="bibr" rid="CR61">2010</xref>), which has proved to be very efficient for generic object detection. Under a strongly supervised scenario, it is assumed that a training database with images annotated with figucial landmarks is available. Several strongly supervised methods exist in the literature (Felzenszwalb and Huttenlocher <xref ref-type="bibr" rid="CR60">2005</xref>; Zhu and Ramanan <xref ref-type="bibr" rid="CR230">2012</xref>; Yan et&#x000a0;al. <xref ref-type="bibr" rid="CR208">2013</xref>; Ghiasi and Fowlkes <xref ref-type="bibr" rid="CR64">2014</xref>). Ghiasi and Fowlkes (<xref ref-type="bibr" rid="CR64">2014</xref>) propose an hierarchical DPM that explicitly models parts&#x02019; occlusions. In Zhu and Ramanan (<xref ref-type="bibr" rid="CR230">2012</xref>) it is shown that a strongly supervised DPM outperforms, by a large margin, a weakly supervised one. In contrast, HeadHunter by Mathias et&#x000a0;al. (<xref ref-type="bibr" rid="CR127">2014</xref>) shows that a weakly supervised DPM can outperform all current state-of-the-art face detection methodologies including the strongly supervised DPM of Zhu and Ramanan (<xref ref-type="bibr" rid="CR230">2012</xref>).</p><p id="Par40">According to FDDB (Jain and Learned-Miller <xref ref-type="bibr" rid="CR78">2010</xref>), which is the most well established face detection benchmark, the currently top-performing methodology is the one by Ranjan et&#x000a0;al. (<xref ref-type="bibr" rid="CR152">2015</xref>), which combines DCNNs with a DPM. Some of the top-performing systems consist of commercial software, thus we did use the deep methods of Hu and Ramanan (<xref ref-type="bibr" rid="CR74">2016</xref>), Zhang et&#x000a0;al. (<xref ref-type="bibr" rid="CR223">2016</xref>) that are available as open source with the method of Hu and Ramanan (<xref ref-type="bibr" rid="CR74">2016</xref>) reporting the latest best performance in FDDB. Additionally, we employ the top performing SVM-based method for learning rigid templates (King <xref ref-type="bibr" rid="CR89">2015</xref>), the best weakly and strongly supervised DPM implementations of Mathias et&#x000a0;al. (<xref ref-type="bibr" rid="CR127">2014</xref>) and Zhu and Ramanan (<xref ref-type="bibr" rid="CR230">2012</xref>), along with the best performing exemplar-based technique of Kumar et&#x000a0;al. (<xref ref-type="bibr" rid="CR100">2015</xref>) . Finally, we also use the popular VJ algorithm (Viola and Jones <xref ref-type="bibr" rid="CR189">2001</xref>, <xref ref-type="bibr" rid="CR190">2004</xref>) as a baseline face detection method. The employed face detection implementations are summarised in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>.</p></sec><sec id="Sec7"><title>Model Free Tracking</title><p id="Par41">Model free tracking is an extremely active area of research. Given the initial state (e.g., position and size of the containing box) of a target object in the first image, model free tracking attempts to estimate the states of the target in subsequent frames. Therefore, model free tracking provides an excellent method of initialising landmark localisation methods.</p><p id="Par42">The literature on model free tracking is vast. For the rest of this section, we will provide an extremely brief overview of model free tracking that focuses primarily on areas that are relevant to the tracking methods we investigated in this paper. We refer the interested reader to the wealth of tracking surveys (Li et&#x000a0;al. <xref ref-type="bibr" rid="CR115">2013b</xref>; Smeulders et&#x000a0;al. <xref ref-type="bibr" rid="CR172">2014</xref>; Salti et&#x000a0;al. <xref ref-type="bibr" rid="CR164">2012</xref>; Yang et&#x000a0;al. <xref ref-type="bibr" rid="CR212">2011</xref>) and benchmarks (Wu et&#x000a0;al. <xref ref-type="bibr" rid="CR200">2013</xref>, <xref ref-type="bibr" rid="CR201">2015</xref>; Kristan et&#x000a0;al. <xref ref-type="bibr" rid="CR96">2013</xref>, <xref ref-type="bibr" rid="CR97">2014</xref>, <xref ref-type="bibr" rid="CR98">2015</xref>, <xref ref-type="bibr" rid="CR99">2016</xref>; Smeulders et&#x000a0;al. <xref ref-type="bibr" rid="CR172">2014</xref>) for more information on model free tracking methods.</p><p id="Par43">
<italic>Generative Trackers</italic> These trackers attempt to model the objects appearance directly. This includes template based methods, such as those by Matthews et&#x000a0;al. (<xref ref-type="bibr" rid="CR129">2004</xref>), Baker and Matthews (<xref ref-type="bibr" rid="CR15">2004</xref>), Sevilla-Lara and Learned-Miller (<xref ref-type="bibr" rid="CR169">2012</xref>), as well as parametric generative models such as Balan and Black (<xref ref-type="bibr" rid="CR16">2006</xref>), Ross et&#x000a0;al. (<xref ref-type="bibr" rid="CR157">2008</xref>), Black and Jepson (<xref ref-type="bibr" rid="CR24">1998</xref>) , Xiao et&#x000a0;al. (<xref ref-type="bibr" rid="CR204">2014</xref>). The work of Ross et&#x000a0;al. (<xref ref-type="bibr" rid="CR157">2008</xref>) introduces online subspace learning for tracking with a sample mean update, which allows the tracker to account for changes in illumination, viewing angle and pose of the object. The idea is to incrementally learn a low-dimensional subspace and adapt the appearance model on object changes. The update is based on an incremental principal component analysis (PCA) algorithm, however it seems to be ineffective at handling large occlusions or non-rigid movements due to its holistic model. To alleviate the partial occlusion, Xiao et&#x000a0;al. (<xref ref-type="bibr" rid="CR204">2014</xref>) suggest the use of square templates along with PCA. Another popular area of generative tracking is the use of sparse representations for appearance. In Mei and Ling (<xref ref-type="bibr" rid="CR130">2011</xref>), a target candidate is represented by a sparse linear combination of target and trivial templates. The coefficients are extracted by solving an <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ell _1$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>&#x02113;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq12.gif"/></alternatives></inline-formula> minimisation problem with non-negativity constraints, while the target templates are updated online. However, solving the <inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ell _1$$\end{document}</tex-math><mml:math id="M26"><mml:msub><mml:mi>&#x02113;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq13.gif"/></alternatives></inline-formula> minimisation for each particle is computationally expensive. A generalisation of this tracker is the work of Zhang et&#x000a0;al. (<xref ref-type="bibr" rid="CR226">2012</xref>), which learns the representation for all particles jointly. It additionally improves the robustness by exploiting the correlation among particles. An even further abstraction is achieved in Zhang et&#x000a0;al. (<xref ref-type="bibr" rid="CR227">2014d</xref>) where a low-rank sparse representation of the particles is encouraged. In Zhang et&#x000a0;al. (<xref ref-type="bibr" rid="CR222">2014c</xref>), the authors generalise the low-rank constraint of Zhang et&#x000a0;al. (<xref ref-type="bibr" rid="CR227">2014d</xref>) and add a sparse error term in order to handle outliers. Another low-rank formulation was used by Wu et&#x000a0;al. (<xref ref-type="bibr" rid="CR199">2012</xref>) which is an online version of the RASL (Peng et&#x000a0;al. <xref ref-type="bibr" rid="CR143">2012</xref>) algorithm and attempts to jointly align the input sequence using convex optimisation.</p><p id="Par44">
<italic>Keypoint Trackers</italic> These trackers (Pernici and Del&#x000a0;Bimbo <xref ref-type="bibr" rid="CR145">2014</xref>; Poling et&#x000a0;al. <xref ref-type="bibr" rid="CR148">2014</xref>; Hare et&#x000a0;al. <xref ref-type="bibr" rid="CR70">2012</xref>; Nebehay and Pflugfelder <xref ref-type="bibr" rid="CR133">2015</xref>) attempt to use the robustness of keypoint detection methodologies like SIFT (Lowe <xref ref-type="bibr" rid="CR124">1999</xref>) or SURF (Bay et&#x000a0;al. <xref ref-type="bibr" rid="CR19">2008</xref>) in order to perform tracking. Pernici and Del&#x000a0;Bimbo (<xref ref-type="bibr" rid="CR145">2014</xref>) collected multiple descriptors of weakly aligned keypoints over time and combined these matched keypoints in a RANSAC voting scheme. Nebehay and Pflugfelder (<xref ref-type="bibr" rid="CR133">2015</xref>) utilises keypoints to vote for the object center in each frame. A consensus-based scheme is applied for outlier detection and the votes are transformed based on the current key point arrangement to consider scale and rotation. However, keypoint methods may suffer from difficulty in capturing the global information of the tracked target by only considering the local points.</p><p id="Par45">
<italic>Discriminative Trackers</italic> These trackers attempt to explicitly model the difference between the object appearance and the background. Most commonly, these methods are named &#x0201c;tracking-by-detection&#x0201d; techniques as they involve classifying image regions as either part of the object or the background. In their work, Grabner et&#x000a0;al. (<xref ref-type="bibr" rid="CR67">2006</xref>) propose an online boosting method to select and update discriminative features which allows the system to account for minor changes in the object appearance. However, the tracker fails to model severe changes in appearance. Babenko et&#x000a0;al. (<xref ref-type="bibr" rid="CR14">2011</xref>) advocate the use of a multiple instance learning boosting algorithm to mitigate the drifting problem. More recently, discriminative correlation filters (DCF) have become highly successful at tracking. The DCF is trained by performing a circular sliding window operation on the training samples. This periodic assumption enables efficient training and detection by utilizing the Fast Fourier Transform (FFT). Danelljan et&#x000a0;al. (<xref ref-type="bibr" rid="CR44">2014</xref>) learn separate correlation filters for the translation and the scale estimation. In Danelljan et&#x000a0;al. (<xref ref-type="bibr" rid="CR45">2015</xref>), the authors introduce a sparse spatial regularisation term to mitigate the artifacts at the boundaries of the circular correlation. In contrast to the linear regression commonly used to learn DCFs, Henriques et&#x000a0;al. (<xref ref-type="bibr" rid="CR72">2015</xref>) apply a kernel regression and propose its multi-channel extension to enable to the use of features such as HOG&#x000a0;Dalal and Triggs (<xref ref-type="bibr" rid="CR43">2005</xref>). Li et&#x000a0;al. (<xref ref-type="bibr" rid="CR117">2015b</xref>) propose a new use for particle filters in order to choose reliables patches to consider part of the object. These patches are modelled using a variant of the method proposed by Henriques et&#x000a0;al. (<xref ref-type="bibr" rid="CR72">2015</xref>). Hare et&#x000a0;al. (<xref ref-type="bibr" rid="CR69">2011</xref>) propose the use of structured output prediction. By explicitly allowing the outputs to parametrize the needs of the tracker, an intermediate classification step is avoided.<table-wrap id="Tab2"><label>Table 2</label><caption><p>The set of trackers that are used in this paper</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Citation(s)</th><th align="left">D</th><th align="left">G</th><th align="left">P</th><th align="left">K</th><th align="left">NN</th><th align="left">Implementation</th></tr></thead><tbody><tr><td align="left">CAMSHIFT</td><td align="left">
Bradski (<xref ref-type="bibr" rid="CR28">1998a</xref>)</td><td align="left">
<inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M28"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq14.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="http://opencv.org">http://opencv.org</ext-link>
</td></tr><tr><td align="left">CCOT</td><td align="left">
Danelljan et&#x000a0;al. (<xref ref-type="bibr" rid="CR46">2016</xref>)</td><td align="left">
<inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M30"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq15.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left">
<inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M32"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq16.gif"/></alternatives></inline-formula>
</td><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://goo.gl/Rnf73K">https://goo.gl/Rnf73K</ext-link>
</td></tr><tr><td align="left">CMT</td><td align="left">
Nebehay and Pflugfelder (<xref ref-type="bibr" rid="CR133">2015</xref>)</td><td align="left"/><td align="left"/><td align="left"/><td align="left">
<inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M34"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq17.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://github.com/gnebehay/CppMT">https://github.com/gnebehay/CppMT</ext-link>
</td></tr><tr><td align="left">DF</td><td align="left">
Sevilla-Lara and Learned-Miller (<xref ref-type="bibr" rid="CR169">2012</xref>)</td><td align="left"/><td align="left">
<inline-formula id="IEq18"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M36"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq18.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="http://goo.gl/YmG6W4">http://goo.gl/YmG6W4</ext-link>
</td></tr><tr><td align="left">DLSSVM</td><td align="left">
Ning et&#x000a0;al. (<xref ref-type="bibr" rid="CR134">2016</xref>)</td><td align="left">
<inline-formula id="IEq19"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M38"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq19.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://goo.gl/m4ro8x">https://goo.gl/m4ro8x</ext-link>
</td></tr><tr><td align="left" rowspan="2">DSST</td><td align="left">
Danelljan et&#x000a0;al. (<xref ref-type="bibr" rid="CR44">2014</xref>)</td><td align="left">
<inline-formula id="IEq20"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M40"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq20.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://github.com/davisking/dlib">https://github.com/davisking/dlib</ext-link>
</td></tr><tr><td align="left">
King (<xref ref-type="bibr" rid="CR88">2009</xref>)</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">FCT</td><td align="left">
Zhang et&#x000a0;al. (<xref ref-type="bibr" rid="CR222">2014c</xref>)</td><td align="left">
<inline-formula id="IEq21"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M42"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq21.gif"/></alternatives></inline-formula>
</td><td align="left">
<inline-formula id="IEq22"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M44"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq22.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="http://goo.gl/Ujc5B0">http://goo.gl/Ujc5B0</ext-link>
</td></tr><tr><td align="left">HDT</td><td align="left">
Qi et&#x000a0;al. (<xref ref-type="bibr" rid="CR149">2016</xref>)</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left">
<inline-formula id="IEq23"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M46"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq23.gif"/></alternatives></inline-formula>
</td><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://goo.gl/9KgteR">https://goo.gl/9KgteR</ext-link>
</td></tr><tr><td align="left">IVT</td><td align="left">
Ross et&#x000a0;al. (<xref ref-type="bibr" rid="CR157">2008</xref>)</td><td align="left"/><td align="left">
<inline-formula id="IEq24"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M48"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq24.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="http://goo.gl/WtbOIX">http://goo.gl/WtbOIX</ext-link>
</td></tr><tr><td align="left">KCF</td><td align="left">
Henriques et&#x000a0;al. (<xref ref-type="bibr" rid="CR72">2015</xref>)</td><td align="left">
<inline-formula id="IEq25"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M50"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq25.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://github.com/joaofaro/KCFcpp">https://github.com/joaofaro/KCFcpp</ext-link>
</td></tr><tr><td align="left">LCT</td><td align="left">
Ma et&#x000a0;al. (<xref ref-type="bibr" rid="CR125">2015</xref>)</td><td align="left">
<inline-formula id="IEq26"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M52"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq26.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://goo.gl/8kaO7T">https://goo.gl/8kaO7T</ext-link>
</td></tr><tr><td align="left">LRST</td><td align="left">
Zhang et&#x000a0;al. (<xref ref-type="bibr" rid="CR227">2014d</xref>)</td><td align="left"/><td align="left">
<inline-formula id="IEq27"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M54"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq27.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="http://goo.gl/ZC9JbQ">http://goo.gl/ZC9JbQ</ext-link>
</td></tr><tr><td align="left">MDNET</td><td align="left">
Nam and Han (<xref ref-type="bibr" rid="CR132">2016</xref>)</td><td align="left">
<inline-formula id="IEq28"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M56"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq28.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left">
<inline-formula id="IEq29"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M58"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq29.gif"/></alternatives></inline-formula>
</td><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://github.com/HyeonseobNam/MDNet">https://github.com/HyeonseobNam/MDNet</ext-link>
</td></tr><tr><td align="left">MEEM</td><td align="left">
Zhang et&#x000a0;al. (<xref ref-type="bibr" rid="CR220">2014a</xref>)</td><td align="left">
<inline-formula id="IEq30"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M60"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq30.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://goo.gl/Bj6typ">https://goo.gl/Bj6typ</ext-link>
</td></tr><tr><td align="left" rowspan="2">MIL</td><td align="left">
Babenko et&#x000a0;al. (<xref ref-type="bibr" rid="CR14">2011</xref>)</td><td align="left" rowspan="2">
<inline-formula id="IEq31"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M62"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq31.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left" rowspan="2">
<ext-link ext-link-type="uri" xlink:href="http://opencv.org">http://opencv.org</ext-link>
</td></tr><tr><td align="left">
Bradski (<xref ref-type="bibr" rid="CR27">2000</xref>)</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">ORIA</td><td align="left">
Wu et&#x000a0;al. (<xref ref-type="bibr" rid="CR199">2012</xref>)</td><td align="left"/><td align="left">
<inline-formula id="IEq32"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M64"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq32.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://goo.gl/RT3zNC">https://goo.gl/RT3zNC</ext-link>
</td></tr><tr><td align="left">PF</td><td align="left">
Isard and Blake (<xref ref-type="bibr" rid="CR76">1996</xref>)</td><td align="left"/><td align="left">
<inline-formula id="IEq33"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M66"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq33.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://goo.gl/tSZcAg">https://goo.gl/tSZcAg</ext-link>
</td></tr><tr><td align="left">RPT</td><td align="left">
Li et&#x000a0;al. (<xref ref-type="bibr" rid="CR117">2015b</xref>)</td><td align="left">
<inline-formula id="IEq34"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M68"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq34.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://github.com/ihpdep/rpt">https://github.com/ihpdep/rpt</ext-link>
</td></tr><tr><td align="left">SIAM-OXF</td><td align="left">
Bertinetto et&#x000a0;al. (<xref ref-type="bibr" rid="CR22">2016b</xref>)</td><td align="left">
<inline-formula id="IEq35"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M70"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq35.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left">
<inline-formula id="IEq36"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M72"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq36.gif"/></alternatives></inline-formula>
</td><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://goo.gl/sjGgVj">https://goo.gl/sjGgVj</ext-link>
</td></tr><tr><td align="left">SPOT</td><td align="left">
Zhang and van&#x000a0;der Maaten (<xref ref-type="bibr" rid="CR225">2014</xref>)</td><td align="left">
<inline-formula id="IEq37"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M74"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq37.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left">
<inline-formula id="IEq38"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M76"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq38.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="http://visionlab.tudelft.nl/spot">http://visionlab.tudelft.nl/spot</ext-link>
</td></tr><tr><td align="left">SPT</td><td align="left">
Yang et&#x000a0;al. (<xref ref-type="bibr" rid="CR211">2014b</xref>)</td><td align="left">
<inline-formula id="IEq39"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M78"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq39.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://goo.gl/EOquai">https://goo.gl/EOquai</ext-link>
</td></tr><tr><td align="left">SRDCF</td><td align="left">
Danelljan et&#x000a0;al. (<xref ref-type="bibr" rid="CR45">2015</xref>)</td><td align="left">
<inline-formula id="IEq40"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M80"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq40.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://goo.gl/Q9d1O5">https://goo.gl/Q9d1O5</ext-link>
</td></tr><tr><td align="left">STAPLE</td><td align="left">
Bertinetto et&#x000a0;al. (<xref ref-type="bibr" rid="CR21">2016a</xref>)</td><td align="left">
<inline-formula id="IEq41"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M82"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq41.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://github.com/bertinetto/staple">https://github.com/bertinetto/staple</ext-link>
</td></tr><tr><td align="left">STCL</td><td align="left">
Zhang et&#x000a0;al. (<xref ref-type="bibr" rid="CR221">2014b</xref>)</td><td align="left">
<inline-formula id="IEq42"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M84"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq42.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://goo.gl/l29dQg">https://goo.gl/l29dQg</ext-link>
</td></tr><tr><td align="left">STRUCK</td><td align="left">
Hare et&#x000a0;al. (<xref ref-type="bibr" rid="CR69">2011</xref>)</td><td align="left">
<inline-formula id="IEq43"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M86"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq43.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="http://goo.gl/gLR93b">http://goo.gl/gLR93b</ext-link>
</td></tr><tr><td align="left">TGPR</td><td align="left">
Gao et&#x000a0;al. (<xref ref-type="bibr" rid="CR63">2014</xref>)</td><td align="left">
<inline-formula id="IEq44"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M88"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq44.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://goo.gl/EBw0WI">https://goo.gl/EBw0WI</ext-link>
</td></tr><tr><td align="left">TLD</td><td align="left">
Kalal et&#x000a0;al. (<xref ref-type="bibr" rid="CR84">2012</xref>)</td><td align="left">
<inline-formula id="IEq45"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M90"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq45.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://github.com/zk00006/OpenTLD">https://github.com/zk00006/OpenTLD</ext-link>
</td></tr></tbody></table><table-wrap-foot><p>The table reports the short name of the method, the relevant citation(s) as well as the link to the implementation used. The initials stand for: (<italic>D</italic>)iscriminative, (<italic>G</italic>)enerative, (<italic>P</italic>)art-based, (<italic>K</italic>)eypoint trackers, and <italic>NN</italic> for trackers that employ neural networks</p></table-wrap-foot></table-wrap>
</p><p id="Par46">
<italic>Part-based Trackers</italic> These trackers attempt to implicitly model the parts of an object in order to improve tracking performance. Adam et&#x000a0;al. (<xref ref-type="bibr" rid="CR1">2006</xref>) represent the object with multiple arbitrary patches. Each patch votes on potential positions and scales of the object and a robust statistic is employed to minimise the voting error. Kalal et&#x000a0;al. (<xref ref-type="bibr" rid="CR83">2010b</xref>) sample the object and the points are tracked independently in each frame by estimating optical flow. Using a forward&#x02013;backward measure, the erroneous points are identified and the remaining reliable points are utilised to compute the optimal object trajectory. Yao et&#x000a0;al. (<xref ref-type="bibr" rid="CR216">2013</xref>) adapt the latent SVM of Felzenszwalb et&#x000a0;al. (<xref ref-type="bibr" rid="CR61">2010</xref>) for online tracking, by restricting the search in the vicinity of the location of the target object in the previous frame. In comparison to the weakly supervised part-based model of Yao et&#x000a0;al. (<xref ref-type="bibr" rid="CR216">2013</xref>), in Zhang and van&#x000a0;der Maaten (<xref ref-type="bibr" rid="CR224">2013</xref>) the authors recommend an online strongly supervised part-based deformable model that learns the representation of the object and the representation of the background by training a classifier. Wang et&#x000a0;al. (<xref ref-type="bibr" rid="CR193">2015</xref>) employ a part-based tracker by estimating a direct displacement prediction of the object. A cascade of regressors is utilised to localise the parts, while the model is updated online and the regressors are initialised by multiple motion models at each frame.</p><p id="Par47">Given the wealth of available trackers, selecting appropriate trackers for deformable tracking purposes poses a difficult proposition. In order to attempt to give as broad an overview as possible, we selected trackers from each of the aforementioned categories. Therefore, in this paper we compare against 27 trackers which are outlined in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>. SRDCF&#x000a0;(Danelljan et&#x000a0;al. <xref ref-type="bibr" rid="CR45">2015</xref>), KCF&#x000a0;(Henriques et&#x000a0;al. <xref ref-type="bibr" rid="CR72">2015</xref>), LCT&#x000a0;(Ma et&#x000a0;al. <xref ref-type="bibr" rid="CR125">2015</xref>), STAPLE&#x000a0;(Bertinetto et&#x000a0;al. <xref ref-type="bibr" rid="CR21">2016a</xref>) and DSST&#x000a0;(Danelljan et&#x000a0;al. <xref ref-type="bibr" rid="CR44">2014</xref>) are all discriminative trackers based on DCFs. They all performed well in the VOT 2015&#x000a0;(Kristan et&#x000a0;al. <xref ref-type="bibr" rid="CR98">2015</xref>) challenge and DSST was the winner of VOT 2014&#x000a0;(Kristan et&#x000a0;al. <xref ref-type="bibr" rid="CR97">2014</xref>). The trackers of Danelljan et&#x000a0;al. (<xref ref-type="bibr" rid="CR46">2016</xref>), Qi et&#x000a0;al. (<xref ref-type="bibr" rid="CR149">2016</xref>); Nam and Han (<xref ref-type="bibr" rid="CR132">2016</xref>), Bertinetto et&#x000a0;al. (<xref ref-type="bibr" rid="CR22">2016b</xref>) are indicative trackers that employ neural networks and achieve top results. STRUCK&#x000a0;(Hare et&#x000a0;al. <xref ref-type="bibr" rid="CR69">2011</xref>) is a discriminative tracker that performed very well in the Online Object Tracking benchmark&#x000a0;(Wu et&#x000a0;al. <xref ref-type="bibr" rid="CR200">2013</xref>), while the more recent method of Ning et&#x000a0;al. (<xref ref-type="bibr" rid="CR134">2016</xref>) improves the computational burden of the structural SVM of STRUCK and reports superior results. SPOT&#x000a0;(Zhang and van&#x000a0;der Maaten <xref ref-type="bibr" rid="CR225">2014</xref>) is a strong performing part based tracker, CMT&#x000a0;(Nebehay and Pflugfelder <xref ref-type="bibr" rid="CR133">2015</xref>) is a strong performing keypoint based tracker, LRST&#x000a0;(Zhang et&#x000a0;al. <xref ref-type="bibr" rid="CR227">2014d</xref>) and ORIA&#x000a0;(Wu et&#x000a0;al. <xref ref-type="bibr" rid="CR199">2012</xref>) are recent generative trackers. RPT&#x000a0;(Li et&#x000a0;al. <xref ref-type="bibr" rid="CR117">2015b</xref>) is a recently proposed technique that reported state-of-the-art results on the Online Object Tracking benchmark&#x000a0;(Wu et&#x000a0;al. <xref ref-type="bibr" rid="CR200">2013</xref>). TLD&#x000a0;(Kalal et&#x000a0;al. <xref ref-type="bibr" rid="CR84">2012</xref>), MIL&#x000a0;(Babenko et&#x000a0;al. <xref ref-type="bibr" rid="CR14">2011</xref>), FCT&#x000a0;(Zhang et&#x000a0;al. <xref ref-type="bibr" rid="CR222">2014c</xref>), DF&#x000a0;(Sevilla-Lara and Learned-Miller <xref ref-type="bibr" rid="CR169">2012</xref>) and IVT&#x000a0;(Ross et&#x000a0;al. <xref ref-type="bibr" rid="CR157">2008</xref>) were included as baseline tracking methods with publicly available implementations. Finally, the CAMSHIFT and PF methods (Bradski <xref ref-type="bibr" rid="CR28">1998a</xref>; Isard and Blake <xref ref-type="bibr" rid="CR76">1996</xref>) are included as very influential trackers used in the previous decades for tracking.</p></sec><sec id="Sec8"><title>Facial Landmark Localisation</title><p id="Par48">Statistical deformable models have emerged as an important research field over the last few decades, existing at the intersection of computer vision, statistical pattern recognition and machine learning. Statistical deformable models aim to solve generic object alignment in terms of localisation of fiducial points. Although deformable models can be built for a variety of object classes, the majority of ongoing research has focused on the task of facial alignment. Recent large-scale challenges on facial alignment (Sagonas et&#x000a0;al. <xref ref-type="bibr" rid="CR160">2013b</xref>, <xref ref-type="bibr" rid="CR159">a</xref>, <xref ref-type="bibr" rid="CR162">2015</xref>) are characteristic examples of the rapid progress being made in the field.<table-wrap id="Tab3"><label>Table 3</label><caption><p>The landmark localisation methods employed in this paper</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Citation(s)</th><th align="left">Discriminative</th><th align="left">Generative</th><th align="left">Implementation</th></tr></thead><tbody><tr><td align="left" rowspan="2">AAM</td><td align="left">
Tzimiropoulos (<xref ref-type="bibr" rid="CR182">2015</xref>)</td><td align="left"/><td align="left" rowspan="2">
<inline-formula id="IEq46"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M92"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq46.gif"/></alternatives></inline-formula>
</td><td align="left" rowspan="2">
<ext-link ext-link-type="uri" xlink:href="https://github.com/menpo/menpofit">https://github.com/menpo/menpofit</ext-link>
</td></tr><tr><td align="left">
Alabort-i-Medina et&#x000a0;al. (<xref ref-type="bibr" rid="CR4">2014</xref>)</td><td align="left"/></tr><tr><td align="left" rowspan="2">ERT</td><td align="left">
Kazemi and Sullivan (<xref ref-type="bibr" rid="CR86">2014</xref>)</td><td align="left" rowspan="2">
<inline-formula id="IEq47"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M94"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq47.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left" rowspan="2">
<ext-link ext-link-type="uri" xlink:href="https://github.com/davisking/dlib">https://github.com/davisking/dlib</ext-link>
</td></tr><tr><td align="left">
King (<xref ref-type="bibr" rid="CR88">2009</xref>)</td><td align="left"/></tr><tr><td align="left">CFSS</td><td align="left">
Zhu et&#x000a0;al. (<xref ref-type="bibr" rid="CR229">2015</xref>)</td><td align="left">
<inline-formula id="IEq48"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M96"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq48.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left">
<ext-link ext-link-type="uri" xlink:href="https://github.com/zhusz/CVPR15-CFSS">https://github.com/zhusz/CVPR15-CFSS</ext-link>
</td></tr><tr><td align="left" rowspan="2">SDM</td><td align="left">
Xiong and De&#x000a0;la Torre (<xref ref-type="bibr" rid="CR205">2013</xref>)</td><td align="left" rowspan="2">
<inline-formula id="IEq49"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M98"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq49.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left" rowspan="2">
<ext-link ext-link-type="uri" xlink:href="https://github.com/menpo/menpofit">https://github.com/menpo/menpofit</ext-link>
</td></tr><tr><td align="left">
Alabort-i-Medina et&#x000a0;al. (<xref ref-type="bibr" rid="CR4">2014</xref>)</td><td align="left"/></tr></tbody></table><table-wrap-foot><p>The table reports the short name of the method, the relevant citation(s) as well as the link to the implementation used</p></table-wrap-foot></table-wrap>
</p><p id="Par49">Currently, the most commonly-used and well-studied face alignment methods can be separated into two major families: (i)&#x000a0;<italic>discriminative</italic> models that employ regression in a cascaded manner, and (ii)&#x000a0;<italic>generative</italic> models that are iteratively optimised.</p><p id="Par50">
<italic>Regression-Based Models</italic> The methodologies of this category aim to learn a regression function that regresses from the object&#x02019;s appearance (e.g. commonly handcrafted features) to the target output variables (either the landmark coordinates or the parameters of a statistical shape model). Although the history behind using linear regression in order to tackle the problem of face alignment spans back many years&#x000a0;(Cootes et&#x000a0;al. <xref ref-type="bibr" rid="CR41">2001</xref>), the research community turned towards alternative approaches due to the lack of sufficient data for training accurate regression functions. Nevertheless, recently regression-based techniques have prevailed in the field thanks to the wealth of annotated data and effective handcrafted features&#x000a0;(Lowe <xref ref-type="bibr" rid="CR124">1999</xref>; Dalal and Triggs <xref ref-type="bibr" rid="CR43">2005</xref>). Recent works have shown that excellent performance can be achieved by employing a cascade of regression functions&#x000a0;(Burgos-Artizzu et&#x000a0;al. <xref ref-type="bibr" rid="CR30">2013</xref>; Xiong and De&#x000a0;la Torre <xref ref-type="bibr" rid="CR205">2013</xref>, <xref ref-type="bibr" rid="CR206">2015</xref>; Doll&#x000e1;r et&#x000a0;al. <xref ref-type="bibr" rid="CR52">2010</xref>; Cao et&#x000a0;al. <xref ref-type="bibr" rid="CR33">2014</xref>; Kazemi and Sullivan <xref ref-type="bibr" rid="CR86">2014</xref>; Ren et&#x000a0;al. <xref ref-type="bibr" rid="CR154">2014</xref>; Asthana et&#x000a0;al. <xref ref-type="bibr" rid="CR12">2014</xref>; Tzimiropoulos <xref ref-type="bibr" rid="CR182">2015</xref>; Zhu et&#x000a0;al. <xref ref-type="bibr" rid="CR229">2015</xref>). Regression based methods can be approximately seperated into two categories depending on the nature of the regression function employed. Methods that employ a linear regression such as the supervised descent method (SDM) of Xiong and De&#x000a0;la Torre (<xref ref-type="bibr" rid="CR205">2013</xref>) tend to employ robust hand-crafted features&#x000a0;(Xiong and De&#x000a0;la Torre <xref ref-type="bibr" rid="CR205">2013</xref>; Asthana et&#x000a0;al. <xref ref-type="bibr" rid="CR12">2014</xref>; Xiong and De&#x000a0;la Torre <xref ref-type="bibr" rid="CR206">2015</xref>; Tzimiropoulos <xref ref-type="bibr" rid="CR182">2015</xref>; Zhu et&#x000a0;al. <xref ref-type="bibr" rid="CR229">2015</xref>). On the other hand, methods that employ tree-based regressors such as the explicit shape regression (ESR) method of Cao et&#x000a0;al. (<xref ref-type="bibr" rid="CR33">2014</xref>), tend to rely on data driven features that are optimised directly by the regressor (Burgos-Artizzu et&#x000a0;al. <xref ref-type="bibr" rid="CR30">2013</xref>; Cao et&#x000a0;al. <xref ref-type="bibr" rid="CR33">2014</xref>; Doll&#x000e1;r et&#x000a0;al. <xref ref-type="bibr" rid="CR52">2010</xref>; Kazemi and Sullivan <xref ref-type="bibr" rid="CR86">2014</xref>).<table-wrap id="Tab4"><label>Table 4</label><caption><p>The set of experiments conducted in this paper</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Experiment</th><th align="left">Section</th><th align="left">Tracking</th><th align="left">Detection</th><th align="left">Landmark localisation</th><th align="left">Failure checking</th><th align="left">Re-initialisation</th><th align="left">Kalman Smoothing</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">
<xref rid="Sec14" ref-type="sec">4.3</xref>
</td><td align="left"/><td align="left">
<inline-formula id="IEq50"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M100"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq50.gif"/></alternatives></inline-formula>
</td><td align="left">
<inline-formula id="IEq51"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M102"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq51.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">2</td><td align="left">
<xref rid="Sec15" ref-type="sec">4.4</xref>
</td><td align="left"/><td align="left">
<inline-formula id="IEq52"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M104"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq52.gif"/></alternatives></inline-formula>
</td><td align="left">
<inline-formula id="IEq53"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M106"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq53.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left">
<inline-formula id="IEq54"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M108"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq54.gif"/></alternatives></inline-formula>
</td><td align="left"/></tr><tr><td align="left">3</td><td align="left">
<xref rid="Sec16" ref-type="sec">4.5</xref>
</td><td align="left">
<inline-formula id="IEq55"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M110"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq55.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left">
<inline-formula id="IEq56"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M112"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq56.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">4</td><td align="left">
<xref rid="Sec17" ref-type="sec">4.6</xref>
</td><td align="left">
<inline-formula id="IEq57"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M114"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq57.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left">
<inline-formula id="IEq58"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M116"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq58.gif"/></alternatives></inline-formula>
</td><td align="left">
<inline-formula id="IEq59"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M118"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq59.gif"/></alternatives></inline-formula>
</td><td align="left">
<inline-formula id="IEq60"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M120"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq60.gif"/></alternatives></inline-formula>
</td><td align="left"/></tr><tr><td align="left">5</td><td align="left">
<xref rid="Sec18" ref-type="sec">4.7</xref>
</td><td align="left">
<inline-formula id="IEq61"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M122"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq61.gif"/></alternatives></inline-formula>
</td><td align="left">
<inline-formula id="IEq62"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M124"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq62.gif"/></alternatives></inline-formula>
</td><td align="left">
<inline-formula id="IEq63"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M126"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq63.gif"/></alternatives></inline-formula>
</td><td align="left"/><td align="left"/><td align="left">
<inline-formula id="IEq64"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark $$\end{document}</tex-math><mml:math id="M128"><mml:mo stretchy="false">&#x02713;</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq64.gif"/></alternatives></inline-formula>
</td></tr><tr><td align="left">6</td><td align="left">
<xref rid="Sec19" ref-type="sec">4.8</xref>
</td><td align="left" colspan="6">Comparison against state-of-the-art of 300&#x000a0;VW competition (Shen et&#x000a0;al. <xref ref-type="bibr" rid="CR170">2015</xref>).</td></tr></tbody></table><table-wrap-foot><p>This table is intended as an overview of the battery of experiments that were conducted, as well as providing a reference to the relevant section</p></table-wrap-foot></table-wrap>
</p><p id="Par51">
<italic>Generative Models</italic> The most dominant representative algorithm of this category is, by far, the active appearance model (AAM). AAMs consist of parametric linear models of both shape and appearance of an object, typically modelled by Principal Component Analysis (PCA). The AAM objective function involves the minimisation of the appearance reconstruction error with respect to the shape parameters. AAMs were initially proposed by Cootes et&#x000a0;al. (<xref ref-type="bibr" rid="CR40">1995</xref>, <xref ref-type="bibr" rid="CR41">2001</xref>), where the optimisation was performed by a single regression step between the current image reconstruction residual and an increment to the shape parameters. However, Matthews and Baker (<xref ref-type="bibr" rid="CR128">2004</xref>), Baker and Matthews (<xref ref-type="bibr" rid="CR15">2004</xref>) linearised the AAM objective function and optimised it using the Gauss-Newton algorithm. Following this, Gauss-Newton optimisation has been the modern method for optimising AAMs. Numerous extensions have been published, either related to the optimisation procedure&#x000a0;(Papandreou and Maragos <xref ref-type="bibr" rid="CR139">2008</xref>; Tzimiropoulos and Pantic <xref ref-type="bibr" rid="CR183">2013</xref>; Alabort-i-Medina and Zafeiriou <xref ref-type="bibr" rid="CR2">2014</xref>, <xref ref-type="bibr" rid="CR3">2015</xref>; Tzimiropoulos and Pantic <xref ref-type="bibr" rid="CR184">2014</xref>) or the model structure&#x000a0;(Tzimiropoulos et&#x000a0;al. <xref ref-type="bibr" rid="CR185">2012</xref>; Antonakos et&#x000a0;al. <xref ref-type="bibr" rid="CR8">2014</xref>; Tzimiropoulos et&#x000a0;al. <xref ref-type="bibr" rid="CR186">2014</xref>; Antonakos et&#x000a0;al. <xref ref-type="bibr" rid="CR10">2015b</xref>, <xref ref-type="bibr" rid="CR9">a</xref>).</p><p id="Par52">In recent challenges by Sagonas et&#x000a0;al. (<xref ref-type="bibr" rid="CR159">2013a</xref>, <xref ref-type="bibr" rid="CR162">2015</xref>), discriminative methods have been shown to represent the current state-of-the-art. However, in order to enable a fair comparison between types of methods we selected a representative set of landmark localisation methods to compare with in this paper. The set of landmark localisation methods used in the paper is given in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>. We chose to use ERT&#x000a0;(Kazemi and Sullivan <xref ref-type="bibr" rid="CR86">2014</xref>) as it is extremely fast and the implementation provided by King (<xref ref-type="bibr" rid="CR88">2009</xref>) is the best known implementation of a tree-based regressor. We chose CFSS&#x000a0;(Zhu et&#x000a0;al. <xref ref-type="bibr" rid="CR229">2015</xref>) as it is the current state-of-the-art on the data provided by the 300W competition of Sagonas et&#x000a0;al. (<xref ref-type="bibr" rid="CR159">2013a</xref>). We used the Gauss-Newton Part-based AAM of Tzimiropoulos and Pantic (<xref ref-type="bibr" rid="CR184">2014</xref>) as the top performing generative localisation method, as provided by the Menpo Project&#x000a0;(Alabort-i-Medina et&#x000a0;al. <xref ref-type="bibr" rid="CR4">2014</xref>). Finally, we also demonstrated an SDM&#x000a0;(Xiong and De&#x000a0;la Torre <xref ref-type="bibr" rid="CR205">2013</xref>) as implemented by Alabort-i-Medina et&#x000a0;al. (<xref ref-type="bibr" rid="CR4">2014</xref>) as a baseline.</p></sec></sec><sec id="Sec9"><title>Experiments</title><p id="Par53">In this section, details of the experimental evaluation are established. Firstly, the datasets employed for the evaluation, training and validation are introduced in Sect.&#x000a0;<xref rid="Sec10" ref-type="sec">4.1</xref>. Next, Sect.&#x000a0;<xref rid="Sec11" ref-type="sec">4.2</xref> provides details of the training procedures and of the implementations that are relevant to all experiments. Following this, in Sects.&#x000a0;<xref rid="Sec14" ref-type="sec">4.3</xref>&#x02212;<xref rid="Sec18" ref-type="sec">4.7</xref>, we describe the set of experiments that were conducted in this paper, which are summarised in Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>. Finally, experimental Sect.&#x000a0;<xref rid="Sec19" ref-type="sec">4.8</xref> compares the best results from the previous experiments to the winners of the 300&#x000a0;VW competition in&#x000a0;Shen et&#x000a0;al. (<xref ref-type="bibr" rid="CR170">2015</xref>).</p><p id="Par54">In the following sections, due to the very large amount of methodologies taken into account, we provide a summary of all the results as tables and only the top five methods as graphs for clarity. Please refer to the supplementary material for an extensive report of the experimental results. Additionally, we provide videos with the tracking results for the experiments of Sects.&#x000a0;<xref rid="Sec14" ref-type="sec">4.3</xref>, and&#x000a0;<xref rid="Sec16" ref-type="sec">4.5</xref> for qualitative comparison.<xref ref-type="fn" rid="Fn5">5</xref>
<sup>,</sup>
<xref ref-type="fn" rid="Fn6">6</xref>
</p><sec id="Sec10"><title>Dataset</title><p id="Par57">All the comparisons are conducted in the testset of the 300&#x000a0;VW dataset collected by Shen et&#x000a0;al. (<xref ref-type="bibr" rid="CR170">2015</xref>). This recently introduced dataset contains 114 videos (50 for training and 64 for testing). The videos are separated into the following 3 categories:<list list-type="bullet"><list-item><p id="Par58">
<italic>Category 1</italic> This category is composed of videos captured in well-lit environments without any occlusions.</p></list-item><list-item><p id="Par59">
<italic>Category 2</italic> The second category includes videos captured in unconstrained illumination conditions.</p></list-item><list-item><p id="Par60">
<italic>Category 3</italic> The final category consists of video sequences captured in totally arbitrary conditions (including severe occlusions and extreme illuminations).</p></list-item></list>Each video includes only one person and is annotated using the 68 point mark-up employed by Gross et&#x000a0;al. (<xref ref-type="bibr" rid="CR68">2010</xref>) and Sagonas et&#x000a0;al. (<xref ref-type="bibr" rid="CR162">2015</xref>) for Multi-PIE and 300W databases, respectively. All videos include between 1500 frames and 3000 frames with a large variety of expressions, poses and capturing conditions, which makes the dataset very challenging for deformable facial tracking. A number of exemplar images, which are indicative of the challenges of each category, are provided in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>. We note that, in contrast to the results of Shen et&#x000a0;al. (<xref ref-type="bibr" rid="CR170">2015</xref>) in the original 300&#x000a0;VW competition, we used the most recently provided annotations (See footnote 1) which have been corrected and do not contain missing frames. Therefore, we also provide updated results following the participants of the 300&#x000a0;VW competition.</p><p id="Par61">The public datasets of IBUG (Sagonas et&#x000a0;al. <xref ref-type="bibr" rid="CR159">2013a</xref>), HELEN (Le et&#x000a0;al. <xref ref-type="bibr" rid="CR103">2012</xref>), AFW (Zhu and Ramanan <xref ref-type="bibr" rid="CR230">2012</xref>) and LFPW (Belhumeur et&#x000a0;al. <xref ref-type="bibr" rid="CR20">2013</xref>) are employed for training all the landmark localisation methods. This is further explained in Sect.&#x000a0;<xref rid="Sec12" ref-type="sec">4.2.1</xref> below.<fig id="Fig2"><label>Fig. 2</label><caption><p>Example frames from the 300&#x000a0;VW dataset by Shen et&#x000a0;al. (<xref ref-type="bibr" rid="CR170">2015</xref>). Each <italic>row</italic> contains 10 exemplar images from each category, that are indicative of the challenges that characterise the videos of the category. <bold>a</bold> Category 1. <bold>b</bold> Category 2. <bold>c</bold> Category 3</p></caption><graphic xlink:href="11263_2017_999_Fig2_HTML" id="MO2"/></fig>
</p></sec><sec id="Sec11"><title>Implementation Details</title><p id="Par62">The authors&#x02019; implementations are utilised for the trackers, as outlined in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>. Similarly, the face detectors&#x02019; implementations are outlined in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>. HOG&#x000a0;+&#x000a0;SVM was provided by the Dlib project of King (<xref ref-type="bibr" rid="CR89">2015</xref>, <xref ref-type="bibr" rid="CR88">2009</xref>), the Weakly Supervised DPM (DPM) (Felzenszwalb et&#x000a0;al. <xref ref-type="bibr" rid="CR61">2010</xref>) was the model provided by Mathias et&#x000a0;al. (<xref ref-type="bibr" rid="CR127">2014</xref>) and the code of Dubout and Fleuret (<xref ref-type="bibr" rid="CR54">2012</xref>, <xref ref-type="bibr" rid="CR55">2013</xref>) was used to perform the detection. Moreover, the Strongly Supervised DPM (SS-DPM) of Zhu and Ramanan (<xref ref-type="bibr" rid="CR230">2012</xref>) was provided by the authors and, finally, the OpenCV implementation by Bradski (<xref ref-type="bibr" rid="CR27">2000</xref>) was used for the VJ detector (Viola and Jones <xref ref-type="bibr" rid="CR190">2004</xref>). The default parameters were used in all cases. The pre-trained detectors&#x02019; models were utilised; only the most confident detection was exported per frame, there was no effort to maximise the overlap with the ground-truth bounding box; in all videos there is only one person per frame.</p><p id="Par63">For face alignment, as outlined in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>, the implementation of CFSS provided by Zhu et&#x000a0;al. (<xref ref-type="bibr" rid="CR229">2015</xref>) is adopted, while the implementations provided by Alabort-i-Medina et&#x000a0;al. (<xref ref-type="bibr" rid="CR4">2014</xref>) in the Menpo Project are employed for the patch-based AAM of Tzimiropoulos and Pantic (<xref ref-type="bibr" rid="CR184">2014</xref>) and the SDM of Xiong and De&#x000a0;la Torre (<xref ref-type="bibr" rid="CR205">2013</xref>). Lastly, the implementation of ERT (Kazemi and Sullivan <xref ref-type="bibr" rid="CR86">2014</xref>) is provided by King (<xref ref-type="bibr" rid="CR88">2009</xref>) in the Dlib library. For the three latter methods, following the original papers and the code&#x02019;s documentation, several parameters were validated and chosen based on the results in a validation set that consisted of a few videos from the 300&#x000a0;VW training set.</p><p id="Par64">The details of the parameters utilised for the patch-based AAM, SDM and ERT are the following: For AAM, we used the algorithm of Tzimiropoulos and Pantic (<xref ref-type="bibr" rid="CR184">2014</xref>) and applied a 2-level Gaussian pyramid with 4 and 10 shape components, and 60 and 150 appearance components in each scale, respectively. For the SDM, a 4-level Gaussian pyramid was employed. SIFT (Lowe <xref ref-type="bibr" rid="CR124">1999</xref>) feature vectors of length 128 were extracted at the first 3 scales, using RootSIFT by Arandjelovi&#x00107; and Zisserman (<xref ref-type="bibr" rid="CR11">2012</xref>). Raw pixel intensities were used at the highest scale.</p><p id="Par65">
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Exemplar deformable tracking results that are indicative of the fitting quality that corresponds to each error value for all video categories</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" colspan="2"><p>
<graphic position="anchor" xlink:href="11263_2017_999_Figa_HTML" id="MO3"/>
</p></td></tr></tbody></table><table-wrap-foot><p>The area under the curve (AUC) and failure rate for all the experiments are computed based on the Cumulative error distributions (CED) limited at maximum error of 0.08</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab6"><label>Table 6</label><caption><p>Results for experiment 1 of Sect.&#x000a0;<xref rid="Sec14" ref-type="sec">4.3</xref> (detection&#x000a0;+&#x000a0;landmark localisation) (Color table online)</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" colspan="2"><p>
<graphic position="anchor" xlink:href="11263_2017_999_Figb_HTML" id="MO4"/>
</p></td></tr></tbody></table><table-wrap-foot><p>The area under the curve (AUC) and Failure Rate are reported. The top four performing curves are highlighted for each video category. The current implementation of HR-TF cannot be executed to CPU mode, thus it would be unfair for the rest of the timing comparisons to include its GPU performance</p></table-wrap-foot></table-wrap>
</p><p id="Par66">Part of the experiments was conducted on the cloud software of Koukis et&#x000a0;al. (<xref ref-type="bibr" rid="CR95">2013</xref>) and the web application of P&#x000e9;rez and Granger (<xref ref-type="bibr" rid="CR144">2007</xref>), while the rest of the functionality was provided by the Python libraries of Alabort-i-Medina et&#x000a0;al. (<xref ref-type="bibr" rid="CR4">2014</xref>), Pedregosa et&#x000a0;al. (<xref ref-type="bibr" rid="CR142">2011</xref>). The source code as well as the list of errors for the top methods will be released for the research community in the link <ext-link ext-link-type="uri" xlink:href="https://github.com/grigorisg9gr/deformable%5ftracking%5freview%5fijcv2016">https://github.com/grigorisg9gr/deformable_tracking_review_ijcv2016</ext-link>.</p><sec id="Sec12"><title>Landmark Localisation Training</title><p id="Par67">All the landmark localisation methods were trained with respect to the 68 facial points mark-up employed by Sagonas et&#x000a0;al. (<xref ref-type="bibr" rid="CR159">2013a</xref>, <xref ref-type="bibr" rid="CR162">2015</xref>) in 300W, while the rest of the parameters were determined via cross-validation. Again, this validation set consisted of frames from the 300&#x000a0;VW trainset, as well as 60 privately collected images with challenging poses. All of the discriminative landmark localisation methods (SDM, ERT, CFSS) were trained from images in the public datasets of IBUG (Sagonas et&#x000a0;al. <xref ref-type="bibr" rid="CR159">2013a</xref>), HELEN (Le et&#x000a0;al. <xref ref-type="bibr" rid="CR103">2012</xref>), AFW (Zhu and Ramanan <xref ref-type="bibr" rid="CR230">2012</xref>) and LFPW (Belhumeur et&#x000a0;al. <xref ref-type="bibr" rid="CR20">2013</xref>). The generative AAM was trained on less data, since generative methods do not benefit as strongly from large training datasets. The training data used for the AAM was the recently released 300 images from the 600W dataset (Sagonas et&#x000a0;al. <xref ref-type="bibr" rid="CR162">2015</xref>), 500 challenging images from LFPW (Belhumeur et&#x000a0;al. <xref ref-type="bibr" rid="CR20">2013</xref>) and the 135 images of the IBUG dataset (Sagonas et&#x000a0;al. <xref ref-type="bibr" rid="CR159">2013a</xref>).</p><p id="Par68">Discriminative landmark localisation methods are tightly coupled with the initialisation statistics, as they learn to model a given variance of initialisations. Therefore, it is necessary to re-train each discriminative method for each face detection method employed. This allows the landmark localisation methods to correctly model the large amount of variance present between detectors. On aggregate 5 different detector and landmark localisation models are trained. One for each detector and landmark localisation pair (totalling 4) and a single model trained using a validation set that estimates the variance of the ground truth bounding box throughout the sequences. This model is used for all trackers.</p></sec><sec id="Sec13"><title>Quantitative Metrics</title><p id="Par69">The errors reported for all the following experiments are with respect to the landmark localisation error. The error metric employed is the mean Euclidean distance of the 68 points, normalised by the diagonal of the ground truth bounding box (<inline-formula id="IEq65"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sqrt{width^2 + height^2}$$\end{document}</tex-math><mml:math id="M130"><mml:msqrt><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:msup><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq65.gif"/></alternatives></inline-formula>). This metric was chosen as it is robust to changes in head pose which are frequent within the 300&#x000a0;VW sequences. The graphs that are shown are cumulative error distribution (CED) plots that provide the proportion of images less than or equal to a particular error. We also provide summary tables with respect to the Area Under the Curve (AUC) of the CED plots, considered up to a maximum error. Errors above this maximum threshold, which is fixed to 0.08, are considered failures to accurately localise the facial landmarks. Therefore, we also report the failure rate, as a percentage, which marks the proportion of images that are not considered within the CED plots. Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref> shows some indicative examples of the deformable fitting quality that corresponds to each error value for all video categories. When ranking methods, we consider the AUC as the primary statistic and only resort to considering the failure rate in cases where there is little distinction between methods&#x02019; AUC values.</p><p id="Par70">The indicative speed metric (times) reported in the outcomes is measured on 100 frames of a single video with <inline-formula id="IEq66"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$640 \times 360$$\end{document}</tex-math><mml:math id="M132"><mml:mrow><mml:mn>640</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>360</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq66.gif"/></alternatives></inline-formula> resolution. Note that the utlised detectors&#x02019; performance is highly affected by the resolution. The times were measured in a single machine with a i7 processor, 3.6 GHz, all in CPU mode, with 8GB RAM and report the time in seconds. The implementations were not optimised to minimise the computational complexity, i.e. the public implementations in C/C++ have a considerable advantage.<fig id="Fig3"><label>Fig. 3</label><caption><p>Results for experiment 1 of Sect.&#x000a0;<xref rid="Sec14" ref-type="sec">4.3</xref> (detection&#x000a0;+&#x000a0;landmark localisation). The top 5 performing curves are highlighted in each legend. Please see Table&#x000a0;<xref rid="Tab6" ref-type="table">6</xref> for a full summary</p></caption><graphic xlink:href="11263_2017_999_Fig3_HTML" id="MO5"/></fig>
<fig id="Fig4"><label>Fig. 4</label><caption><p>This figure gives a diagram of the reinitialisation scheme proposed in Sect.&#x000a0;<xref rid="Sec15" ref-type="sec">4.4</xref>. Specifically, in case the face detector does not return a bounding box for a frame, the bounding box of the previous frame is used as a successful detection for the missing frame</p></caption><graphic xlink:href="11263_2017_999_Fig4_HTML" id="MO6"/></fig>
<table-wrap id="Tab7"><label>Table 7</label><caption><p>Results for experiment 2 of Sect.&#x000a0;<xref rid="Sec15" ref-type="sec">4.4</xref> (detection&#x000a0;+&#x000a0;landmark localisation&#x000a0;+&#x000a0;initialisation from previous frame) (Color table online)</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" colspan="2"><p>
<graphic position="anchor" xlink:href="11263_2017_999_Figc_HTML" id="MO7"/>
</p></td></tr></tbody></table><table-wrap-foot><p>The area under the curve (AUC) and failure rate are reported. The top four performing curves are highlighted for each video category</p></table-wrap-foot></table-wrap>
</p></sec></sec><sec id="Sec14"><title>Experiment 1: Detection and Landmark Localisation</title><p id="Par71">In this experiment, we validate the most frequently used facial deformable tracking strategy, i.e. performing face detection followed by landmark localisation <italic>on each frame independently</italic>. If a detector fails to return a frame, that frame is considered as having infinite error and thus will appear as part of the failures in Table&#x000a0;<xref rid="Tab6" ref-type="table">6</xref>. Note that the AUC is robust to the use of infinite errors. In frames where multiple bounding boxes are returned, the box with the highest confidence is kept, limiting the results of the detectors to a single bounding box per image. A high level diagram explaining the detection procedure for this experiment is given by Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>.</p><p id="Par72">Specifically, in this experiment we consider the 8 face detectors of Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> (DPM, HR-TF, MTCNN, NPD, SS-DPM, HOG <inline-formula id="IEq67"><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$+$$\end{document}</tex-math><mml:math id="M134"><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq67.gif"/></alternatives></inline-formula> SVM, VJ, VPHR) with the 4 landmark localisation techniques of Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> (AAM, CFSS, ERT, SDM), for a total of 32 results. The results of the experiment are given in Table&#x000a0;<xref rid="Tab6" ref-type="table">6</xref> and Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>. The results indicate that the AAM performs poorly as it achieves the lowest performance across all face detectors. The discriminative CFSS and ERT landmark localisation methods consistently outperform SDM. From the detectors point of view, it seems that the strongly supervised DPM (SS-DPM) is the worst and provides the highest failure rates. On the other hand, the weakly supervised DPM (DPM) outperforms the rest of the detectors in the first two categories in terms of both accuracy (i.e. AUC) and robustness (i.e. Failure Rate), while in the third one, the deep detector of Zhang et&#x000a0;al. (<xref ref-type="bibr" rid="CR223">2016</xref>) outperforms marginally DPM. In all three categories the state-of-the-art deep networks fetch top results, however they do not seem to be consistently better than DPM or VPHR of Kumar et&#x000a0;al. (<xref ref-type="bibr" rid="CR100">2015</xref>). The detailed graphs per method (32 methods in total), as well as a video with the results of the top five methods (see footnote 5) are deferred to the supplementary material.</p></sec><sec id="Sec15"><title>Experiment 2: Detection and Landmark Localisation with Reinitialisation</title><p id="Par73">Complementing the experiments of Sect.&#x000a0;<xref rid="Sec14" ref-type="sec">4.3</xref>, the same set-up was utilised to study the effect of missed frames by assuming a first order Markov dependency. If the detector does not return a bounding box in a frame, the bounding box of the previous frame is used as a successful detection for the missing frame. This procedure is depicted in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>. Given that the frame rate of the input videos is adequately high (over 20&#x000a0;fps), this assumption is a reasonable one. The results of this experiment are summarised in Table&#x000a0;<xref rid="Tab7" ref-type="table">7</xref> and in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>. As expected, the ranking of the methods is almost identical as the previous experiment of Sect.&#x000a0;<xref rid="Sec14" ref-type="sec">4.3</xref>, with the minor differences emerging from the threshold of the different detectors. For instance, the SVM <inline-formula id="IEq68"><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$+$$\end{document}</tex-math><mml:math id="M136"><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq68.gif"/></alternatives></inline-formula> HOG that has a high threshold, i.e. in the previous experiment it &#x02018;missed&#x02019; several challenging frames, can benefit further from the Markov dependency, while the VPHR one has exactly the same statistics as it returned a detection in every single frame in the previous experiment.</p><p id="Par74">In order to better investigate the effect of this reinitialisation scheme, we also provide Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> that directly shows the improvement. Specifically, we plot the CED curves with and without the reinitialisation strategy for 3 top performing methods, as well as the 3 techniques for which the highest improvement is achieved. It becomes evident that the top performing methods from Sect.&#x000a0;<xref rid="Sec14" ref-type="sec">4.3</xref> do not benefit from reinitialisation, since the improvement is marginal. This is explained by the fact that these methods already achieve a very high true positive rate. The largest difference is observed for methods that utilise AAM. As shown by Antonakos et&#x000a0;al. (<xref ref-type="bibr" rid="CR10">2015b</xref>), AAMs are very sensitive to initialisation, due to the nature of Gauss-Newton optimisation. Additionally, note that we have not attempted to apply any kind of greedy approach for improving the detectors&#x02019; bounding boxes in order to provide a better AAM initialisation. Since the initialisation of a frame with failed detection is achieved by the bounding box of the previous frame&#x02019;s landmarks, it is highly likely that its area will be well constrained to include only the facial parts and not the forehead or background. This kind of initialisation is very beneficial for AAMs, which justifies the large improvements that are shown in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>. For the graphs that correspond to all 32 methods, please refer to the supplementary material.</p></sec><sec id="Sec16"><title>Experiment 3: Model-free Tracking and Landmark Localisation</title><p id="Par75">In this section, we provide, to the best of our knowledge, the first detailed analysis of the performance of model free trackers for tracking &#x0201c;in-the-wild&#x0201d; facial sequences. For this reason, we have considered a large number of trackers in order to attempt to give a balanced overview of the performance of modern model trackers for deformable face alignment. The 27 trackers considered in this section are summarised in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>. To initialise all trackers, the tightest possible bounding box of the ground truth facial landmarks is provided as the initial tracker state. We also include a baseline method, which appears in results Table&#x000a0;<xref rid="Tab8" ref-type="table">8</xref>, referred to as PREV, which is defined as applying the landmark localisation methods initialised from the bounding box of the result in the previous frame. Obviously this scheme is highly sensitive to drifting and therefore we have included it as a basic baseline that does not include any model free tracking. A high level diagram explaining the detection procedure for this experiment is given by Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>.</p><p id="Par76">Specifically, in this experiment we consider the 27 model free trackers of Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>, plus the PREV baseline, with the 4 landmark localisation techniques of Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> (AAM, CFSS, ERT, SDM), for a total of 112 results. The results of the experiment are given in Table&#x000a0;<xref rid="Tab8" ref-type="table">8</xref> and Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>. Please see the supplementary material for full statistics.</p><p id="Par77">By inspecting the results, we can firstly notice that most generative trackers perform poorly (i.e. ORIA, DF, FCT, IVT), except LRST which achieves decent performance for the most challenging video category.The discriminative approaches of SRDCF and SPOT are consistently performing very well, however the trackers employing deep neural networks fetch the most accurate outcomes, consistent with the latest VOT competition outcomes. Additionally, similar to the face detection experiments, the combination of all trackers with CFSS returns the best result, whereas AAM constantly demonstrates the poorest performance. Finally, it becomes evident that a straightforward application of the simplistic baseline approach (PREV) is not suitable for deformable tracking, even though it is surprisingly outperforming some model free trackers, such as DF, ORIA and FCT. For the curves that correspond to all 112 methods as well as a video with the tracking result of the top five methods (see footnote 6), please refer to the supplementary material.<fig id="Fig5"><label>Fig. 5</label><caption><p>Results for experiment 2 of Sect.&#x000a0;<xref rid="Sec15" ref-type="sec">4.4</xref> (detection&#x000a0;+&#x000a0;landmark localisation&#x000a0;+&#x000a0;initialisation from previous frame). The top five performing <italic>curves</italic> are highlighted in each legend. Please see Table&#x000a0;<xref rid="Tab7" ref-type="table">7</xref> for a full summary</p></caption><graphic xlink:href="11263_2017_999_Fig5_HTML" id="MO8"/></fig>
<fig id="Fig6"><label>Fig. 6</label><caption><p>Results for experiment 2 of Sect.&#x000a0;<xref rid="Sec15" ref-type="sec">4.4</xref> (detection&#x000a0;+&#x000a0;landmark localisation&#x000a0;+&#x000a0;initialisation from previous frame). These results show the effect of initialisation from the previous frame, in comparison to missing detections. The top three performing results are given in <italic>red</italic>, <italic>green</italic> and <italic>blue</italic>, respectively, and the top three most improved are given in <italic>cyan</italic>, <italic>yellow</italic> and <italic>brown</italic>, respectively. The <italic>dashed lines</italic> represent the results before the reinitialisation strategy is applied, <italic>solid lines</italic> are after (Color figure online)</p></caption><graphic xlink:href="11263_2017_999_Fig6_HTML" id="MO9"/></fig>
</p></sec><sec id="Sec17"><title>Experiment 4: Failure Checking and Tracking Reinitialisation</title><p id="Par78">Complementing the experiments of Sect.&#x000a0;<xref rid="Sec16" ref-type="sec">4.5</xref>, we investigate the improvement in performance of performing failure checking during tracking. Here we define failure checking as the process of determining whether or not the currently tracked object is a face. Given that we have prior knowledge of the class of object we are tracking, namely faces, this enables us to train an offline classifier that attempts to determine whether a given input is a face or not. Furthermore, since we are also applying landmark localisation, we can perform a strong classification by using the facial landmarks as position priors when extracting features for the failure checking. To train the failure checking classifier, we perform the following methodology:<table-wrap id="Tab8"><label>Table 8</label><caption><p>Results for experiment 3 of Sect.&#x000a0;<xref rid="Sec16" ref-type="sec">4.5</xref> (model free tracking&#x000a0;+&#x000a0;landmark localisation) (Color table online)</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" colspan="2"><p>
<graphic position="anchor" xlink:href="11263_2017_999_Figd_HTML" id="MO10"/>
</p></td></tr><tr><td align="left" colspan="2"><p>
<graphic position="anchor" xlink:href="11263_2017_999_Fige_HTML" id="MO11"/>
</p></td></tr></tbody></table></table-wrap>
</p><p id="Par79">
<fig id="Fig7"><label>Fig. 7</label><caption><p>Results for experiment 3 of Sect.&#x000a0;<xref rid="Sec16" ref-type="sec">4.5</xref> (model free tracking&#x000a0;+&#x000a0;landmark localisation). The top five performing <italic>curves</italic> are highlighted in each legend. Please see Table&#x000a0;<xref rid="Tab8" ref-type="table">8</xref> for a full summary</p></caption><graphic xlink:href="11263_2017_999_Fig7_HTML" id="MO12"/></fig>
</p><p id="Par80">
<list list-type="order"><list-item><p id="Par81">For all images in the Landmark Localisation training set, extract a fixed sized patch around each of the 68 landmarks and compute HOG (Dalal and Triggs <xref ref-type="bibr" rid="CR43">2005</xref>) features for each patch. These patches are the positive training samples.</p></list-item><list-item><p id="Par82">Generate negative training samples by perturbing the ground truth bounding box, extracting fixed size patches and computing HOG.</p></list-item><list-item><p id="Par83">Train an SVM classifier using the positive and negative samples.</p></list-item></list>For the experiments in this section, we use a fixed patch size of <inline-formula id="IEq69"><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$18 \times 18$$\end{document}</tex-math><mml:math id="M138"><mml:mrow><mml:mn>18</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq69.gif"/></alternatives></inline-formula> pixels, with 100 negative patches sampled for each positive patch. The failure checking classification threshold is chosen via cross-validation on two sequences from the 300&#x000a0;VW training videos. Any hyper-parameters of the SVM are also trained using these two validation videos.</p><p id="Par84">Given the failure detector, our restart procedure, is as follows:<list list-type="bullet"><list-item><p id="Par85">Classify the current frame to determine if the tracking has failed. If a failure is verified, perform a restart, otherwise continue.</p></list-item><list-item><p id="Par86">Following the convention of the VOT challenges by Kristan et&#x000a0;al. (<xref ref-type="bibr" rid="CR96">2013</xref>, <xref ref-type="bibr" rid="CR97">2014</xref>, <xref ref-type="bibr" rid="CR98">2015</xref>), we attempt to reduce the probability that poor trackers will overly rely on the output of the failure detection system. In the worst case, a very poor tracker would fail on most frames and thus the accuracy of the detector would be validated rather than the tracker itself. Therefore, when a failure is identified, the tracker is allowed to continue for 10 more frames. The results from the drifting tracker are used in these 10 frames in order reduce the affect of the detector. The tracker is then reinitialised at the frame it was first detected as failing at. The next 10 frames, as previously described, already have results computed and therefore no landmark localisation or failure checking is performed in these frames. At the 11th frame, the tracker continues as normal, with landmark localisation and failure checking.</p></list-item><list-item><p id="Par87">In the unlikely event that the detector fails to detect the face, the previous frame is used as described in Sect.&#x000a0;<xref rid="Sec15" ref-type="sec">4.4</xref>.</p></list-item></list>The diagram given in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref> gives a pictorial representation of this scheme.<fig id="Fig8"><label>Fig. 8</label><caption><p>This figure gives a diagram of the reinitialisation scheme proposed in Sect.&#x000a0;<xref rid="Sec17" ref-type="sec">4.6</xref> for tracking with failure detection. For all frames after the first, the result of the current landmark localisation is used to decide whether or not a face is still being tracked. If the classification fails, a re-detection is performed and the tracker is reinitialised with the bounding box returned by the detector</p></caption><graphic xlink:href="11263_2017_999_Fig8_HTML" id="MO13"/></fig>
<table-wrap id="Tab9"><label>Table 9</label><caption><p>Results for experiment 4 of Sect.&#x000a0;<xref rid="Sec17" ref-type="sec">4.6</xref> (model free tracking&#x000a0;+&#x000a0;landmark localisation&#x000a0;+&#x000a0;failure checking) (Color table online)</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" colspan="2"><p>
<graphic position="anchor" xlink:href="11263_2017_999_Figf_HTML" id="MO14"/>
</p></td></tr></tbody></table><table-wrap-foot><p>The area under the curve (AUC) and failure rate are reported. The top 3 performing curves are highlighted for each video category</p></table-wrap-foot></table-wrap>
</p><p id="Par88">
<fig id="Fig9"><label>Fig. 9</label><caption><p>Results for experiment 4 of Sect.&#x000a0;<xref rid="Sec17" ref-type="sec">4.6</xref> (model free tracking&#x000a0;+&#x000a0;landmark localisation&#x000a0;+&#x000a0;failure checking). The top five performing <italic>curves</italic> are highlighted in each legend. Please see Table&#x000a0;<xref rid="Tab9" ref-type="table">9</xref> for a full summary</p></caption><graphic xlink:href="11263_2017_999_Fig9_HTML" id="MO15"/></fig>
<fig id="Fig10"><label>Fig. 10</label><caption><p>Results for experiment 4 of Sect.&#x000a0;<xref rid="Sec17" ref-type="sec">4.6</xref> (model free tracking&#x000a0;+&#x000a0;landmark localisation&#x000a0;+&#x000a0;failure checking). These results show the effect of the failure checking, in comparison to only tracking. The results are coloured by their performance <italic>red</italic>, <italic>green</italic>, <italic>blue</italic> and <italic>orange</italic>, respectively. The <italic>dashed lines</italic> represent the results before the reinitialisation strategy is applied, <italic>solid lines</italic> are after (Color figure online)</p></caption><graphic xlink:href="11263_2017_999_Fig10_HTML" id="MO16"/></fig>
</p><p id="Par89">The results of this experiment are given in Table&#x000a0;<xref rid="Tab9" ref-type="table">9</xref> and Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref>. In contrast to Sect.&#x000a0;<xref rid="Sec16" ref-type="sec">4.5</xref>, we only perform the experiments on a subset of the total trackers using CFSS. We use 3 among the top performing trackers (SRDCF, RPT, SPOT) as well as FCT which had mediocre performance in Sect.&#x000a0;<xref rid="Sec16" ref-type="sec">4.5</xref>. The results indicate that SRDCF is the best model free tracking methodology for the task.</p><p id="Par90">In order to better investigate the effect of this failure checking scheme, we also provide Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> which shows the differences between the initial tracking results of Sect.&#x000a0;<xref rid="Sec16" ref-type="sec">4.5</xref> and the results after applying failure detection. The performance of top trackers (i.e. SRDCF, SPOT, RPT) does not improve much, which is expected since they are already able to return a robust tracking result. However, FCT benefits from the failure checking process, which apparently minimises its drifting issues.<table-wrap id="Tab10"><label>Table 10</label><caption><p>Results for experiment 5 of Sect.&#x000a0;<xref rid="Sec18" ref-type="sec">4.7</xref> (Kalman Smoothing) (Color table online)</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" colspan="2"><p>
<graphic position="anchor" xlink:href="11263_2017_999_Figg_HTML" id="MO17"/>
</p></td></tr></tbody></table><table-wrap-foot><p>The area under the curve (AUC) and failure rate are reported. The top four performing curves are highlighted for each video category</p></table-wrap-foot></table-wrap>
</p></sec><sec id="Sec18"><title>Experiment 5: Kalman Smoothing</title><p id="Par91">In this section, we report the effect of performing Kalman Smoothing (Kalman <xref ref-type="bibr" rid="CR85">1960</xref>) on the results of the detectors of Sect.&#x000a0;<xref rid="Sec14" ref-type="sec">4.3</xref> and the trackers of Sect.&#x000a0;<xref rid="Sec16" ref-type="sec">4.5</xref>. This experiment is designed to highlight the stability of the current landmark localisation methods with respect to noisy movement between frames (or jittering as it often known). However, when attempting to smooth the trajectories of the tracked bounding boxes themselves, we found an extremely negative effect on the results. Therefore, to remove jitter from the results we perform Kalman smoothing on the landmarks themselves. To robustly smooth the landmark trajectories, a generic facial shape model is constructed in a similar manner as described in the AAM literature by Cootes et&#x000a0;al. (<xref ref-type="bibr" rid="CR41">2001</xref>). Specifically, given the sparse shape of the face consisting of <italic>n</italic> landmark points, we denote the coordinates of the <italic>i</italic>-th landmark point within the Cartesian space of the image <inline-formula id="IEq70"><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {I}$$\end{document}</tex-math><mml:math id="M140"><mml:mi mathvariant="bold">I</mml:mi></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq70.gif"/></alternatives></inline-formula> as <inline-formula id="IEq71"><alternatives><tex-math id="M141">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {x}_i=[x_i,y_i]^T$$\end{document}</tex-math><mml:math id="M142"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq71.gif"/></alternatives></inline-formula>. Then a <italic>shape instance</italic> of the face is given by the <inline-formula id="IEq72"><alternatives><tex-math id="M143">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2n\times 1$$\end{document}</tex-math><mml:math id="M144"><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq72.gif"/></alternatives></inline-formula> vector <inline-formula id="IEq73"><alternatives><tex-math id="M145">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {s} = \left[ \mathbf {x}_1^T,\ldots ,\mathbf {x}_n^T\right] ^T = \left[ x_1,y_1,\ldots ,x_n,y_n\right] ^T$$\end{document}</tex-math><mml:math id="M146"><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mfenced close="]" open="[" separators=""><mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mfenced><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mfenced><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq73.gif"/></alternatives></inline-formula>. Given a set of <italic>N</italic> such shape samples <inline-formula id="IEq74"><alternatives><tex-math id="M147">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\{\mathbf {s}^1,\ldots ,\mathbf {s}^N\}$$\end{document}</tex-math><mml:math id="M148"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq74.gif"/></alternatives></inline-formula>, a parametric statistical subspace of the object&#x02019;s shape variance can be retrieved by first applying Generalised Procrustes Analysis on the shapes to normalise them with respect to the global similarity transform (i.e., scale, in-plane rotation and translation) and then using Principal Component Analysis (PCA). The resulting <italic>shape model</italic>, denoted as <inline-formula id="IEq75"><alternatives><tex-math id="M149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\{\mathbf {U}_s,\bar{\mathbf {s}}\}$$\end{document}</tex-math><mml:math id="M150"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi mathvariant="bold">U</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">&#x000af;</mml:mo></mml:mrow></mml:mover><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq75.gif"/></alternatives></inline-formula>, consists of the orthonormal basis <inline-formula id="IEq76"><alternatives><tex-math id="M151">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {U}_s\in \mathbb {R}^{2n\times n_s}$$\end{document}</tex-math><mml:math id="M152"><mml:mrow><mml:msub><mml:mi mathvariant="bold">U</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq76.gif"/></alternatives></inline-formula> with <inline-formula id="IEq77"><alternatives><tex-math id="M153">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_s$$\end{document}</tex-math><mml:math id="M154"><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq77.gif"/></alternatives></inline-formula> eigenvectors and the mean shape vector <inline-formula id="IEq78"><alternatives><tex-math id="M155">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar{\mathbf {s}}\in \mathbb {R}^{2n}$$\end{document}</tex-math><mml:math id="M156"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">&#x000af;</mml:mo></mml:mrow></mml:mover><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq78.gif"/></alternatives></inline-formula>. This parametric model can be used to generate new shape instances as <inline-formula id="IEq79"><alternatives><tex-math id="M157">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {s}(\mathbf {p})=\bar{\mathbf {s}}\,+\,\mathbf {U}_s\mathbf {p}$$\end{document}</tex-math><mml:math id="M158"><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">&#x000af;</mml:mo></mml:mrow></mml:mover><mml:mspace width="0.166667em"/><mml:mo>+</mml:mo><mml:mspace width="0.166667em"/><mml:msub><mml:mi mathvariant="bold">U</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi mathvariant="bold">p</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq79.gif"/></alternatives></inline-formula> where <inline-formula id="IEq80"><alternatives><tex-math id="M159">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {p}=[p_1,\ldots ,p_{n_s}]^T$$\end{document}</tex-math><mml:math id="M160"><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq80.gif"/></alternatives></inline-formula> is the <inline-formula id="IEq81"><alternatives><tex-math id="M161">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_s\times 1$$\end{document}</tex-math><mml:math id="M162"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq81.gif"/></alternatives></inline-formula> vector of <italic>shape parameters</italic> that control the linear combination of the eigenvectors. The Kalman smoothing is thus learnt via Expectation-Maximisation (EM) for the parameters <inline-formula id="IEq82"><alternatives><tex-math id="M163">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {p}$$\end{document}</tex-math><mml:math id="M164"><mml:mi mathvariant="bold">p</mml:mi></mml:math><inline-graphic xlink:href="11263_2017_999_Article_IEq82.gif"/></alternatives></inline-formula> of each shape within a sequence (Fig.&#x000a0;<xref rid="Fig10" ref-type="fig">10</xref>).</p><p id="Par92">The results of this experiment are given in Table&#x000a0;<xref rid="Tab10" ref-type="table">10</xref> and Fig.&#x000a0;<xref rid="Fig11" ref-type="fig">11</xref>. These experiments also provide a direct comparison between the best detection and model free tracking based techniques. In categories 1 and 2 the Kalman smoothing applied to the model free trackers followed by the discriminative landmark localisation methods of ERT or CFSS score better, with the trackers MDNET and SRDCF being the top performers. In category 3 the DPM and the deep tracker MTCNN achieve the top performance, because they are less prone to drifting (in comparison to trackers) in the most challenging clips of the dataset.</p><p id="Par93">In order to better investigate the effect of the smoothing, we also provide Fig.&#x000a0;<xref rid="Fig12" ref-type="fig">12</xref> which shows the differences between the initial tracking results and the results after applying Kalman smoothing. This comparison is shown for the best methods of Table&#x000a0;<xref rid="Tab10" ref-type="table">10</xref>. It becomes obvious that the improvement introduced by Kalman smoothing is consistent, but marginal.<fig id="Fig11"><label>Fig. 11</label><caption><p>Results for experiment 5 of Sect.&#x000a0;<xref rid="Sec18" ref-type="sec">4.7</xref> (Kalman Smoothing). The top five performing <italic>curves</italic> are highlighted in each legend. Please see Table&#x000a0;<xref rid="Tab10" ref-type="table">10</xref> for a full summary</p></caption><graphic xlink:href="11263_2017_999_Fig11_HTML" id="MO18"/></fig>
<fig id="Fig12"><label>Fig. 12</label><caption><p>Results for experiment 5 of Sect.&#x000a0;<xref rid="Sec18" ref-type="sec">4.7</xref> (Kalman Smoothing). These results show the effect of Kalman smoothing on the final landmark localisation results. The top three performing results are given in <italic>red</italic>, <italic>green</italic> and <italic>blue</italic>, respectively, and the top three most improved are given in <italic>cyan</italic>, <italic>yellow</italic> and <italic>brown</italic>, respectively. The <italic>dashed lines</italic> represent the results before the smoothing is applied, <italic>solid lines</italic> are after (Color figure online)</p></caption><graphic xlink:href="11263_2017_999_Fig12_HTML" id="MO19"/></fig>
<table-wrap id="Tab11"><label>Table 11</label><caption><p>Comparison between the best methods of Sects.&#x000a0;<xref rid="Sec14" ref-type="sec">4.3</xref>&#x02013;<xref rid="Sec18" ref-type="sec">4.7</xref> and the participants of the 300&#x000a0;VW challenge by Shen et&#x000a0;al. (<xref ref-type="bibr" rid="CR170">2015</xref>) (Color table online)</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" colspan="2"><p>
<graphic position="anchor" xlink:href="11263_2017_999_Figh_HTML" id="MO20"/>
</p></td></tr></tbody></table><table-wrap-foot><p>The area under the curve (AUC) and failure rate are reported. The top five performing curves are highlighted for each video category</p></table-wrap-foot></table-wrap>
</p><p id="Par94">
<fig id="Fig13"><label>Fig. 13</label><caption><p>Comparison between the best methods of Sects.&#x000a0;<xref rid="Sec14" ref-type="sec">4.3</xref>&#x02013;<xref rid="Sec18" ref-type="sec">4.7</xref> and the participants of the 300&#x000a0;VW challenge by Shen et&#x000a0;al. (<xref ref-type="bibr" rid="CR170">2015</xref>). The top five methods are shown and are coloured <italic>red, blue, green, orange and purple</italic>, respectively. Please see Table&#x000a0;<xref rid="Tab11" ref-type="table">11</xref> for a full summary (Color figure online)</p></caption><graphic xlink:href="11263_2017_999_Fig13_HTML" id="MO21"/></fig>
</p></sec><sec id="Sec19"><title>300&#x000a0;VW Comparison</title><p id="Par95">In this section we provide results that compare the best performing methods of the previous Sects.&#x000a0;(<xref rid="Sec14" ref-type="sec">4.3</xref>&#x02013;<xref rid="Sec18" ref-type="sec">4.7</xref>) to the participants of the 300&#x000a0;VW challenge by Shen et&#x000a0;al. (<xref ref-type="bibr" rid="CR170">2015</xref>). The challenge had 5 competitors. Rajamanoharan and Cootes (<xref ref-type="bibr" rid="CR151">2015</xref>) employ a multi-view Constrained Local Model (CLM) with a global shape model and different response maps per pose and explore shape-space clustering strategies to determine the optimal pose-specific CLM. Uricar and Franc (<xref ref-type="bibr" rid="CR187">2015</xref>) apply a DPM at each frame as well as Kalman smoothing on the face positions. Wu and Ji (<xref ref-type="bibr" rid="CR198">2015</xref>) utilise a shape augmented regression model, where the regression function is automatically selected based on the facial shape. Xiao et&#x000a0;al. (<xref ref-type="bibr" rid="CR203">2015</xref>) propose a multi-stage regression-based approach that progressively provides initialisations for ambiguous landmarks such as boundary and eyebrows, based on landmarks with semantically strong meaning such as eyes and mouth corners. Finally, Yang et&#x000a0;al. (<xref ref-type="bibr" rid="CR213">2015a</xref>) employ a multi-view spatio-temporal cascade shape regression model along with a novel reinitialisation mechanism.</p><p id="Par96">The results are summarised in Table&#x000a0;<xref rid="Tab11" ref-type="table">11</xref> and Fig.&#x000a0;<xref rid="Fig13" ref-type="fig">13</xref>. Note that the error metric considered in this paper (as described in Sect.&#x000a0;<xref rid="Sec13" ref-type="sec">4.2.2</xref>) differs from that of the original competition. This was intended to improve the robustness of the results with respect to variation in pose. Also, as noted in Sect.&#x000a0;<xref rid="Sec11" ref-type="sec">4.2</xref>, the 300&#x000a0;VW annotations have been corrected and thus this experiment represents updated results for the 300&#x000a0;VW competitors. The results indicate that Yang et&#x000a0;al. (<xref ref-type="bibr" rid="CR213">2015a</xref>) outperforms the rest of the methods for the videos of categories 1 and 2, whereas the deep network of Zhang et&#x000a0;al. (<xref ref-type="bibr" rid="CR223">2016</xref>) combined with CFSS and Kalman smoothing or initialisation from previous are the top performing for the challenging videos of category 3. Moreover, it becomes evident that methodologies which employ face detection dominate category 3, whereas in categories 1 and 2 the model free trackers dominate.</p></sec></sec><sec id="Sec20"><title>Discussion and Conclusions</title><p id="Par97">In Sect.&#x000a0;<xref rid="Sec9" ref-type="sec">4</xref> we presented a number of experiments on deformable tracking of sequences containing a single face. We investigated the performance of state-of-the-art face detectors and model free trackers on the recently released 300&#x000a0;VW dataset (see footnote 1). We also devised a number of hybrid systems that attempt to improve the performance of both detectors and trackers with respect to tracking failures. A summary of the proposed experiments are given in Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>.</p><p id="Par98">Overall, it appears that modern detectors are capable of handling videos of the complexity provided by the 300&#x000a0;VW dataset. This supports the most commonly proposed deformable face tracking methodology that couples a detector with a landmark localisation algorithm. More interestingly, it appears that modern model free trackers are also highly capable of tracking videos that contain variations in pose, expression and illumination. This is particularly evident in the videos of category 2 where the model free trackers perform the best. The performance on the videos of category 2 is likely due to the decreased amount of pose variation in comparison to the other two categories. Category 2 contains many illumination variations which model free trackers appear invariant to. Our work also supports the most recent model free tracking benchmarks (Kristan et&#x000a0;al. <xref ref-type="bibr" rid="CR98">2015</xref> and Wu et&#x000a0;al. <xref ref-type="bibr" rid="CR201">2015</xref>) which have demonstrated that DCF-based trackers are currently the most competitive along with the deep neural network approaches. However, the performance of the trackers does deteriorate significantly in category 3 which supports the categorisation of these videos in the 300&#x000a0;VW as the most difficult category. The difficulty in the videos of category 3 largely stems from the amount of pose variation present, which both detectors and model free trackers struggle with.</p><p id="Par99">The DPM detector provided by Mathias et&#x000a0;al. (<xref ref-type="bibr" rid="CR127">2014</xref>) is very robust across a variety of poses and illumination conditions. The more recent face detector of Zhang et&#x000a0;al. (<xref ref-type="bibr" rid="CR223">2016</xref>) outperforms the rest employed methods in the challenging category 3, however it seems less robust than the DPM detector in the easier categories. The recent advances in the model free trackers, dictate the MDNET tracker of Nam and Han (<xref ref-type="bibr" rid="CR132">2016</xref>) as a top performing method, which outperforms the pre-trained detectors in the first two categories. MDNET belongs to the discriminatively learned Convolutional Neural Networks trackers with their architecture having several shared CNN layers along with a branched last layer during the training. During the inference, the last layer is discarded and a new layer that is updated online is added. This online update capability of the last layer makes the tracker very robust to abrupt changes and a top performing method in all tracking benchmarks. The SRDCF tracker of Danelljan et&#x000a0;al. (<xref ref-type="bibr" rid="CR45">2015</xref>) from the category of trackers with discriminatively learned correlation filters (DCF) consists an alternative top performing method. DCF trackers are currently a very popular method of choice for bounding box based tracking. They capitalise on a periodic assumption of the training samples to efficiently learn a classifier on all patches in the target neighborhood. Nevertheless, the periodic assumption may introduce unwanted boundary effects, which severely degrade the quality of the tracking model. SRDCF incorporates a spatial regularization component in the learning to penalize correlation filter coefficients depending on their spatial location. The CFSS landmark localisation method of Zhu et&#x000a0;al. (<xref ref-type="bibr" rid="CR229">2015</xref>) outperforms all other considered landmark localisation methods, although the random forest based ERT method of Kazemi and Sullivan (<xref ref-type="bibr" rid="CR86">2014</xref>) also performed very well. In contrast to the conventional Cascade Regression approaches that iteratively refine an initial shape in a cascaded manner, CFSS explores a diverse shape space and employs a probabilistic heuristic to constrain the finer search in the subsequent cascade levels. The authors argue that this procedure prevents the final solution from being trapped in a local optimum like similar regression techniques. The experimental results support the claim of the authors of Zhu et&#x000a0;al. (<xref ref-type="bibr" rid="CR229">2015</xref>) as the videos contain very challenging pose variations.</p><p id="Par100">The stable performance of both the best model free trackers and detectors on these videos is further demonstrated by the minimal improvement gained from the proposed hybrid systems. Neither reinitialisation from the previous frame (Sect.&#x000a0;<xref rid="Sec15" ref-type="sec">4.4</xref>), nor the failure detection methodology proposed (Sect.&#x000a0;<xref rid="Sec17" ref-type="sec">4.6</xref>) improved the best performing methods with any significance. Such hybrid systems could be very useful, though, in case of person re-appearance, multiple person cross-overs. Furthermore, smoothing the facial shapes across the sequences (Kalman) also had a very minimal positive improvement, which can be attributed to the human factor, nonetheless the usage of this smoothing could be more useful for reducing the amount of jiterring in consecutive frames.</p><p id="Par101">In comparison to the recent results of the 300&#x000a0;VW competition (Shen et&#x000a0;al. <xref ref-type="bibr" rid="CR170">2015</xref>), our review of combinations of modern state-of-the-art detectors and trackers found that very strong performance can be obtained through fairly simple deformable tracking schemes. In fact, only the work of Yang et&#x000a0;al. (<xref ref-type="bibr" rid="CR213">2015a</xref>) outperforms our best performing methods in the easier categories of 1 and 2, while the difference shown by Fig.&#x000a0;<xref rid="Fig13" ref-type="fig">13</xref> appears to be marginal. However, the overall results show that, particularly for videos that contain significant pose, there are still improvements to be made.</p><p id="Par102">To summarise, there are a number of important issues that must be tackled in order to improve deformable face tracking:<list list-type="order"><list-item><p id="Par103">Pose is still a challenging issue for landmark localisation methods. In fact, the videos of 300&#x000a0;VW do not even exhibit the full range of possible facial pose as they do not contain profile faces. The challenges of considering profile faces have yet to be adequately addressed and have not be verified with respect to current state-of-the-art benchmarks.</p></list-item><list-item><p id="Par104">In this work, we only consider videos that contain a single visible face. However, there are many scenarios in which multiple faces may be present and this represents further challenges to deformable tracking. Detectors for example, are particularly vulnerable to multi-object tracking scenarios as they require extending with the ability to determine whether the object being localised is the same as in the previous frame.</p></list-item><list-item><p id="Par105">It is very common for objects to leave the frame of the camera during a sequence, and then reappear. Few model free trackers are robust to reinitialisation after an object has disappeared and then reappeared. When combined with multiple objects, this scenario becomes particularly challenging as it requires a re-identification step in order to verify whether the object to be tracked is one that was seen before.</p></list-item></list>We believe that deformable face tracking is a very exciting line of research and future advances on the field can have an important impact on several areas of Computer Vision.</p></sec><sec sec-type="supplementary-material"><title>Electronic supplementary material</title><sec id="Sec21"><p>Below is the link to the electronic supplementary material.
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="11263_2017_999_MOESM1_ESM.pdf"><caption><p>Supplementary material 1 (pdf 2664 KB)</p></caption></media></supplementary-material>
</p></sec></sec></body><back><fn-group><fn id="Fn1"><label>1</label><p id="Par7">The results and dataset of the 300&#x000a0;VW Challenge by Shen et&#x000a0;al. (<xref ref-type="bibr" rid="CR170">2015</xref>) can be found at <ext-link ext-link-type="uri" xlink:href="http://ibug.doc.ic.ac.uk/resources/300-VW/">http://ibug.doc.ic.ac.uk/resources/300-VW/</ext-link> This is the first facial landmark tracking challenge on challenging long-term sequences.</p></fn><fn id="Fn2"><label>2</label><p id="Par15">The problem of face tracking using commodity depth cameras, which has received a lot of attention (G&#x000f6;kt&#x000fc;rk and Tomasi <xref ref-type="bibr" rid="CR65">2004</xref>; Cai et&#x000a0;al. <xref ref-type="bibr" rid="CR31">2010</xref>; Weise et&#x000a0;al. <xref ref-type="bibr" rid="CR195">2011</xref>), falls outside the scope of this paper.</p></fn><fn id="Fn3"><label>3</label><p id="Par21">The Dudek sequence has been annotated with regards to certain facial landmarks only to be used for the estimation of an affine transformation.</p></fn><fn id="Fn4"><label>4</label><p id="Par24">In a private communication, the authors of Xiong and De&#x000a0;la Torre (<xref ref-type="bibr" rid="CR206">2015</xref>) informed us that the annotated data, as described in the paper, will not be made publicly available (at least not in the near future).</p></fn><fn id="Fn5"><label>5</label><p id="Par55">In <ext-link ext-link-type="uri" xlink:href="https://youtu.be/Lx5gHvErqX8">https://youtu.be/Lx5gHvErqX8</ext-link> we provide a video with the tracking results of the top methods for face detection followed by landmark localisation (Sect.&#x000a0;<xref rid="Sec14" ref-type="sec">4.3</xref>, Table&#x000a0;<xref rid="Tab6" ref-type="table">6</xref>, Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>) for qualitative comparison.</p></fn><fn id="Fn6"><label>6</label><p id="Par56">In <ext-link ext-link-type="uri" xlink:href="https://youtu.be/SNr39MH3dh8">https://youtu.be/SNr39MH3dh8</ext-link> we provide a video with the tracking results of the top methods for model free tracking followed by landmark localisation (Sect.&#x000a0;<xref rid="Sec16" ref-type="sec">4.5</xref>, Table&#x000a0;<xref rid="Tab8" ref-type="table">8</xref>, Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>) for qualitative comparison.</p></fn><fn><p>E. Antonakos and P. Snape contributed equally and have joint second authorship.</p></fn></fn-group><ack><title>Acknowledgements</title><p>GC was supported by EPSRC DTA award at Imperial College London, as well as from the EPSRC project ADAMANT (EP/L026813/1). The work of PS and EA was funded by the European Community Horizon 2020 [H2020/2014-2020] under Grant Agreement No. 688520 (TeSLA). The work of S. Zafeiriou was funded by the FiDiPro program of Tekes (Project No. 1849/31/2015), as well as from EPSRC Programme Grant FACER2VM (EP/N007743/1).</p></ack><ref-list id="Bib1"><title>References</title><ref id="CR1"><mixed-citation publication-type="other">Adam, A., Rivlin, E., &#x00026; Shimshoni, I. (2006). Robust fragments-based tracking using the integral histogram. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (Vol. 1, pp. 798&#x02013;805). IEEE.</mixed-citation></ref><ref id="CR2"><mixed-citation publication-type="other">Alabort-i-Medina, &#x00026; J., Zafeiriou, S. (2014). Bayesian active appearance models. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 3438&#x02013;3445).</mixed-citation></ref><ref id="CR3"><mixed-citation publication-type="other">Alabort-i-Medina, J., &#x00026; Zafeiriou, S. (2015). Unifying holistic and parts-based deformable model fitting. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 3679&#x02013;3688).</mixed-citation></ref><ref id="CR4"><mixed-citation publication-type="other">Alabort-i-Medina, J., Antonakos, E., Booth, J., Snape, P., &#x00026; Zafeiriou, S. (2014). Menpo: A comprehensive platform for parametric image alignment and visual deformable models. In <italic>Proceedings of ACM international conference on multimedia (ACM&#x02019;MM)</italic> (pp. 679&#x02013;682). ACM (Code <ext-link ext-link-type="uri" xlink:href="http://www.menpo.org/">http://www.menpo.org/</ext-link>, Status: Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR5"><mixed-citation publication-type="other">Allen, J. G., Xu, R. Y., &#x00026; Jin, J. S. (2004). Object tracking using camshift algorithm and multiple quantized feature spaces. In <italic>Proceedings of the Pan-Sydney area workshop on visual information processing</italic> (pp. 3&#x02013;7). Australian Computer Society, Inc.</mixed-citation></ref><ref id="CR6"><mixed-citation publication-type="other">Amberg, B. (2011). <italic>Editing faces in videos</italic>. PhD thesis, University of Basel.</mixed-citation></ref><ref id="CR7"><mixed-citation publication-type="other">Amberg, B., Blake, A., &#x00026; Vetter, T. (2009). On compositional image alignment, with an application to active appearance models. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 1714&#x02013;1721). IEEE.</mixed-citation></ref><ref id="CR8"><mixed-citation publication-type="other">Antonakos, E., Alabort-i-Medina, J., Tzimiropoulos, G., &#x00026; Zafeiriou, S. (2014). Hog active appearance models. <italic>In IEEE proceedings of international conference on image processing (ICIP)</italic> (pp. 224&#x02013;228).</mixed-citation></ref><ref id="CR9"><mixed-citation publication-type="other">Antonakos, E., Alabort-i-Medina, J., &#x00026; Zafeiriou, S. (2015a). Active pictorial structures. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 5435&#x02013;5444).</mixed-citation></ref><ref id="CR10"><mixed-citation publication-type="other">Antonakos, E., Alabort-i-Medina, J., Tzimiropoulos, G., &#x00026; Zafeiriou, S., (2015b). Feature-based lucas-kanade and active appearance models. <italic>IEEE Transactions in Image Processing (TIP)</italic>, <italic>24</italic>(9), 2617&#x02013;2632.</mixed-citation></ref><ref id="CR11"><mixed-citation publication-type="other">Arandjelovi&#x00107;, R., &#x00026; Zisserman, A. (2012). Three things everyone should know to improve object retrieval. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 2911&#x02013;2918). IEEE.</mixed-citation></ref><ref id="CR12"><mixed-citation publication-type="other">Asthana, A., Zafeiriou, S., Cheng, S., &#x00026; Pantic, M. (2014). Incremental face alignment in the wild. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 1859&#x02013;1866).</mixed-citation></ref><ref id="CR13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asthana</surname><given-names>A</given-names></name><name><surname>Zafeiriou</surname><given-names>S</given-names></name><name><surname>Tzimiropoulos</surname><given-names>G</given-names></name><name><surname>Cheng</surname><given-names>S</given-names></name><name><surname>Pantic</surname><given-names>M</given-names></name></person-group><article-title>From pixels to response maps: Discriminative image filtering for face alignment in the wild</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2015</year><volume>37</volume><issue>6</issue><fpage>1312</fpage><lpage>1320</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2014.2362142</pub-id></element-citation></ref><ref id="CR14"><mixed-citation publication-type="other">Babenko, B., Yang, M. H., &#x00026; Belongie, S. (2011). Robust object tracking with online multiple instance learning. <italic>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 33</italic>(8), 1619&#x02013;1632. doi:10.1109/TPAMI.2010.226</mixed-citation></ref><ref id="CR15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>S</given-names></name><name><surname>Matthews</surname><given-names>I</given-names></name></person-group><article-title>Lucas-kanade 20 years on: A unifying framework</article-title><source>International Journal of Computer Vision (IJCV)</source><year>2004</year><volume>56</volume><issue>3</issue><fpage>221</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1023/B:VISI.0000011205.11775.fd</pub-id></element-citation></ref><ref id="CR16"><mixed-citation publication-type="other">Balan, A. O., &#x00026; Black, M. J. (2006). An adaptive appearance model approach for model-based articulated object tracking. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (vol&#x000a0;1, pp. 758&#x02013;765). IEEE.</mixed-citation></ref><ref id="CR17"><mixed-citation publication-type="other">Barbu, A., Lay, N., &#x00026; Gramajo, G. (2014). <italic>Face detection with a 3d model</italic>. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1404.3596">arXiv:1404.3596</ext-link>.</mixed-citation></ref><ref id="CR18"><mixed-citation publication-type="other">Basu, S., Essa, I., &#x00026; Pentland, A. (1996). Motion regularization for model-based head tracking. In <italic>IEEE international conference on pattern recognition (ICPR)</italic> (vol&#x000a0;3, pp. 611&#x02013;616). IEEE.</mixed-citation></ref><ref id="CR19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bay</surname><given-names>H</given-names></name><name><surname>Ess</surname><given-names>A</given-names></name><name><surname>Tuytelaars</surname><given-names>T</given-names></name><name><surname>Van Gool</surname><given-names>L</given-names></name></person-group><article-title>Speeded-up robust features (surf)</article-title><source>Computer Vision and Image Understanding</source><year>2008</year><volume>110</volume><issue>3</issue><fpage>346</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1016/j.cviu.2007.09.014</pub-id></element-citation></ref><ref id="CR20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belhumeur</surname><given-names>PN</given-names></name><name><surname>Jacobs</surname><given-names>DW</given-names></name><name><surname>Kriegman</surname><given-names>DJ</given-names></name><name><surname>Kumar</surname><given-names>N</given-names></name></person-group><article-title>Localizing parts of faces using a consensus of exemplars</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2013</year><volume>35</volume><issue>12</issue><fpage>2930</fpage><lpage>2940</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2013.23</pub-id></element-citation></ref><ref id="CR21"><mixed-citation publication-type="other">Bertinetto, L., Valmadre, J., Golodetz, S., Miksik, O., &#x00026; Torr, P. H. S. (2016a). Staple: Complementary learners for real-time tracking. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic>, IEEE. (Code: <ext-link ext-link-type="uri" xlink:href="https://github.com/bertinetto/staple">https://github.com/bertinetto/staple</ext-link>, Status: Online; accessed August 18 , 2016).</mixed-citation></ref><ref id="CR22"><mixed-citation publication-type="other">Bertinetto, L., Valmadre, J., Henriques, J. F., Vedaldi, A., &#x00026; Torr, P. (2016b). <italic>Fully-convolutional siamese networks for object tracking</italic>. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1606.09549">arXiv:1606.09549</ext-link>.</mixed-citation></ref><ref id="CR23"><mixed-citation publication-type="other">Best-Rowden, L., Klare, B., Klontz, J., &#x00026; Jain, A. K. (2013). Video-to-video face matching: Establishing a baseline for unconstrained face recognition. In <italic>IEEE sixth international conference on biometrics: Theory, applications and systems (BTAS)</italic> (pp. 1&#x02013;8). IEEE.</mixed-citation></ref><ref id="CR24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Black</surname><given-names>MJ</given-names></name><name><surname>Jepson</surname><given-names>AD</given-names></name></person-group><article-title>Eigentracking: Robust matching and tracking of articulated objects using a view-based representation</article-title><source>International Journal of Computer Vision (IJCV)</source><year>1998</year><volume>26</volume><issue>1</issue><fpage>63</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1023/A:1007939232436</pub-id></element-citation></ref><ref id="CR25"><mixed-citation publication-type="other">Black, M. J., &#x00026; Yacoob, Y. (1995). Tracking and recognizing rigid and non-rigid facial motions using local parametric models of image motion. In <italic>IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (pp. 374&#x02013;381).</mixed-citation></ref><ref id="CR26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bozda&#x0011f;i</surname><given-names>G</given-names></name><name><surname>Tekalp</surname><given-names>AM</given-names></name><name><surname>Onural</surname><given-names>L</given-names></name></person-group><article-title>3-d motion estimation and wireframe adaptation including photometric effects for model-based coding of facial image sequences</article-title><source>IEEE Transactions on Circuits and Systems for Video Technology</source><year>1994</year><volume>4</volume><issue>3</issue><fpage>246</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1109/76.305870</pub-id></element-citation></ref><ref id="CR27"><mixed-citation publication-type="other">Bradski, G. (2000). <italic>The opencv library</italic>. Dr Dobb&#x02019;s Journal of Software Tools (Code: <ext-link ext-link-type="uri" xlink:href="http://opencv.org">http://opencv.org</ext-link>, Status: Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR28"><mixed-citation publication-type="other">Bradski, G. R. (1998a). Computer vision face tracking as a component of a perceptual user interface. In <italic>Proceedings IEEE workshop on applications of computer vision</italic>, Princeton, NJ, October 1998 (pp. 214&#x02013;219).</mixed-citation></ref><ref id="CR29"><mixed-citation publication-type="other">Bradski, G. R., &#x00026; (1998b). Real time face and object tracking as a component of a perceptual user interface. In <italic>4th IEEE workshop on applications of computer vision, WACV&#x02019;98</italic> (pp. 214&#x02013;219). IEEE.</mixed-citation></ref><ref id="CR30"><mixed-citation publication-type="other">Burgos-Artizzu, X. P., Perona, P., &#x00026; Doll&#x000e1;r, P. (2013). Robust face landmark estimation under occlusion. In <italic>IEEE proceedings of international conference on computer vision (ICCV)</italic>.</mixed-citation></ref><ref id="CR31"><mixed-citation publication-type="other">Cai, Q., Gallup, D., Zhang, C., &#x00026; Zhang, Z. (2010). 3d deformable face tracking with a commodity depth camera. In <italic>Proceedings of European conference on computer vision (ECCV)</italic> (pp. 229&#x02013;242). Springer.</mixed-citation></ref><ref id="CR32"><mixed-citation publication-type="other">Campbell, K. L. (2016). <italic>Transportation Research Board of the National Academies of Science. The 2nd strategic highway research program naturalistic driving study dataset.</italic><ext-link ext-link-type="uri" xlink:href="https://insight.shrp2nds.us/">https://insight.shrp2nds.us/</ext-link>, (Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>X</given-names></name><name><surname>Wei</surname><given-names>Y</given-names></name><name><surname>Wen</surname><given-names>F</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><article-title>Face alignment by explicit shape regression</article-title><source>International Journal of Computer Vision (IJCV)</source><year>2014</year><volume>107</volume><issue>2</issue><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1007/s11263-013-0667-3</pub-id></element-citation></ref><ref id="CR34"><mixed-citation publication-type="other">Chen, D., Ren, S., Wei, Y., Cao, X., &#x00026; Sun, J.(2014). Joint cascade face detection and alignment. In <italic>Proceedings of European conference on computer vision (ECCV)</italic> (pp. 109&#x02013;122). Springer.</mixed-citation></ref><ref id="CR35"><mixed-citation publication-type="other">Chrysos, G., Antonakos, E., Zafeiriou, S., &#x00026; Snape, P. (2015). Offline deformable face tracking in arbitrary videos. In <italic>IEEE proceedings of international conference on computer vision, 300 videos in the wild (300-VW): Facial landmark tracking in-the-wild challenge &#x00026; workshop (ICCV-W)</italic>.</mixed-citation></ref><ref id="CR36"><mixed-citation publication-type="other">Colmenarez, A., Frey, B., &#x00026; Huang, T. S. (1999). Detection and tracking of faces and facial features. In <italic>IEEE proceedings of international conference on image processing (ICIP)</italic> (vol&#x000a0;1, pp. 657&#x02013;661). IEEE.</mixed-citation></ref><ref id="CR37"><mixed-citation publication-type="other">Comaniciu, D., &#x00026; Meer, P. (1999). Mean shift analysis and applications. In <italic>IEEE proceedings of international conference on computer vision (ICCV)</italic> (vol&#x000a0;2, pp 1197&#x02013;1203). IEEE.</mixed-citation></ref><ref id="CR38"><mixed-citation publication-type="other">Comaniciu, D., Ramesh, V., &#x00026; Meer, P. (2000). Real-time tracking of non-rigid objects using mean shift. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (vol&#x000a0;2, pp 142&#x02013;149). IEEE.</mixed-citation></ref><ref id="CR39"><mixed-citation publication-type="other">Cootes, T. F. (2016). Talking face video. <ext-link ext-link-type="uri" xlink:href="http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/data/talking%5fface/talking%5fface.html">http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/data/talking_face/talking_face.html</ext-link>, (Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cootes</surname><given-names>TF</given-names></name><name><surname>Taylor</surname><given-names>CJ</given-names></name><name><surname>Cooper</surname><given-names>DH</given-names></name><name><surname>Graham</surname><given-names>J</given-names></name></person-group><article-title>Active shape models-their training and application</article-title><source>Computer vision and image understanding</source><year>1995</year><volume>61</volume><issue>1</issue><fpage>38</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1006/cviu.1995.1004</pub-id></element-citation></ref><ref id="CR41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cootes</surname><given-names>TF</given-names></name><name><surname>Edwards</surname><given-names>GJ</given-names></name><name><surname>Taylor</surname><given-names>CJ</given-names></name></person-group><article-title>Active appearance models</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2001</year><volume>23</volume><issue>6</issue><fpage>681</fpage><lpage>685</lpage><pub-id pub-id-type="doi">10.1109/34.927467</pub-id></element-citation></ref><ref id="CR42"><mixed-citation publication-type="other">Crowley, J. L., &#x00026; Berard, F. (1997). Multi-modal tracking of faces for video communications. In <italic>IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (pp. 640&#x02013;645). IEEE.</mixed-citation></ref><ref id="CR43"><mixed-citation publication-type="other">Dalal, N., &#x00026; Triggs, B. (2005). Histograms of oriented gradients for human detection. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 886&#x02013;893).</mixed-citation></ref><ref id="CR44"><mixed-citation publication-type="other">Danelljan, M., H&#x000e4;ger, G., Khan, F. S., &#x00026; Felsberg, M. (2014). Accurate scale estimation for robust visual tracking. In <italic>Proceedings of british machine vision conference (BMVC)</italic>.</mixed-citation></ref><ref id="CR45"><mixed-citation publication-type="other">Danelljan, M., H&#x000e4;ger, G., Shahbaz&#x000a0;Khan, F., &#x00026; Felsberg, M. (2015). Learning spatially regularized correlation filters for visual tracking. In <italic>IEEE Proceedings of International Conference on Computer Vision (ICCV)</italic> (pp. 4310&#x02013;4318). (Code: <ext-link ext-link-type="uri" xlink:href="https://www.cvl.isy.liu.se/en/research/objrec/visualtracking/regvistrack/">https://www.cvl.isy.liu.se/en/research/objrec/visualtracking/regvistrack/</ext-link>, Status: Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR46"><mixed-citation publication-type="other">Danelljan, M., Robinson, A., Khan, F. S., &#x00026; Felsberg, M. (2016). Beyond correlation filters: Learning continuous convolution operators for visual tracking. In <italic>Proceedings of European Conference on Computer Vision (ECCV)</italic> (pp. 472&#x02013;488). (Code: <ext-link ext-link-type="uri" xlink:href="https://github.com/martin-danelljan/Continuous-ConvOp">https://github.com/martin-danelljan/Continuous-ConvOp</ext-link>, Status: Online; accessed December 22, 2016).</mixed-citation></ref><ref id="CR47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Decarlo</surname><given-names>D</given-names></name><name><surname>Metaxas</surname><given-names>D</given-names></name></person-group><article-title>Optical flow constraints on deformable models with applications to face tracking</article-title><source>International Journal of Computer Vision (IJCV)</source><year>2000</year><volume>38</volume><issue>2</issue><fpage>99</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1023/A:1008122917811</pub-id></element-citation></ref><ref id="CR48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dedeo&#x0011f;lu</surname><given-names>G</given-names></name><name><surname>Kanade</surname><given-names>T</given-names></name><name><surname>Baker</surname><given-names>S</given-names></name></person-group><article-title>The asymmetry of image registration and its application to face tracking</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2007</year><volume>29</volume><issue>5</issue><fpage>807</fpage><lpage>823</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2007.1054</pub-id></element-citation></ref><ref id="CR49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De&#x000a0;la Torre</surname><given-names>F</given-names></name></person-group><article-title>A least-squares framework for component analysis</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2012</year><volume>34</volume><issue>6</issue><fpage>1041</fpage><lpage>1055</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2011.184</pub-id></element-citation></ref><ref id="CR50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Del Moral</surname><given-names>P</given-names></name></person-group><article-title>Non-linear filtering: Interacting particle resolution</article-title><source>Markov processes and related fields</source><year>1996</year><volume>2</volume><issue>4</issue><fpage>555</fpage><lpage>581</lpage></element-citation></ref><ref id="CR51"><mixed-citation publication-type="other">Doll&#x000e1;r, P., Tu, Z., Perona, P., &#x00026; Belongie, S. (2009). Integral channel features. In <italic>Proceedings of British machine vision conference (BMVC)</italic>.</mixed-citation></ref><ref id="CR52"><mixed-citation publication-type="other">Doll&#x000e1;r, P., Welinder, P., &#x00026; Perona, P. (2010). Cascaded pose regression. In <italic>IEEE Conference on computer vision and pattern recognition (CVPR)</italic> (pp.&#x000a0;1078&#x02013;1085). IEEE.</mixed-citation></ref><ref id="CR53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dornaika</surname><given-names>F</given-names></name><name><surname>Ahlberg</surname><given-names>J</given-names></name></person-group><article-title>Fast and reliable active appearance model search for 3-d face tracking</article-title><source>IEEE Transactions On Systems, Man, and Cybernetics, Part B: Cybernetics</source><year>2004</year><volume>34</volume><issue>4</issue><fpage>1838</fpage><lpage>1853</lpage><pub-id pub-id-type="doi">10.1109/TSMCB.2004.829135</pub-id></element-citation></ref><ref id="CR54"><mixed-citation publication-type="other">Dubout, C., &#x00026; Fleuret, F. (2012). Exact acceleration of linear object detectors. In <italic>Proceedings of european conference on computer vision (ECCV)</italic> (pp. 301&#x02013;311) Springer.</mixed-citation></ref><ref id="CR55"><mixed-citation publication-type="other">Dubout, C., &#x00026; Fleuret, F. (2013). Deformable part models with individual part scaling. In <italic>Proceedings of British machine vision conference (BMVC), EPFL-CONF-192393</italic>.</mixed-citation></ref><ref id="CR56"><mixed-citation publication-type="other">Essa, I., Basu, S., Darrell, T., &#x00026; Pentland, A. (1996). Modeling, tracking and interactive animation of faces and heads using input from video. In <italic>Proceedings of computer animation</italic> (pp. 68&#x02013;79).</mixed-citation></ref><ref id="CR57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Essa</surname><given-names>I</given-names></name><name><surname>Pentland</surname><given-names>AP</given-names></name><etal/></person-group><article-title>Coding, analysis, interpretation, and recognition of facial expressions</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>1997</year><volume>19</volume><issue>7</issue><fpage>757</fpage><lpage>763</lpage><pub-id pub-id-type="doi">10.1109/34.598232</pub-id></element-citation></ref><ref id="CR58"><mixed-citation publication-type="other">Essa, I. A., &#x00026; Pentland, A. (1994). A vision system for observing and extracting facial action parameters. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 76&#x02013;83). IEEE.</mixed-citation></ref><ref id="CR59"><mixed-citation publication-type="other">Essa, I. A., Darrell, T., &#x00026; Pentland, A. (1994). Tracking facial motion. In <italic>IEEE proceedings of workshop on motion of non-rigid and articulated objects</italic> (pp. 36&#x02013;42). IEEE.</mixed-citation></ref><ref id="CR60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felzenszwalb</surname><given-names>PF</given-names></name><name><surname>Huttenlocher</surname><given-names>DP</given-names></name></person-group><article-title>Pictorial structures for object recognition</article-title><source>International Journal of Computer Vision (IJCV)</source><year>2005</year><volume>61</volume><issue>1</issue><fpage>55</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1023/B:VISI.0000042934.15159.49</pub-id></element-citation></ref><ref id="CR61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felzenszwalb</surname><given-names>PF</given-names></name><name><surname>Girshick</surname><given-names>RB</given-names></name><name><surname>McAllester</surname><given-names>D</given-names></name><name><surname>Ramanan</surname><given-names>D</given-names></name></person-group><article-title>Object detection with discriminatively trained part-based models</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2010</year><volume>32</volume><issue>9</issue><fpage>1627</fpage><lpage>1645</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2009.167</pub-id></element-citation></ref><ref id="CR62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischler</surname><given-names>MA</given-names></name><name><surname>Elschlager</surname><given-names>RA</given-names></name></person-group><article-title>The representation and matching of pictorial structures</article-title><source>IEEE Transactions on Computers</source><year>1973</year><volume>22</volume><issue>1</issue><fpage>67</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1109/T-C.1973.223602</pub-id></element-citation></ref><ref id="CR63"><mixed-citation publication-type="other">Gao, J., Ling, H., Hu, W., &#x00026; Xing, J. (2014). Transfer learning based visual tracking with gaussian processes regression. In <italic>Proceedings of European Conference on Computer Vision (ECCV)</italic> (pp. 188&#x02013;203). Springer. (Code: <ext-link ext-link-type="uri" xlink:href="http://www.dabi.temple.edu/%7ehbling/code/TGPR.htm">http://www.dabi.temple.edu/~hbling/code/TGPR.htm</ext-link>, Status: Online; accessed December 4, 2016).</mixed-citation></ref><ref id="CR64"><mixed-citation publication-type="other">Ghiasi, G., &#x00026; Fowlkes, C. (2014). Occlusion coherence: Localizing occluded faces with a hierarchical deformable part model. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 2385&#x02013;2392).</mixed-citation></ref><ref id="CR65"><mixed-citation publication-type="other">G&#x000f6;kt&#x000fc;rk, S. B., &#x00026; Tomasi, C. (2004). 3d head tracking based on recognition and interpolation using a time-of-flight depth sensor. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (vol&#x000a0;2, pp 2&#x02013;211). IEEE.</mixed-citation></ref><ref id="CR66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>NJ</given-names></name><name><surname>Salmond</surname><given-names>DJ</given-names></name><name><surname>Smith</surname><given-names>AF</given-names></name></person-group><article-title>Novel approach to nonlinear/non-gaussian bayesian state estimation</article-title><source>Radar and Signal Processing, IEE Proceedings F, IET</source><year>1993</year><volume>140</volume><fpage>107</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1049/ip-f-2.1993.0015</pub-id></element-citation></ref><ref id="CR67"><mixed-citation publication-type="other">Grabner, H., Grabner, M., &#x00026; Bischof, H. (2006). Real-time tracking via on-line boosting. In <italic>Proceedings of British machine vision conference (BMVC)</italic> (vol&#x000a0;5, p. 6).</mixed-citation></ref><ref id="CR68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname><given-names>R</given-names></name><name><surname>Matthews</surname><given-names>I</given-names></name><name><surname>Cohn</surname><given-names>J</given-names></name><name><surname>Kanade</surname><given-names>T</given-names></name><name><surname>Baker</surname><given-names>S</given-names></name></person-group><article-title>Multi-pie</article-title><source>Image and Vision Computing</source><year>2010</year><volume>28</volume><issue>5</issue><fpage>807</fpage><lpage>813</lpage><pub-id pub-id-type="doi">10.1016/j.imavis.2009.08.002</pub-id></element-citation></ref><ref id="CR69"><mixed-citation publication-type="other">Hare, S., Saffari, A., &#x00026; Torr, P. H. (2011). Struck: Structured output tracking with kernels. In <italic>IEEE proceedings of international conference on computer vision (ICCV)</italic> (pp 263&#x02013;270). IEEE. (Code: <ext-link ext-link-type="uri" xlink:href="http://www.samhare.net/research/struck">http://www.samhare.net/research/struck</ext-link>, Status: Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR70"><mixed-citation publication-type="other">Hare, S., Saffari, A., &#x00026; Torr, P. H. (2012). Efficient online structured output learning for keypoint-based object tracking. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 1894&#x02013;1901). IEEE.</mixed-citation></ref><ref id="CR71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heisele</surname><given-names>B</given-names></name><name><surname>Serre</surname><given-names>T</given-names></name><name><surname>Prentice</surname><given-names>S</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><article-title>Hierarchical classification and feature reduction for fast face detection with support vector machines</article-title><source>Pattern Recognition</source><year>2003</year><volume>36</volume><issue>9</issue><fpage>2007</fpage><lpage>2017</lpage><pub-id pub-id-type="doi">10.1016/S0031-3203(03)00062-1</pub-id></element-citation></ref><ref id="CR72"><mixed-citation publication-type="other">Henriques, J. F., Caseiro, R., Martins, P., &#x00026; Batista, J. (2015). High-speed tracking with kernelized correlation filters. <italic>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI) 37,</italic>(3): 583&#x02013;596, (Code: <ext-link ext-link-type="uri" xlink:href="https://github.com/joaofaro/KCFcpp">https://github.com/joaofaro/KCFcpp</ext-link>, Status: Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hjelm&#x000e5;s</surname><given-names>E</given-names></name><name><surname>Low</surname><given-names>BK</given-names></name></person-group><article-title>Face detection: A survey</article-title><source>Computer Vision and Image Understanding</source><year>2001</year><volume>83</volume><issue>3</issue><fpage>236</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1006/cviu.2001.0921</pub-id></element-citation></ref><ref id="CR74"><mixed-citation publication-type="other">Hu, P., &#x00026; Ramanan, D. (2016). <italic>Finding tiny faces</italic>. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1612.04402">arXiv:1612.04402</ext-link> (Code: <ext-link ext-link-type="uri" xlink:href="https://www.cs.cmu.edu/%7epeiyunh/tiny/">https://www.cs.cmu.edu/~peiyunh/tiny/</ext-link>, Status: Online; accessed December 24, 2016).</mixed-citation></ref><ref id="CR75"><mixed-citation publication-type="other">Huang, G. B., Ramesh, M., Berg, T., &#x00026; Learned-Miller, E. (2007). <italic>Labeled faces in the wild: A database for studying face recognition in unconstrained environments</italic>. Technical Report 07-49, University of Massachusetts, Amherst.</mixed-citation></ref><ref id="CR76"><mixed-citation publication-type="other">Isard, M., &#x00026; Blake, A. (1996). Contour tracking by stochastic propagation of conditional density. In <italic>Proceedings of European conference on computer vision (ECCV)</italic> (pp. 343&#x02013;356). (Code: <ext-link ext-link-type="uri" xlink:href="https://github.com/gnebehay/SIR-PF">https://github.com/gnebehay/SIR-PF</ext-link>, Status: Online; accessed December 23, 2016).</mixed-citation></ref><ref id="CR77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isard</surname><given-names>M</given-names></name><name><surname>Blake</surname><given-names>A</given-names></name></person-group><article-title>Condensationconditional density propagation for visual tracking</article-title><source>International Journal of Computer Vision (IJCV)</source><year>1998</year><volume>29</volume><issue>1</issue><fpage>5</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1023/A:1008078328650</pub-id></element-citation></ref><ref id="CR78"><mixed-citation publication-type="other">Jain, V., &#x00026; Learned-Miller, E. (2010). <italic>Fddb: A benchmark for face detection in unconstrained settings</italic>. Technical Report UM-CS-2010-009, University of Massachusetts, Amherst.</mixed-citation></ref><ref id="CR79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jepson</surname><given-names>AD</given-names></name><name><surname>Fleet</surname><given-names>DJ</given-names></name><name><surname>El-Maraghi</surname><given-names>TF</given-names></name></person-group><article-title>Robust online appearance models for visual tracking</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2003</year><volume>25</volume><issue>10</issue><fpage>1296</fpage><lpage>1311</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2003.1233903</pub-id></element-citation></ref><ref id="CR80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>B</given-names></name><name><surname>Choi</surname><given-names>I</given-names></name><name><surname>Kim</surname><given-names>D</given-names></name></person-group><article-title>Local transform features and hybridization for accurate face and human detection</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2013</year><volume>35</volume><issue>6</issue><fpage>1423</fpage><lpage>1436</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2012.219</pub-id></element-citation></ref><ref id="CR81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jurie</surname><given-names>F</given-names></name></person-group><article-title>A new log-polar mapping for space variant imaging.: Application to face detection and tracking</article-title><source>Pattern Recognition</source><year>1999</year><volume>32</volume><issue>5</issue><fpage>865</fpage><lpage>875</lpage><pub-id pub-id-type="doi">10.1016/S0031-3203(98)00096-X</pub-id></element-citation></ref><ref id="CR82"><mixed-citation publication-type="other">Kalal, Z., Mikolajczyk, K., &#x00026; Matas, J. (2010a) Face-tld: Tracking-learning-detection applied to faces. In <italic>IEEE proceedings of international conference on image processing (ICIP)</italic> (pp 3789&#x02013;3792).</mixed-citation></ref><ref id="CR83"><mixed-citation publication-type="other">Kalal, Z., Mikolajczyk, K., &#x00026; Matas, J. (2010b). Forward-backward error: Automatic detection of tracking failures. In <italic>IEEE international conference on pattern recognition (ICPR)</italic> (pp 2756&#x02013;2759). IEEE.</mixed-citation></ref><ref id="CR84"><mixed-citation publication-type="other">Kalal, Z., Mikolajczyk, K., &#x00026; Matas, J. (2012). Tracking-learning-detection. <italic>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 34</italic>(7):1409&#x02013;1422, (Code: <ext-link ext-link-type="uri" xlink:href="https://github.com/zk00006/OpenTLD">https://github.com/zk00006/OpenTLD</ext-link>, Status: Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalman</surname><given-names>RE</given-names></name></person-group><article-title>A new approach to linear filtering and prediction problems</article-title><source>Journal of Basic Engineering</source><year>1960</year><volume>82</volume><issue>1</issue><fpage>35</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1115/1.3662552</pub-id></element-citation></ref><ref id="CR86"><mixed-citation publication-type="other">Kazemi, V., &#x00026; Sullivan, J. (2014). One millisecond face alignment with an ensemble of regression trees. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 1867&#x02013;1874).</mixed-citation></ref><ref id="CR87"><mixed-citation publication-type="other">Kim, M., Kumar, S., Pavlovic, V., &#x00026; Rowley, H. (2008). Face tracking and recognition with visual constraints in real-world videos. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 1&#x02013;8) IEEE.</mixed-citation></ref><ref id="CR88"><mixed-citation publication-type="other">King, D. E. (2009). Dlib-ml: A machine learning toolkit. <italic>The Journal of Machine Learning Research, 10</italic>:1755&#x02013;1758, (Code: <ext-link ext-link-type="uri" xlink:href="http://dlib.net/">http://dlib.net/</ext-link>, Status: Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR89"><mixed-citation publication-type="other">King, D. E. (2015). Max-margin object detection. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1502.00046">arXiv:1502.00046</ext-link>.</mixed-citation></ref><ref id="CR90"><mixed-citation publication-type="other">Klare, B. F., Klein, B., Taborsky, E., Blanton, A., Cheney, J., Allen, K., Grother, P., Mah, A., Burge, M., &#x00026; Jain, A. K. (2015). Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp 1931&#x02013;1939) IEEE.</mixed-citation></ref><ref id="CR91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelstra</surname><given-names>S</given-names></name><name><surname>Pantic</surname><given-names>M</given-names></name><name><surname>Patras</surname><given-names>IY</given-names></name></person-group><article-title>A dynamic texture-based approach to recognition of facial actions and their temporal models</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2010</year><volume>32</volume><issue>11</issue><fpage>1940</fpage><lpage>1954</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2010.50</pub-id></element-citation></ref><ref id="CR92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kokiopoulou</surname><given-names>E</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Saad</surname><given-names>Y</given-names></name></person-group><article-title>Trace optimization and eigenproblems in dimension reduction methods</article-title><source>Numerical Linear Algebra with Applications</source><year>2011</year><volume>18</volume><issue>3</issue><fpage>565</fpage><lpage>602</lpage><pub-id pub-id-type="doi">10.1002/nla.743</pub-id></element-citation></ref><ref id="CR93"><mixed-citation publication-type="other">K&#x000f6;stinger, M., Wohlhart, P., Roth, P. M., &#x00026; Bischof, H. (2011). Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization. In <italic>IEEE proceedings of international conference on computer vision workshops (ICCV&#x02019;W)</italic> (pp. 2144&#x02013;2151).</mixed-citation></ref><ref id="CR94"><mixed-citation publication-type="other">K&#x000f6;stinger, M., Wohlhart, P., Roth, P. M., &#x00026; Bischof, H. (2012). Robust face detection by simple means. In <italic>DAGM 2012 CVAW workshop</italic>.</mixed-citation></ref><ref id="CR95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koukis</surname><given-names>V</given-names></name><name><surname>Venetsanopoulos</surname><given-names>C</given-names></name><name><surname>Koziris</surname><given-names>N</given-names></name></person-group><article-title>Okeanos: Building a cloud, cluster by cluster</article-title><source>IEEE Internet Computing</source><year>2013</year><volume>17</volume><issue>3</issue><fpage>67</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1109/MIC.2013.43</pub-id></element-citation></ref><ref id="CR96"><mixed-citation publication-type="other">Kristan, M., Pflugfelder, R., Leonardis, A., Matas, J., Porikli, F., &#x0010c;ehovin, L., Nebehay, G., et al. (2013). The visual object tracking vot2013 challenge results. In <italic>IEEE Proceedings of international conference on computer vision workshops (ICCV&#x02019;W)</italic>.</mixed-citation></ref><ref id="CR97"><mixed-citation publication-type="other">Kristan, M., Pflugfelder, R., Leonardis, A., Matas, J., &#x0010c;ehovin, L., Nebehay, G., et al. (2014). The visual object tracking vot2014 challenge results. In <italic>Proceedings of European conference on computer vision workshops (ECCV&#x02019;W)</italic>, <ext-link ext-link-type="uri" xlink:href="http://www.votchallenge.net/vot2014/program.html">http://www.votchallenge.net/vot2014/program.html</ext-link>.</mixed-citation></ref><ref id="CR98"><mixed-citation publication-type="other">Kristan, M., Matas, J., Leonardis, A., Felsberg, M., &#x0010c;ehovin, L., Fernandez, G., et al. (2015). The visual object tracking vot2015 challenge results. In <italic>IEEE proceedings of international conference on computer vision workshops (ICCV&#x02019;W)</italic>.</mixed-citation></ref><ref id="CR99"><mixed-citation publication-type="other">Kristan, M., Matas, J., Leonardis, A., Voj&#x000ed;&#x00159;, T., Pflugfelder, R., Fernandez, G., et al. (2016). A novel performance evaluation methodology for single-target trackers. <italic>IEEE Transactions on Pattern Analysis and Machine Intelligence, 38</italic>(11), 2137&#x02013;2155.</mixed-citation></ref><ref id="CR100"><mixed-citation publication-type="other">Kumar, V., Namboodiri, A., &#x00026; Jawahar, C. (2015). Visual phrases for exemplar face detection. In <italic>IEEE proceedings of international conference on computer vision (ICCV)</italic> (pp. 1994&#x02013;2002). (Code: <ext-link ext-link-type="uri" xlink:href="http://cvit.iiit.ac.in/projects/exemplar/">http://cvit.iiit.ac.in/projects/exemplar/</ext-link>, Status: Online; accessed December 24, 2016).</mixed-citation></ref><ref id="CR101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>La&#x000a0;Cascia</surname><given-names>M</given-names></name><name><surname>Sclaroff</surname><given-names>S</given-names></name><name><surname>Athitsos</surname><given-names>V</given-names></name></person-group><article-title>Fast, reliable head tracking under varying illumination: An approach based on registration of texture-mapped 3d models</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2000</year><volume>22</volume><issue>4</issue><fpage>322</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1109/34.845375</pub-id></element-citation></ref><ref id="CR102"><mixed-citation publication-type="other">Lanitis, A., Taylor, C. J., &#x00026; Cootes, T. F. (1995). A unified approach to coding and interpreting face images. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 368&#x02013;373).</mixed-citation></ref><ref id="CR103"><mixed-citation publication-type="other">Le, V., Brandt, J., Lin, Z., Bourdev, L., &#x00026; Huang, T. S. (2012). Interactive facial feature localization. In <italic>Proceedings of European conference on computer vision (ECCV)</italic> (pp. 679&#x02013;692). Springer.</mixed-citation></ref><ref id="CR104"><mixed-citation publication-type="other">Learned-Miller, E., Huang, G. B., RoyChowdhury, A., Li, H., &#x00026; Hua, G. (2016). Labeled faces in the wild: A survey. In <italic>Advances in face detection and facial image analysis</italic> (pp. 189&#x02013;248). Springer International Publishing.</mixed-citation></ref><ref id="CR105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levey</surname><given-names>A</given-names></name><name><surname>Lindenbaum</surname><given-names>M</given-names></name></person-group><article-title>Sequential karhunen-loeve basis extraction and its application to images</article-title><source>IEEE Transactions on Image Processing</source><year>2000</year><volume>9</volume><issue>8</issue><fpage>1371</fpage><lpage>1374</lpage><pub-id pub-id-type="doi">10.1109/83.855432</pub-id><pub-id pub-id-type="pmid">18262974</pub-id></element-citation></ref><ref id="CR106"><mixed-citation publication-type="other">Li, A., Lin, M., Wu, Y., Yang, M. H., &#x00026; Yan, S. (2016a). Nus-pro: A new visual tracking challenge. <italic>IEEE Transactions on Pattern Analysis and Machine Intelligence, 38</italic>(2), 335&#x02013;349.</mixed-citation></ref><ref id="CR107"><mixed-citation publication-type="other">Li, A., Lin, M., Wu, Y., Yang, M. H., &#x00026; Yan, S. (2016b). <italic>Nus-pro tracking challenge</italic>. <ext-link ext-link-type="uri" xlink:href="http://www.lv-nus.org/pro/nus%5fpro.html">http://www.lv-nus.org/pro/nus_pro.html</ext-link>, (Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Roivainen</surname><given-names>P</given-names></name><name><surname>Forchheimer</surname><given-names>R</given-names></name></person-group><article-title>3-d motion estimation in model-based facial image coding</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>1993</year><volume>15</volume><issue>6</issue><fpage>545</fpage><lpage>555</lpage><pub-id pub-id-type="doi">10.1109/34.216724</pub-id></element-citation></ref><ref id="CR109"><mixed-citation publication-type="other">Li, H., Hua, G., Lin, Z., Brandt, J., &#x00026; Yang, J. (2013a). Probabilistic elastic part model for unsupervised face detector adaptation. In <italic>IEEE proceedings of international conference on computer vision (ICCV)</italic> (pp. 793&#x02013;800).</mixed-citation></ref><ref id="CR110"><mixed-citation publication-type="other">Li, H., Lin, Z., Brandt, J., Shen, X., &#x00026; Hua, G. (2014). Efficient boosted exemplar-based face detection. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 1843&#x02013;1850).</mixed-citation></ref><ref id="CR111"><mixed-citation publication-type="other">Li, H., Lin, Z., Shen, X., Brandt, J., &#x00026; Hua, G. (2015a). A convolutional neural network cascade for face detection. In <italic>IEEE Proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 5325&#x02013;5334).</mixed-citation></ref><ref id="CR112"><mixed-citation publication-type="other">Li, J., &#x00026; Zhang, Y. (2013). Learning surf cascade for fast and accurate object detection. In <italic>Proceedings of the IEEE conference on computer vision and pattern recognition</italic> (pp. 3468&#x02013;3475).</mixed-citation></ref><ref id="CR113"><mixed-citation publication-type="other">Li, J., Wang, T., &#x00026; Zhang, Y. (2011). Face detection using surf cascade. In <italic>IEEE proceedings of international conference on computer vision workshops (ICCV&#x02019;W)</italic> (pp. 2183&#x02013;2190). IEEE.</mixed-citation></ref><ref id="CR114"><mixed-citation publication-type="other">Li, S. Z., Zhu, L., Zhang, Z., Blake, A., Zhang, H., &#x00026; Shum, H. (2002). Statistical learning of multi-view face detection. In <italic>Proceedings of European conference on computer vision (ECCV)</italic> (pp. 67&#x02013;81). Springer.</mixed-citation></ref><ref id="CR115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Hu</surname><given-names>W</given-names></name><name><surname>Shen</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Dick</surname><given-names>A</given-names></name><name><surname>Hengel</surname><given-names>AVD</given-names></name></person-group><article-title>A survey of appearance models in visual object tracking</article-title><source>ACM Transactions on Intelligent Systems and Technology (TIST)</source><year>2013</year><volume>4</volume><issue>4</issue><fpage>58</fpage></element-citation></ref><ref id="CR116"><mixed-citation publication-type="other">Li, Y., Gong, S., &#x00026; Liddell, H. (2000). Support vector regression and classification based multi-view face detection and recognition. In <italic>IEEE proceedings of international conference on automatic face and gesture recognition (FG)</italic> (pp. 300&#x02013;305) IEEE.</mixed-citation></ref><ref id="CR117"><mixed-citation publication-type="other">Li, Y., Zhu, J., &#x00026; Hoi, S. C. (2015b). Reliable patch trackers: Robust visual tracking by exploiting reliable patches. In <italic>IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (pp 353&#x02013;361). (Code: <ext-link ext-link-type="uri" xlink:href="https://github.com/ihpdep/rpt">https://github.com/ihpdep/rpt</ext-link>, Status: Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR118"><mixed-citation publication-type="other">Liao, S., Jain, A. K., &#x00026; Li, S. Z. (2016). A fast and accurate unconstrained face detector. <italic>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 38</italic>(2):211&#x02013;223, (Code: <ext-link ext-link-type="uri" xlink:href="http://www.cbsr.ia.ac.cn/users/scliao/projects/npdface/">http://www.cbsr.ia.ac.cn/users/scliao/projects/npdface/</ext-link>, Status: Online; accessed December 24, 2016).</mixed-citation></ref><ref id="CR119"><mixed-citation publication-type="other">Liwicki, S., Zafeiriou, S., &#x00026; Pantic, M. (2012a). Incremental slow feature analysis with indefinite kernel for online temporal video segmentation. In <italic>Asian conference on computer vision (ACCV)</italic> (pp 162&#x02013;176). Springer.</mixed-citation></ref><ref id="CR120"><mixed-citation publication-type="other">Liwicki, S., Zafeiriou, S., Tzimiropoulos, G., &#x00026; Pantic, M. (2012b). Efficient online subspace learning with an indefinite kernel for visual tracking and recognition. <italic>IEEE Transactions on Neural Networks and Learning Systems (T-NN), 23</italic>(10):1624&#x02013;1636.</mixed-citation></ref><ref id="CR121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liwicki</surname><given-names>S</given-names></name><name><surname>Tzimiropoulos</surname><given-names>G</given-names></name><name><surname>Zafeiriou</surname><given-names>S</given-names></name><name><surname>Pantic</surname><given-names>M</given-names></name></person-group><article-title>Euler principal component analysis</article-title><source>International Journal of Computer Vision (IJCV)</source><year>2013</year><volume>101</volume><issue>3</issue><fpage>498</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1007/s11263-012-0558-z</pub-id></element-citation></ref><ref id="CR122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liwicki</surname><given-names>S</given-names></name><name><surname>Zafeiriou</surname><given-names>SP</given-names></name><name><surname>Pantic</surname><given-names>M</given-names></name></person-group><article-title>Online kernel slow feature analysis for temporal video segmentation and tracking</article-title><source>IEEE Transactions in Image Processing (TIP)</source><year>2015</year><volume>24</volume><issue>10</issue><fpage>2955</fpage><lpage>2970</lpage><pub-id pub-id-type="doi">10.1109/TIP.2015.2428052</pub-id></element-citation></ref><ref id="CR123"><mixed-citation publication-type="other">Liwicki, S., Zafeiriou, S., Tzimiropoulos, G., &#x00026; Pantic, M. (2016). <italic>Annotated face videos</italic>. <ext-link ext-link-type="uri" xlink:href="http://www.robots.ox.ac.uk/%7estephan/dikt/">http://www.robots.ox.ac.uk/~stephan/dikt/</ext-link>, (Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR124"><mixed-citation publication-type="other">Lowe, D. G. (1999). Object recognition from local scale-invariant features. In <italic>IEEE proceedings of international conference on computer vision (ICCV)</italic> (pp. 1150&#x02013;1157).</mixed-citation></ref><ref id="CR125"><mixed-citation publication-type="other">Ma, C., Yang, X., Zhang, C., &#x00026; Yang, M. H. (2015). Long-term correlation tracking. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 5388&#x02013;5396). IEEE (Code: <ext-link ext-link-type="uri" xlink:href="https://github.com/chaoma99/lct-tracker">https://github.com/chaoma99/lct-tracker</ext-link>, Status: Online; accessed August 18, 2016).</mixed-citation></ref><ref id="CR126"><mixed-citation publication-type="other">Malciu, M., &#x00026; Pr&#x0011b;teux, F. (2000). A robust model-based approach for 3d head tracking in video sequences. In <italic>IEEE proceedings of international conference on automatic face and gesture recognition (FG)</italic> (pp 169&#x02013;174). IEEE.</mixed-citation></ref><ref id="CR127"><mixed-citation publication-type="other">Mathias, M., Benenson, R., Pedersoli, M., &#x00026; Van&#x000a0;Gool, L. (2014). Face detection without bells and whistles. In <italic>Proceedings of European conference on computer vision (ECCV)</italic> (pp 720&#x02013;735) Springer.</mixed-citation></ref><ref id="CR128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matthews</surname><given-names>I</given-names></name><name><surname>Baker</surname><given-names>S</given-names></name></person-group><article-title>Active appearance models revisited</article-title><source>International Journal of Computer Vision (IJCV)</source><year>2004</year><volume>60</volume><issue>2</issue><fpage>135</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1023/B:VISI.0000029666.37597.d3</pub-id></element-citation></ref><ref id="CR129"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matthews</surname><given-names>I</given-names></name><name><surname>Ishikawa</surname><given-names>T</given-names></name><name><surname>Baker</surname><given-names>S</given-names></name></person-group><article-title>The template update problem</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2004</year><volume>26</volume><issue>6</issue><fpage>810</fpage><lpage>815</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2004.16</pub-id></element-citation></ref><ref id="CR130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mei</surname><given-names>X</given-names></name><name><surname>Ling</surname><given-names>H</given-names></name></person-group><article-title>Robust visual tracking and vehicle classification via sparse representation</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2011</year><volume>33</volume><issue>11</issue><fpage>2259</fpage><lpage>2272</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2011.66</pub-id></element-citation></ref><ref id="CR131"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mita</surname><given-names>T</given-names></name><name><surname>Kaneko</surname><given-names>T</given-names></name><name><surname>Hori</surname><given-names>O</given-names></name></person-group><article-title>Joint haar-like features for face detection</article-title><source>IEEE Proceedings of International Conference on Computer Vision (ICCV)</source><year>2005</year><volume>2</volume><fpage>1619</fpage><lpage>1626</lpage></element-citation></ref><ref id="CR132"><mixed-citation publication-type="other">Nam, H., &#x00026; Han, B. (2016). Learning multi-domain convolutional neural networks for visual tracking. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic>, IEEE, (Code: <ext-link ext-link-type="uri" xlink:href="https://github.com/HyeonseobNam/MDNet">https://github.com/HyeonseobNam/MDNet</ext-link>, Status: Online; accessed August 18, 2016).</mixed-citation></ref><ref id="CR133"><mixed-citation publication-type="other">Nebehay, G., &#x00026; Pflugfelder, R. (2015). Clustering of static-adaptive correspondences for deformable object tracking. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic>, IEEE (Code: <ext-link ext-link-type="uri" xlink:href="https://github.com/gnebehay/CppMT">https://github.com/gnebehay/CppMT</ext-link>, Status: Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR134"><mixed-citation publication-type="other">Ning, J., Yang, J., Jiang, S., Zhang, L., &#x00026; Yang, M. H. (2016). Object tracking via dual linear structured svm and explicit feature map. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic>, (Code: <ext-link ext-link-type="uri" xlink:href="http://www4.comp.polyu.edu.hk/%7ecslzhang/code/DLSSVM%5fCVPR.zip">www4.comp.polyu.edu.hk/~cslzhang/code/DLSSVM_CVPR.zip</ext-link>, Status: Online; accessed August 18, 2016)</mixed-citation></ref><ref id="CR135"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ojala</surname><given-names>T</given-names></name><name><surname>Pietik&#x000e4;inen</surname><given-names>M</given-names></name><name><surname>M&#x000e4;enp&#x000e4;&#x000e4;</surname><given-names>T</given-names></name></person-group><article-title>Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2002</year><volume>24</volume><issue>7</issue><fpage>971</fpage><lpage>987</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2002.1017623</pub-id></element-citation></ref><ref id="CR136"><mixed-citation publication-type="other">Oliver, N., Pentland, A. P., &#x00026; Berard, F. (1997). Lafter: Lips and face real time tracker. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 123&#x02013;129)</mixed-citation></ref><ref id="CR137"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orozco</surname><given-names>J</given-names></name><name><surname>Rudovic</surname><given-names>O</given-names></name><name><surname>Gonz&#x000e0;lez</surname><given-names>J</given-names></name><name><surname>Pantic</surname><given-names>M</given-names></name></person-group><article-title>Hierarchical on-line appearance-based tracking for 3d head pose, eyebrows, lips, eyelids and irises</article-title><source>Image and Vision Computing</source><year>2013</year><volume>31</volume><issue>4</issue><fpage>322</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1016/j.imavis.2013.02.001</pub-id></element-citation></ref><ref id="CR138"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Osadchy</surname><given-names>M</given-names></name><name><surname>Cun</surname><given-names>YL</given-names></name><name><surname>Miller</surname><given-names>ML</given-names></name></person-group><article-title>Synergistic face detection and pose estimation with energy-based models</article-title><source>Journal of Machine Learning Research</source><year>2007</year><volume>8</volume><fpage>1197</fpage><lpage>1215</lpage></element-citation></ref><ref id="CR139"><mixed-citation publication-type="other">Papandreou, G., &#x00026; Maragos, P. (2008). Adaptive and constrained algorithms for inverse compositional active appearance model fitting. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic>, IEEE, (pp. 1&#x02013;8).</mixed-citation></ref><ref id="CR140"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parkhi</surname><given-names>OM</given-names></name><name><surname>Vedaldi</surname><given-names>A</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><article-title>Deep face recognition</article-title><source>Proceedings of the British Machine Vision</source><year>2015</year><volume>1</volume><issue>3</issue><fpage>6</fpage></element-citation></ref><ref id="CR141"><mixed-citation publication-type="other">Patras, I., &#x00026; Pantic, M. (2004). Particle filtering with factorized likelihoods for tracking facial features. In <italic>IEEE proceedings of international conference on automatic face and gesture recognition (FG)</italic> (pp. 97&#x02013;102).</mixed-citation></ref><ref id="CR142"><mixed-citation publication-type="other">Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., &#x00026; Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. <italic>Journal of Machine Learning Research, 12</italic>, 2825&#x02013;2830, (Code: <ext-link ext-link-type="uri" xlink:href="http://scikit-learn.org/">http://scikit-learn.org/</ext-link>, Status: Online; accessed December 22, 2016).</mixed-citation></ref><ref id="CR143"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>Y</given-names></name><name><surname>Ganesh</surname><given-names>A</given-names></name><name><surname>Wright</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>W</given-names></name><name><surname>Ma</surname><given-names>Y</given-names></name></person-group><article-title>Rasl: Robust alignment by sparse and low-rank decomposition for linearly correlated images</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><year>2012</year><volume>34</volume><issue>11</issue><fpage>2233</fpage><lpage>2246</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2011.282</pub-id><pub-id pub-id-type="pmid">22213763</pub-id></element-citation></ref><ref id="CR144"><mixed-citation publication-type="other">P&#x000e9;rez, F., &#x00026; Granger, B. E. (2007). IPython: a system for interactive scientific computing. <italic>Computing in Science and Engineering 9,</italic> 21&#x02013;29, (Code: <ext-link ext-link-type="uri" xlink:href="https://ipython.org/">https://ipython.org/</ext-link>, Status: Online; accessed December 22, 2016).</mixed-citation></ref><ref id="CR145"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pernici</surname><given-names>F</given-names></name><name><surname>Del&#x000a0;Bimbo</surname><given-names>A</given-names></name></person-group><article-title>Object tracking by oversampling local features</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2014</year><volume>36</volume><issue>12</issue><fpage>2538</fpage><lpage>2551</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2013.250</pub-id></element-citation></ref><ref id="CR146"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname><given-names>PJ</given-names></name><name><surname>Moon</surname><given-names>H</given-names></name><name><surname>Rizvi</surname><given-names>SA</given-names></name><name><surname>Rauss</surname><given-names>PJ</given-names></name></person-group><article-title>The feret evaluation methodology for face-recognition algorithms</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2000</year><volume>22</volume><issue>10</issue><fpage>1090</fpage><lpage>1104</lpage><pub-id pub-id-type="doi">10.1109/34.879790</pub-id></element-citation></ref><ref id="CR147"><mixed-citation publication-type="other">Pighin, F., Szeliski, R., &#x00026; Salesin, D. H. (1999). Resynthesizing facial animation through 3d model-based tracking. In <italic>IEEE Proceedings of International Conference on Computer Vision (ICCV)</italic> (vol&#x000a0;1, pp. 143&#x02013;150). IEEE.</mixed-citation></ref><ref id="CR148"><mixed-citation publication-type="other">Poling, B., Lerman, G., &#x00026; Szlam, A. (2014). Better feature tracking through subspace constraints. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 3454&#x02013;3461).</mixed-citation></ref><ref id="CR149"><mixed-citation publication-type="other">Qi, Y., Zhang, S., Qin, L., Yao, H., Huang, Q., &#x00026; Yang, J. L. M. H. (2016). Hedged deep tracking. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic>. IEEE (Code: <ext-link ext-link-type="uri" xlink:href="https://sites.google.com/site/yuankiqi/hdt/">https://sites.google.com/site/yuankiqi/hdt/</ext-link>, Status: Online; accessed December 4, 2016).</mixed-citation></ref><ref id="CR150"><mixed-citation publication-type="other">Qian, R. J., Sezan, M. I., &#x00026; Matthews, K. E. (1998). A robust real-time face tracking algorithm. In <italic>IEEE proceedings of international conference on image processing (ICIP)</italic> (vol&#x000a0;1, pp 131&#x02013;135). IEEE.</mixed-citation></ref><ref id="CR151"><mixed-citation publication-type="other">Rajamanoharan, G., &#x00026; Cootes, T. (2015). Multi-view constrained local models for large head angle face tracking. In <italic>IEEE proceedings of international conference on computer vision, 300 videos in the wild (300-VW): Facial landmark tracking in-the-wild challenge &#x00026; workshop (ICCV-W)</italic>.</mixed-citation></ref><ref id="CR152"><mixed-citation publication-type="other">Ranjan, R., Patel, V. M., &#x00026; Chellappa, R. (2015). A deep pyramid deformable part model for face detection. <italic>IEEE International Conference on Biometrics Theory</italic> (pp. 1&#x02013;8). IEEE: Applications and Systems (BTAS).</mixed-citation></ref><ref id="CR153"><mixed-citation publication-type="other">R&#x000e4;tsch, M., Romdhani, S., &#x00026; Vetter, T. (2004). Efficient face detection by a cascaded support vector machine using haar-like features. In <italic>Pattern recognition</italic> (pp. 62&#x02013;70). Springer.</mixed-citation></ref><ref id="CR154"><mixed-citation publication-type="other">Ren, S., Cao, X., Wei, Y., &#x00026; Sun, J. (2014). Face alignment at 3000 fps via regressing local binary features. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 1685&#x02013;1692). IEEE.</mixed-citation></ref><ref id="CR155"><mixed-citation publication-type="other">Romdhani, S., Torr, P., Sch&#x000f6;lkopf, B., &#x00026; Blake, A. (2001). Computationally efficient face detection. In <italic>IEEE proceedings of international conference on computer vision (ICCV)</italic> (vol&#x000a0;2, pp 695&#x02013;700). IEEE.</mixed-citation></ref><ref id="CR156"><mixed-citation publication-type="other">Ross, D., Lim, J., Lin, R. S., &#x00026; Yang, M. H. (2015). <italic>Dudek face sequence</italic>. <ext-link ext-link-type="uri" xlink:href="http://www.cs.toronto.edu/%7edross/ivt/">http://www.cs.toronto.edu/~dross/ivt/</ext-link> (Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR157"><mixed-citation publication-type="other">Ross, D. A., Lim, J., Lin, R. S., &#x00026; Yang, M. H. (2008). Incremental learning for robust visual tracking. <italic>International Journal of Computer Vision (IJCV) 77,</italic>(1&#x02013;3), 125&#x02013;141, (Code: <ext-link ext-link-type="uri" xlink:href="http://www.cs.toronto.edu/%7edross/ivt/">http://www.cs.toronto.edu/~dross/ivt/</ext-link>, Status: Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR158"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rueckert</surname><given-names>D</given-names></name><name><surname>Sonoda</surname><given-names>LI</given-names></name><name><surname>Hayes</surname><given-names>C</given-names></name><name><surname>Hill</surname><given-names>DL</given-names></name><name><surname>Leach</surname><given-names>MO</given-names></name><name><surname>Hawkes</surname><given-names>DJ</given-names></name></person-group><article-title>Nonrigid registration using free-form deformations: Application to breast mr images</article-title><source>IEEE Transactions on Medical Imaging</source><year>1999</year><volume>18</volume><issue>8</issue><fpage>712</fpage><lpage>721</lpage><pub-id pub-id-type="doi">10.1109/42.796284</pub-id><pub-id pub-id-type="pmid">10534053</pub-id></element-citation></ref><ref id="CR159"><mixed-citation publication-type="other">Sagonas, C., Tzimiropoulos, G., Zafeiriou, S., &#x00026; Pantic, M. (2013a). 300 faces in-the-wild challenge: The first facial landmark localization challenge. In <italic>IEEE proceedings of international conference on computer vision (ICCV-W), 300 faces in-the-wild challenge (300-W)</italic> (pp. 397&#x02013;403).</mixed-citation></ref><ref id="CR160"><mixed-citation publication-type="other">Sagonas, C., Tzimiropoulos, G., Zafeiriou, S., &#x00026; Pantic, M. (2013b). A semi-automatic methodology for facial landmark annotation. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR-W), 5th workshop on analysis and modeling of faces and gestures</italic> (pp. 896&#x02013;903).</mixed-citation></ref><ref id="CR161"><mixed-citation publication-type="other">Sagonas, C., Panagakis, Y., Zafeiriou, S., &#x00026; Pantic, M. (2014). Raps: Robust and efficient automatic construction of person-specific deformable models. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 1789&#x02013;1796).</mixed-citation></ref><ref id="CR162"><mixed-citation publication-type="other">Sagonas, C., Antonakos, E., Tzimiropoulos, G., Zafeiriou, S., &#x00026; Pantic, M. (2015). 300 Faces in-the-wild challenge: Database and results. In <italic>Image and vision computing</italic>.</mixed-citation></ref><ref id="CR163"><mixed-citation publication-type="other">Sakai, T., Nagao, M., &#x00026; Kanade, T. (1972). Computer analysis and classification of photographs of human faces. In <italic>Proceedings of First USA-JAPAN Computer Conference</italic> (pp. 55&#x02013;62).</mixed-citation></ref><ref id="CR164"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salti</surname><given-names>S</given-names></name><name><surname>Cavallaro</surname><given-names>A</given-names></name><name><surname>Stefano</surname><given-names>LD</given-names></name></person-group><article-title>Adaptive appearance modeling for video tracking: Survey and evaluation</article-title><source>IEEE Transactions in Image Processing (TIP)</source><year>2012</year><volume>21</volume><issue>10</issue><fpage>4334</fpage><lpage>4348</lpage><pub-id pub-id-type="doi">10.1109/TIP.2012.2206035</pub-id></element-citation></ref><ref id="CR165"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saragih</surname><given-names>JM</given-names></name><name><surname>Lucey</surname><given-names>S</given-names></name><name><surname>Cohn</surname><given-names>JF</given-names></name></person-group><article-title>Deformable model fitting by regularized landmark mean-shift</article-title><source>International Journal of Computer Vision</source><year>2011</year><volume>91</volume><issue>2</issue><fpage>200</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1007/s11263-010-0380-4</pub-id></element-citation></ref><ref id="CR166"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneiderman</surname><given-names>H</given-names></name><name><surname>Kanade</surname><given-names>T</given-names></name></person-group><article-title>Object detection using the statistics of parts</article-title><source>International Journal of Computer Vision (IJCV)</source><year>2004</year><volume>56</volume><issue>3</issue><fpage>151</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1023/B:VISI.0000011202.85607.00</pub-id></element-citation></ref><ref id="CR167"><mixed-citation publication-type="other">Schroff, F., Kalenichenko, D., &#x00026; Philbin, J. (2015). Facenet: A unified embedding for face recognition and clustering. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 815&#x02013;823).</mixed-citation></ref><ref id="CR168"><mixed-citation publication-type="other">Schwerdt, K., &#x00026; Crowley, J. L. (2000). Robust face tracking using color. In <italic>IEEE proceedings of international conference on automatic face and gesture recognition (FG)</italic> (pp. 90&#x02013;95). IEEE.</mixed-citation></ref><ref id="CR169"><mixed-citation publication-type="other">Sevilla-Lara, L., &#x00026; Learned-Miller, E. (2012). Distribution fields for tracking. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 1910&#x02013;1917). IEEE. (Code: <ext-link ext-link-type="uri" xlink:href="http://people.cs.umass.edu/%7elsevilla/trackingDF.html">http://people.cs.umass.edu/~lsevilla/trackingDF.html</ext-link>, Status: Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR170"><mixed-citation publication-type="other">Shen, J., Zafeiriou, S., Chrysos, G., Kossaifi, J., Tzimiropoulos, G., &#x00026; Pantic, M. (2015). The first facial landmark tracking in-the-wild challenge: Benchmark and results. In <italic>IEEE proceedings of international conference on computer vision, 300 videos in the wild (300-VW): Facial landmark tracking in-the-wild challenge &#x00026; workshop (ICCV-W)</italic>.</mixed-citation></ref><ref id="CR171"><mixed-citation publication-type="other">Shen, X., Lin, Z., Brandt, J., &#x00026; Wu, Y. (2013). Detecting and aligning faces by image retrieval. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 3460&#x02013;3467).</mixed-citation></ref><ref id="CR172"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smeulders</surname><given-names>AW</given-names></name><name><surname>Chu</surname><given-names>DM</given-names></name><name><surname>Cucchiara</surname><given-names>R</given-names></name><name><surname>Calderara</surname><given-names>S</given-names></name><name><surname>Dehghan</surname><given-names>A</given-names></name><name><surname>Shah</surname><given-names>M</given-names></name></person-group><article-title>Visual tracking: An experimental survey</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2014</year><volume>36</volume><issue>7</issue><fpage>1442</fpage><lpage>1468</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2013.230</pub-id></element-citation></ref><ref id="CR173"><mixed-citation publication-type="other">Snape, P., Roussos, A., Panagakis, Y., &#x00026; Zafeiriou, S. (2015). Face flow. In <italic>IEEE proceedings of international conference on computer vision (ICCV)</italic>.</mixed-citation></ref><ref id="CR174"><mixed-citation publication-type="other">Sobottka, K., &#x00026; Pitas, I. (1996). Face localization and facial feature extraction based on shape and color information. In <italic>IEEE proceedings of international conference on image processing (ICIP)</italic> (vol&#x000a0;3, pp 483&#x02013;486). IEEE.</mixed-citation></ref><ref id="CR175"><mixed-citation publication-type="other">Stern, H., &#x00026; Efros, B. (2002). Adaptive color space switching for face tracking in multi-colored lighting environments. In <italic>IEEE proceedings of international conference on automatic face and gesture recognition (FG)</italic> (pp 249&#x02013;254). IEEE.</mixed-citation></ref><ref id="CR176"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sung</surname><given-names>J</given-names></name><name><surname>Kim</surname><given-names>D</given-names></name></person-group><article-title>Adaptive active appearance model with incremental learning</article-title><source>Pattern Recognition Letters</source><year>2009</year><volume>30</volume><issue>4</issue><fpage>359</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2008.11.006</pub-id></element-citation></ref><ref id="CR177"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sung</surname><given-names>J</given-names></name><name><surname>Kanade</surname><given-names>T</given-names></name><name><surname>Kim</surname><given-names>D</given-names></name></person-group><article-title>Pose robust face tracking by combining active appearance models and cylinder head models</article-title><source>International Journal of Computer Vision (IJCV)</source><year>2008</year><volume>80</volume><issue>2</issue><fpage>260</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1007/s11263-007-0125-1</pub-id></element-citation></ref><ref id="CR178"><mixed-citation publication-type="other">Taigman, Y., Yang, M., Ranzato, M., &#x00026; Wolf, L. (2014). Deepface: Closing the gap to human-level performance in face verification. In <italic>IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (pp. 1701&#x02013;1708).</mixed-citation></ref><ref id="CR179"><mixed-citation publication-type="other">Tao, H., &#x00026; Huang, T. S. (1998). Connected vibrations: A modal analysis approach for non-rigid motion tracking. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp 735&#x02013;740). IEEE.</mixed-citation></ref><ref id="CR180"><mixed-citation publication-type="other">Toyama, K. (1998). <italic>Look, ma-no hands! hands-free cursor control with real-time 3d face tracking. PUI98</italic>.</mixed-citation></ref><ref id="CR181"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tresadern</surname><given-names>PA</given-names></name><name><surname>Ionita</surname><given-names>MC</given-names></name><name><surname>Cootes</surname><given-names>TF</given-names></name></person-group><article-title>Real-time facial feature tracking on a mobile device</article-title><source>International Journal of Computer Vision (IJCV)</source><year>2012</year><volume>96</volume><issue>3</issue><fpage>280</fpage><lpage>289</lpage><pub-id pub-id-type="doi">10.1007/s11263-011-0464-9</pub-id></element-citation></ref><ref id="CR182"><mixed-citation publication-type="other">Tzimiropoulos, G. (2015). Project-out cascaded regression with an application to face alignment. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic>. (pp. 3659&#x02013;3667).</mixed-citation></ref><ref id="CR183"><mixed-citation publication-type="other">Tzimiropoulos, G., &#x00026; Pantic, M. (2013). Optimization problems for fast aam fitting in-the-wild. In <italic>IEEE proceedings of international conference on computer vision (ICCV)</italic> (pp. 593&#x02013;600). IEEE.</mixed-citation></ref><ref id="CR184"><mixed-citation publication-type="other">Tzimiropoulos, G., &#x00026; Pantic, M. (2014). Gauss-newton deformable part models for face alignment in-the-wild. In <italic>IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (pp. 1851&#x02013;1858).</mixed-citation></ref><ref id="CR185"><mixed-citation publication-type="other">Tzimiropoulos, G., Alabort-i-Medina, J., Zafeiriou, S., &#x00026; Pantic, M. (2012). Generic active appearance models revisited. In <italic>Asian conference on computer vision (ACCV)</italic> (pp. 650&#x02013;663). Springer.</mixed-citation></ref><ref id="CR186"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tzimiropoulos</surname><given-names>G</given-names></name><name><surname>Alabort-i Medina</surname><given-names>J</given-names></name><name><surname>Zafeiriou</surname><given-names>S</given-names></name><name><surname>Pantic</surname><given-names>M</given-names></name></person-group><article-title>Active orientation models for face alignment in-the-wild</article-title><source>IEEE Transactions on Information Forensics and Security</source><year>2014</year><volume>9</volume><issue>12</issue><fpage>2024</fpage><lpage>2034</lpage><pub-id pub-id-type="doi">10.1109/TIFS.2014.2361018</pub-id></element-citation></ref><ref id="CR187"><mixed-citation publication-type="other">Uricar, M., &#x00026; Franc, V. (2015). Real-time facial landmark tracking by tree-based deformable part model based detector. In <italic>IEEE proceedings of international conference on computer vision, 300 videos in the wild (300-VW): Facial landmark tracking in-the-wild challenge &#x00026; workshop (ICCV-W)</italic>.</mixed-citation></ref><ref id="CR188"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vadakkepat</surname><given-names>P</given-names></name><name><surname>Lim</surname><given-names>P</given-names></name><name><surname>De Silva</surname><given-names>LC</given-names></name><name><surname>Jing</surname><given-names>L</given-names></name><name><surname>Ling</surname><given-names>LL</given-names></name></person-group><article-title>Multimodal approach to human-face detection and tracking</article-title><source>IEEE Transactions on Industrial Electronics</source><year>2008</year><volume>55</volume><issue>3</issue><fpage>1385</fpage><lpage>1393</lpage><pub-id pub-id-type="doi">10.1109/TIE.2007.903993</pub-id></element-citation></ref><ref id="CR189"><mixed-citation publication-type="other">Viola, P., &#x00026; Jones, M. (2001). Rapid object detection using a boosted cascade of simple features. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (vol&#x000a0;1, pp I&#x02013;511). IEEE.</mixed-citation></ref><ref id="CR190"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viola</surname><given-names>P</given-names></name><name><surname>Jones</surname><given-names>MJ</given-names></name></person-group><article-title>Robust real-time face detection</article-title><source>International Journal of Computer Vision (IJCV)</source><year>2004</year><volume>57</volume><issue>2</issue><fpage>137</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1023/B:VISI.0000013087.49260.fb</pub-id></element-citation></ref><ref id="CR191"><mixed-citation publication-type="other">Wang, N., Gao, X., Tao, D., &#x00026; Li, X. (2014). <italic>Facial feature point detection: A comprehensive survey</italic>. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1410.1037">arXiv:1410.1037</ext-link>.</mixed-citation></ref><ref id="CR192"><mixed-citation publication-type="other">Wang, P., Ji, Q. (2004). Multi-view face detection under complex scene based on combined svms. In <italic>IEEE International Conference on Pattern Recognition (ICPR)</italic> (pp. 179&#x02013;182).</mixed-citation></ref><ref id="CR193"><mixed-citation publication-type="other">Wang, X., Valstar, M., Martinez, B., Haris&#x000a0;Khan, M., &#x00026; Pridmore, T. (2015). Tric-track: Tracking by regression with incrementally learned cascades. In <italic>IEEE proceedings of international conference on computer vision (ICCV)</italic> (pp. 4337&#x02013;4345).</mixed-citation></ref><ref id="CR194"><mixed-citation publication-type="other">Wei, X., Zhu, Z., Yin, L., &#x00026; Ji, Q. (2004). A real time face tracking and animation system. In <italic>IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition Workshops (CVPR&#x02019;W)</italic> (pp. 71&#x02013;71). IEEE.</mixed-citation></ref><ref id="CR195"><mixed-citation publication-type="other">Weise, T., Bouaziz, S., Li, H., &#x00026; Pauly, M. (2011). Realtime performance-based facial animation. In <italic>ACM transactions on graphics (TOG)</italic> (vol 30, p. 77). ACM.</mixed-citation></ref><ref id="CR196"><mixed-citation publication-type="other">Wolf, L., Hassner, T., Maoz, I. (2011). Face recognition in unconstrained videos with matched background similarity. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 529&#x02013;534).</mixed-citation></ref><ref id="CR197"><mixed-citation publication-type="other">Wu, B., Ai, H., Huang, C., &#x00026; Lao, S. (2004). Fast rotation invariant multi-view face detection based on real adaboost. In <italic>IEEE proceedings of international conference on automatic face and gesture recognition (FG)</italic> (pp. 79&#x02013;84). IEEE.</mixed-citation></ref><ref id="CR198"><mixed-citation publication-type="other">Wu, Y., &#x00026; Ji, Q. (2015). Shape augmented regression method for face alignment. In <italic>IEEE proceedings of international conference on computer vision, 300 videos in the wild (300-VW): Facial landmark tracking in-the-wild challenge &#x00026; workshop (ICCV-W)</italic>.</mixed-citation></ref><ref id="CR199"><mixed-citation publication-type="other">Wu, Y., Shen, B., &#x00026; Ling, H. (2012). Online robust image alignment via iterative convex optimization. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp.&#x000a0;1808&#x02013;1814). IEEE. (Code: <ext-link ext-link-type="uri" xlink:href="https://sites.google.com/site/trackerbenchmark/benchmarks/v10">https://sites.google.com/site/trackerbenchmark/benchmarks/v10</ext-link>, Status: Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR200"><mixed-citation publication-type="other">Wu, Y., Lim, J., &#x00026; Yang, M. H. (2013). Online object tracking: A benchmark. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic>.</mixed-citation></ref><ref id="CR201"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Lim</surname><given-names>J</given-names></name><name><surname>Yang</surname><given-names>MH</given-names></name></person-group><article-title>Object tracking benchmark</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2015</year><volume>37</volume><issue>9</issue><fpage>1834</fpage><lpage>1848</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2014.2388226</pub-id></element-citation></ref><ref id="CR202"><mixed-citation publication-type="other">Xiao, J., Baker, S., Matthews, I., &#x00026; Kanade, T. (2004). Real-time combined 2d+ 3d active appearance models. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 535&#x02013;542).</mixed-citation></ref><ref id="CR203"><mixed-citation publication-type="other">Xiao, S., Yan, S., &#x00026; Kassim, A. (2015). Facial landmark detection via progressive initialization. In <italic>IEEE Proceedings of international conference on computer vision, 300 videos in the wild (300-VW): facial landmark tracking in-the-wild challenge &#x00026; workshop (ICCV-W)</italic>.</mixed-citation></ref><ref id="CR204"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>Z</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>D</given-names></name></person-group><article-title>L2-RLS-based object tracking</article-title><source>IEEE Transactions on Circuits and Systems for Video Technology</source><year>2014</year><volume>24</volume><issue>8</issue><fpage>1301</fpage><lpage>1309</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2013.2291355</pub-id></element-citation></ref><ref id="CR205"><mixed-citation publication-type="other">Xiong, X., &#x00026; De&#x000a0;la Torre, F. (2013). Supervised descent method and its applications to face alignment. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 532&#x02013;539).</mixed-citation></ref><ref id="CR206"><mixed-citation publication-type="other">Xiong, X., &#x00026; De&#x000a0;la Torre, F. (2015). Global supervised descent method. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 2664&#x02013;2673).</mixed-citation></ref><ref id="CR207"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yacoob</surname><given-names>Y</given-names></name><name><surname>Davis</surname><given-names>LS</given-names></name></person-group><article-title>Recognizing human facial expressions from long image sequences using optical flow</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>1996</year><volume>18</volume><issue>6</issue><fpage>636</fpage><lpage>642</lpage><pub-id pub-id-type="doi">10.1109/34.506414</pub-id></element-citation></ref><ref id="CR208"><mixed-citation publication-type="other">Yan, J., Zhang, X., Lei, Z., Yi, D., &#x00026; Li, S. Z. (2013). Structural models for face detection. In <italic>IEEE Proceedings of International Conference on Automatic Face and Gesture Recognition (FG)</italic> (pp.&#x000a0;1&#x02013;6). IEEE.</mixed-citation></ref><ref id="CR209"><mixed-citation publication-type="other">Yan, J., Lei, Z., Wen, L., &#x00026; Li, S. (2014). The fastest deformable part model for object detection. In <italic>IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (pp.&#x000a0;2497&#x02013;2504).</mixed-citation></ref><ref id="CR210"><mixed-citation publication-type="other">Yang, B., Yan, J., Lei, Z., &#x00026; Li, S. Z. (2014a). Aggregate channel features for multi-view face detection. In <italic>IEEE international joint conference on biometrics (IJCB)</italic> (pp. 1&#x02013;8). IEEE.</mixed-citation></ref><ref id="CR211"><mixed-citation publication-type="other">Yang, F., Lu, H., &#x00026; Yang, M. H. (2014b). Robust superpixel tracking. <italic>IEEE Transactions in Image Processing (TIP), 23</italic>(4), 1639&#x02013;1651, (Code: <ext-link ext-link-type="uri" xlink:href="http://www.umiacs.umd.edu/%7efyang/spt.html">http://www.umiacs.umd.edu/~fyang/spt.html</ext-link>, Status: Online; accessed August 18, 2016).</mixed-citation></ref><ref id="CR212"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>H</given-names></name><name><surname>Shao</surname><given-names>L</given-names></name><name><surname>Zheng</surname><given-names>F</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Song</surname><given-names>Z</given-names></name></person-group><article-title>Recent advances and trends in visual tracking: A review</article-title><source>Neurocomputing</source><year>2011</year><volume>74</volume><issue>18</issue><fpage>3823</fpage><lpage>3831</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2011.07.024</pub-id></element-citation></ref><ref id="CR213"><mixed-citation publication-type="other">Yang, J., Deng, J., Zhang, K., &#x00026; Liu, Q. (2015a). Facial shape tracking via spatio-temporal cascade shape regression. In <italic>IEEE proceedings of international conference on computer vision, 300 videos in the wild (300-VW): Facial landmark tracking in-the-wild challenge &#x00026; workshop (ICCV-W)</italic>.</mixed-citation></ref><ref id="CR214"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>MH</given-names></name><name><surname>Kriegman</surname><given-names>DJ</given-names></name><name><surname>Ahuja</surname><given-names>N</given-names></name></person-group><article-title>Detecting faces in images: A survey</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</source><year>2002</year><volume>24</volume><issue>1</issue><fpage>34</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1109/34.982883</pub-id></element-citation></ref><ref id="CR215"><mixed-citation publication-type="other">Yang, S., Luo, P., Loy, C. C., &#x00026; Tang, X. (2015b). From facial parts responses to face detection: A deep learning approach. In <italic>IEEE Proceedings of International Conference on Computer Vision (ICCV)</italic> (pp.&#x000a0;3676&#x02013;3684).</mixed-citation></ref><ref id="CR216"><mixed-citation publication-type="other">Yao, R., Shi, Q., Shen, C., Zhang, Y., &#x00026; Hengel, A. (2013). Part-based visual tracking with online latent structural learning. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 2363&#x02013;2370).</mixed-citation></ref><ref id="CR217"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zafeiriou</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name></person-group><article-title>A survey on face detection in the wild past, present and future</article-title><source>Computer Vision and Image Understanding</source><year>2015</year><volume>138</volume><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.cviu.2015.03.015</pub-id></element-citation></ref><ref id="CR218"><mixed-citation publication-type="other">Zhang, C., &#x00026; Zhang, Z. (2010). <italic>A survey of recent advances in face detection</italic>. Technical report, Microsoft Research.</mixed-citation></ref><ref id="CR219"><mixed-citation publication-type="other">Zhang, C., &#x00026; Zhang, Z. (2014). Improving multiview face detection with multi-task deep convolutional neural networks. In <italic>IEEE winter conference on applications of computer vision (WACV)</italic> (pp. 1036&#x02013;1041). IEEE.</mixed-citation></ref><ref id="CR220"><mixed-citation publication-type="other">Zhang, J., Ma, S., &#x00026; Sclaroff, S. (2014a). Meem: robust tracking via multiple experts using entropy minimization. In <italic>Proceedings of European conference on computer vision (ECCV)</italic> (pp.&#x000a0;188&#x02013;203). (Code: <ext-link ext-link-type="uri" xlink:href="http://cs-people.bu.edu/jmzhang/MEEM/MEEM.html">http://cs-people.bu.edu/jmzhang/MEEM/MEEM.html</ext-link>, Status: Online; accessed August 18, 2016).</mixed-citation></ref><ref id="CR221"><mixed-citation publication-type="other">Zhang, K., Zhang, L., Liu, Q., Zhang, D., &#x00026; Yang, M. H. (2014b). Fast visual tracking via dense spatio-temporal context learning. In <italic>Proceedings of European conference on computer vision (ECCV)</italic> (pp. 127&#x02013;141). (Code: <ext-link ext-link-type="uri" xlink:href="http://www4.comp.polyu.edu.hk/%7ecslzhang/STC/STC.htm">http://www4.comp.polyu.edu.hk/~cslzhang/STC/STC.htm</ext-link>, Status: Online; accessed August 18, 2016).</mixed-citation></ref><ref id="CR222"><mixed-citation publication-type="other">Zhang, K., Zhang, L., &#x00026; Yang, M. H. (2014c). Fast compressive tracking. <italic>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 36</italic>(10), 2002&#x02013;2015, (Code: <ext-link ext-link-type="uri" xlink:href="http://www4.comp.polyu.edu.hk/%7ecslzhang/FCT/FCT.htm">http://www4.comp.polyu.edu.hk/~cslzhang/FCT/FCT.htm</ext-link>, Status: Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR223"><mixed-citation publication-type="other">Zhang, K., Zhang, Z., Li, Z., &#x00026; Qiao, Y. (2016). Joint face detection and alignment using multi-task cascaded convolutional networks. <italic>IEEE Signal Processing Letters, 23</italic>(10), 1499&#x02013;1503, (Code: <ext-link ext-link-type="uri" xlink:href="https://github.com/kpzhang93/MTCNN%5fface%5fdetection%5falignment">https://github.com/kpzhang93/MTCNN_face_detection_alignment</ext-link>, Status: Online; accessed December 24, 2016).</mixed-citation></ref><ref id="CR224"><mixed-citation publication-type="other">Zhang, L., &#x00026; van&#x000a0;der Maaten, L. (2013). Structure preserving object tracking. In <italic>IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (pp. 1838&#x02013;1845). IEEE.</mixed-citation></ref><ref id="CR225"><mixed-citation publication-type="other">Zhang, L., van&#x000a0;der Maaten, L. (2014). Preserving structure in model-free tracking. <italic>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 36</italic>(4), 756&#x02013;769, (Code: <ext-link ext-link-type="uri" xlink:href="http://visionlab.tudelft.nl/spot">http://visionlab.tudelft.nl/spot</ext-link>, Status: Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR226"><mixed-citation publication-type="other">Zhang, T., Ghanem, B., Liu, S., &#x00026; Ahuja, N. (2012). Robust visual tracking via multi-task sparse learning. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp. 2042&#x02013;2049). IEEE.</mixed-citation></ref><ref id="CR227"><mixed-citation publication-type="other">Zhang, T., Liu, S., Ahuja, N., Yang, M. H., &#x00026; Ghanem, B. (2014d). Robust visual tracking via consistent low-rank sparse learning. <italic>International Journal of Computer Vision (IJCV), 111</italic>(2), 171&#x02013;190, (Code: <ext-link ext-link-type="uri" xlink:href="http://nlpr-web.ia.ac.cn/mmc/homepage/tzzhang/Project%5fTianzhu/zhang%5fIJCV14/Robust%20Visual%20Tracking%20Via%20Consistent%20Low-Rank%20Sparse.html">http://nlpr-web.ia.ac.cn/mmc/homepage/tzzhang/Project_Tianzhu/zhang_IJCV14/Robust%20Visual%20Tracking%20Via%20Consistent%20Low-Rank%20Sparse.html</ext-link>, Status: Online; accessed June 2, 2016).</mixed-citation></ref><ref id="CR228"><mixed-citation publication-type="other">Zhang, W., Wang, Q., &#x00026; Tang, X. (2008). Real time feature based 3-d deformable face tracking. In <italic>Proceedings of European Conference on Computer Vision (ECCV)</italic> (pp. 720&#x02013;732). Springer.</mixed-citation></ref><ref id="CR229"><mixed-citation publication-type="other">Zhu, S., Li, C., Loy, C. C., &#x00026; Tang, X. (2015). Face alignment by coarse-to-fine shape searching. In <italic>IEEE proceedings of international conference on computer vision and pattern recognition (CVPR)</italic> (pp.&#x000a0;4998&#x02013;5006) (Code: <ext-link ext-link-type="uri" xlink:href="https://github.com/zhusz/CVPR15-CFSS">https://github.com/zhusz/CVPR15-CFSS</ext-link>, Status: Online; accessed December 4, 2016).</mixed-citation></ref><ref id="CR230"><mixed-citation publication-type="other">Zhu, X., &#x00026; Ramanan, D. (2012). Face detection, pose estimation, and landmark localization in the wild. In: <italic>IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (pp. 2879&#x02013;2886). IEEE. (Code: <ext-link ext-link-type="uri" xlink:href="https://www.ics.uci.edu/%7exzhu/face">https://www.ics.uci.edu/~xzhu/face</ext-link>, Status: Online; accessed June 2, 2016).</mixed-citation></ref></ref-list></back></article>