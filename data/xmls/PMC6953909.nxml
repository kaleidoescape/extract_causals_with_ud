<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?properties manuscript?><front><journal-meta><journal-id journal-id-type="nlm-journal-id">101758847</journal-id><journal-id journal-id-type="pubmed-jr-id">49079</journal-id><journal-id journal-id-type="nlm-ta">Front Comput Sci</journal-id><journal-id journal-id-type="iso-abbrev">Front Comput Sci</journal-id><journal-title-group><journal-title>Frontiers in computer science</journal-title></journal-title-group><issn pub-type="epub">2624-9898</issn></journal-meta><article-meta><article-id pub-id-type="pmid">31930192</article-id><article-id pub-id-type="pmc">6953909</article-id><article-id pub-id-type="doi">10.3389/fcomp.2019.00011</article-id><article-id pub-id-type="manuscript">NIHMS1061808</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>D-PAttNet: Dynamic Patch-Attentive Deep Network for Action Unit Detection</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ertugrul</surname><given-names>Itir Onal</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref rid="CR1" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Le</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Jeni</surname><given-names>L&#x000e1;szl&#x000f3; A.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Cohn</surname><given-names>Jeffrey F.</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib></contrib-group><aff id="A1"><label>1</label>Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States,</aff><aff id="A2"><label>2</label>School of Computer Science, Northwestern Polytechnical University, Xian, China,</aff><aff id="A3"><label>3</label>Department of Psychology, University of Pittsburgh, Pittsburgh, PA, United States</aff><author-notes><fn fn-type="con" id="FN1"><p id="P1">AUTHOR CONTRIBUTIONS</p><p id="P2">IO designed the architecture, implemented patch-learning and attention modules, and wrote the manuscript. LY implemented dynamic patch encoding and ran the experiments. LJ implemented the 3D registration and face normalization methods. JC contributed to conceptualization, design, and writing. All authors discussed the study and reviewed and approved the final version of the manuscript.</p></fn><corresp id="CR1"><label>*</label><bold>Correspondence:</bold> Itir Onal Ertugrul, <email>iertugru@andrew.cmu.edu</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>18</day><month>12</month><year>2019</year></pub-date><pub-date pub-type="epub"><day>29</day><month>11</month><year>2019</year></pub-date><pub-date pub-type="ppub"><month>11</month><year>2019</year></pub-date><pub-date pub-type="pmc-release"><day>10</day><month>1</month><year>2020</year></pub-date><volume>1</volume><elocation-id>11</elocation-id><permissions><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><self-uri xlink:href="https://www.frontiersin.org/articles/10.3389/fcomp.2019.00011/full"/><abstract id="ABS1"><p id="P3">Facial action units (AUs) relate to specific local facial regions. Recent efforts in automated AU detection have focused on learning the facial patch representations to detect specific AUs. These efforts have encountered three hurdles. First, they implicitly assume that facial patches are robust to head rotation; yet non-frontal rotation is common. Second, mappings between AUs and patches are defined a priori, which ignores co-occurrences among AUs. And third, the dynamics of AUs are either ignored or modeled sequentially rather than simultaneously as in human perception. Inspired by recent advances in human perception, we propose a dynamic patch-attentive deep network, called D-PAttNet, for AU detection that (i) controls for 3D head and face rotation, (ii) learns mappings of patches to AUs, and (iii) models spatiotemporal dynamics. D-PAttNet approach significantly improves upon existing state of the art.</p></abstract><kwd-group><kwd>action unit detection</kwd><kwd>3D face registration</kwd><kwd>3D-CNN</kwd><kwd>sigmoidal attention</kwd><kwd>patch-based</kwd></kwd-group></article-meta></front><body><sec id="S1"><label>1.</label><title>INTRODUCTION</title><p id="P4">Facial actions communicate intention, emotion, and physical state (<xref rid="R49" ref-type="bibr">Tian et al., 2001</xref>). The most comprehensive method to annotate facial action is the anatomically-based Facial Action Coding System (FACS) (<xref rid="R10" ref-type="bibr">Ekman et al., 2002</xref>). Action units defined in FACS correspond to facial muscle movements that individually or in combination can describe nearly all possible facial expressions. Automated detection of AUs has become a crucial computer vision problem.</p><p id="P5">The core of the human neural system for face and facial action perception consists of three bilateral regions, the occipital face area (OFA), fusiform face area (FFA), and superior temporal sulcus (STS) (<xref rid="R18" ref-type="bibr">Haxby et al., 2000</xref>). Previous work suggests that the OFA represents face parts, including eyes, nose, and mouth, in the early stage of face perception (<xref rid="R33" ref-type="bibr">Liu et al., 2010</xref>; <xref rid="R38" ref-type="bibr">Nichols et al., 2010</xref>; <xref rid="R2" ref-type="bibr">Arcurio et al., 2012</xref>). At a higher-level, the FFA performs holistic processing and representations of identity (<xref rid="R14" ref-type="bibr">George et al., 1999</xref>; <xref rid="R19" ref-type="bibr">Hoffman and Haxby, 2000</xref>). The STS is sensitive to facial dynamics and involves the representation of changeable aspects of faces such as expression, lip movement, and eye gaze (<xref rid="R19" ref-type="bibr">Hoffman and Haxby, 2000</xref>). The anatomical location of OFA suggests that it provides input to both the FFA and STS. This system is consistent with hierarchical models (<xref rid="R16" ref-type="bibr">Grill-Spector and Malach, 2004</xref>; <xref rid="R12" ref-type="bibr">Fairhall and Ishai, 2006</xref>) that propose that complex visual objects are recognized via a series of stages in which features of increasing complexity are extracted and analyzed at progressively higher levels of the visual processing stream (<xref rid="R42" ref-type="bibr">Pitcher et al., 2011</xref>). The success of many human-inspired approaches in machine learning urges the following question: Can we model machine perception of facial actions with a hierarchical system analogous to the suggested models of human perception of faces and facial action?</p><p id="P6">Recent approaches to facial action detection have begun to address this question. Analogous to the OFA in human face perception, region learning, or what is referred to as patch learning, separately processes specific facial regions. This work is informed by the observation that the human face is more structured than many other natural images and different face regions have different local statistics (<xref rid="R59" ref-type="bibr">Zhao et al., 2016b</xref>). Variation in local statistics stems from both structural features and transient facial muscle contraction and relaxation. Facial action units (AUs), which are anatomically based, are responsible for muscle contraction and relaxation. For instance, tightening of the eye aperture results from contraction of the inner portion of the orbicularis oculi muscle, which is AU7. Performing AU7 will change the appearance of eye corners and not mouth regions. When the goal is to detect AU7, it is natural to look around eye region more than mouth region. Therefore, due to the locality of AUs, some facial regions are more important than others to detect specific AUs (<xref rid="R57" ref-type="bibr">Zhao et al., 2016a</xref>). Thus, patch learning approaches have components for representing facial parts. These local parts then are integrated holistically in mechanisms analogous to the FFA in human face perception.</p><p id="P7">Patches have been defined in one of two principal ways. One is with respect to fixed grids (<xref rid="R34" ref-type="bibr">Liu et al., 2014</xref>). The other is centered around facial landmarks (<xref rid="R57" ref-type="bibr">Zhao et al., 2016a</xref>). Both approaches assume that patches are invariant to head rotation. That is, when the head moves or rotates, patches are assumed to maintain consistent semantic correspondence. This assumption often is violated. Faces look very different from different poses. Because most registration techniques treat the face as a 2D object, they are unable to accommodate 3D head rotation. In this work, we address this problem.</p><p id="P8">Another problem is that mappings between AUs and patches are defined a priori, and the mappings often fail to exploit co-occurrences among AUs. We know that some AUs frequently co-occur, while others inhibit the activity of others. AU6 (cheek raiser) and AU12 (oblique lip-corner puller) occur together in both Duchenne smiles and in pain expressions. AU24, which presses the lips together, inhibits dropping of the jaw (AU27). Because appearance changes in different facial regions are likely to contribute to the prediction of co-occurring AUs, it may be advantageous to weight the significance of patches to detection of specific AUs. Some patch-based AU detection methods fail to weight the contribution of each patch (<xref rid="R59" ref-type="bibr">Zhao et al., 2016b</xref>). A few of them do by using either regularization on the shallow representation of patches (<xref rid="R57" ref-type="bibr">Zhao et al., 2016a</xref>) or pre-defined attention masks in CNN (<xref rid="R21" ref-type="bibr">Jaiswal and Valstar, 2016</xref>; <xref rid="R44" ref-type="bibr">Sanchez et al., 2018</xref>), which often ignore AU correlations. Below, we show that AU detection can be improved by learning attention maps empirically to accommodate AU correlations.</p><p id="P9">The STS is sensitive to dynamic change in facial parts, and a number of studies have reported that dynamic information contributes to expression perception (<xref rid="R1" ref-type="bibr">Ambadar et al., 2005</xref>; <xref rid="R4" ref-type="bibr">Bould et al., 2008</xref>; <xref rid="R27" ref-type="bibr">K&#x000e4;tsyri and Sams, 2008</xref>; <xref rid="R20" ref-type="bibr">Horstmann and Ansorge, 2009</xref>). Yet, most recent work in machine perception of AUs ignores motion information or dynamics. In static approaches, each video frame is considered independently and outside of its temporal context. Temporal context may matter little for strong AUs but for subtle AUs lack of dynamics weakens the detection. Human observers have difficulty perceiving subtle AUs when motion information is missing (<xref rid="R1" ref-type="bibr">Ambadar et al., 2005</xref>). The same may be true for automated AU detection. When dynamics has been considered, spatial and temporal information typically is handled sequentially. For instance, a CNN represents spatial information and then LSTM models temporal information (<xref rid="R21" ref-type="bibr">Jaiswal and Valstar, 2016</xref>; <xref rid="R6" ref-type="bibr">Chu et al., 2017</xref>; <xref rid="R30" ref-type="bibr">Li et al., 2017</xref>). In human perception, on the other hand, spatiotemporal information may be processed tightly integrated.</p><p id="P10">Informed by human face perception and facial anatomy and dynamics, we propose a dynamic patch-attentive deep network (D-PAttNet) for AU detection. D-PAttNet jointly learns static and dynamic patch representations and weights them for AU detection. We first apply 3D registration to reduce changes from head movement and preserve facial actions that would be distorted by change in pose. Then, we crop local patches that contain the same facial parts across frames and that are informative for detection of specific AUs. We encode patches with individual 2D and 3D CNNs and obtain local representations that capture spatiotemporal information. Inspired by the recent success of attention mechanisms in various tasks including neural machine translation (<xref rid="R37" ref-type="bibr">Luong et al., 2015</xref>), text classification (<xref rid="R54" ref-type="bibr">Yang et al., 2016</xref>), and object detection (<xref rid="R43" ref-type="bibr">Rodr&#x000ed;guez et al., 2018</xref>), we then introduce an attention mechanism to weight the importance of patches in detecting specific AUs. Since our network is trained in an end-to-end manner, the network itself learns (i) static and dynamic encoding of patches and (ii) the degree of attention to those patches to maximize AU detection. Unlike state-of-the-art attention approaches, which employ softmax activation function to &#x0201c;select&#x0201d; where to attend, we propose sigmoidal attention to allow networks to attend to multiple patches when needed.</p><p id="P11">The contributions of this paper are:
<list list-type="bullet" id="L2"><list-item><p id="P12">An end-to-end trainable dynamic patch-attentive deep network that learns to encode static and dynamic patch information and learns to attend to specific patches for the detection of specific AUs.</p></list-item><list-item><p id="P13">A sigmoidal attention mechanism that allows multiple static and dynamic patch encodings to contribute to the prediction of specific AUs.</p></list-item><list-item><p id="P14">Relative to state of the art, an increase of 2.1% performance in F1-score and 0.7% performance in AUC.</p></list-item></list></p></sec><sec id="S2"><label>2.</label><title>RELATED WORK</title><sec id="S3"><label>2.1.</label><title>Using Dynamics for AU Detection</title><p id="P15">Most AU detection approaches model frames individually and ignore the temporal dependencies among them (<xref rid="R5" ref-type="bibr">Chu et al., 2013</xref>; <xref rid="R55" ref-type="bibr">Zeng et al., 2015</xref>; <xref rid="R58" ref-type="bibr">Zhao et al., 2018</xref>; <xref rid="R39" ref-type="bibr">Onal Ertugrul et al., 2019a</xref>,<xref rid="R41" ref-type="bibr">c</xref>). <xref rid="R50" ref-type="bibr">Valstar and Pantic (2007)</xref> combine Support Vector Machines and Hidden Markov Models to incorporate temporal information. <xref rid="R15" ref-type="bibr">Gonzalez et al. (2015)</xref> propose a hidden semi-Markov model (HSMM) and variable duration semi-Markov model (VDHMM) to recognize AU dynamics. <xref rid="R28" ref-type="bibr">Koelstra et al. (2010)</xref> present a dynamic texture based approach that combines a discriminative, frame-based GentleBoost classifier with a dynamic, generative HMM model for temporal AU classification. <xref rid="R53" ref-type="bibr">Yang et al. (2009)</xref> extract temporal information of facial expressions using dynamic haar-like features and uses AdaBoost to select highly discriminating subset of these for AU recognition. <xref rid="R25" ref-type="bibr">Jeni et al. (2014)</xref> represent the spatio-temporal organization of expressions with time-series of shape and appearance descriptors and uses time-warping methods to classify different facial actions.</p><p id="P16">Recently, deep approaches have been proposed to model temporal information for AU detection. <xref rid="R6" ref-type="bibr">Chu et al. (2017)</xref> propose an architecture that combines convolutional neural network (CNN) and long short-term memory network (LSTM) for multilabel AU detection. In this architecture, CNN is used to learn spatial representations within frames while LSTM is used to model temporal dynamics among frames. Similarly, <xref rid="R21" ref-type="bibr">Jaiswal and Valstar (2016)</xref> use CNN to obtain spatial representations of facial parts cropped from the whole face using binary masks and used Bi-directional LSTM to learn the dynamics of facial parts for AU detection. <xref rid="R30" ref-type="bibr">Li et al. (2017)</xref> propose an adaptive region cropping based multi-label learning with deep recurrent net, which is based on combining region-based CNN (RCNN) with LSTM. Although a few deep approaches considering dynamics for AU detection have been proposed, many efforts have been devoted to incorporate dynamics in deep models for emotion recognition (<xref rid="R13" ref-type="bibr">Fan et al., 2016</xref>; <xref rid="R51" ref-type="bibr">Vielzeuf et al., 2017</xref>; <xref rid="R29" ref-type="bibr">Kollias and Zafeiriou, 2018</xref>; <xref rid="R32" ref-type="bibr">Liu et al., 2018</xref>; <xref rid="R35" ref-type="bibr">Lu et al., 2018</xref>). However, focusing on detecting action units is crucial since FACS is a comprehensive, anatomically-based system which describes all visually discernible facial movement and provides an objective measure.</p><p id="P17">As noted above, both shallow and deep AU detection approaches (e.g., SVM and 2D CNN) alike combine spatial and temporal information sequentially. Temporal representation is added only after spatial representation. In contrast, in human perception spatiotemporal processing is tightly integrated.</p><p id="P18">In a recent study, <xref rid="R52" ref-type="bibr">Yang et al. (2019)</xref> have proposed to model spatiotemporal information combining 2D-CNN with 3D-CNN for frame-level AU detection. However, whole video sequences are fed as input to 3D-CNN part to provide summary information about the entire video while modeling each frame. They do not consider modeling the local dynamics of segments, which is more informative to detect AUs.</p></sec><sec id="S4"><label>2.2.</label><title>Patch Learning</title><p id="P19">Traditional AU detection methods are based on (i) extracting appearance (<xref rid="R26" ref-type="bibr">Jiang et al., 2011</xref>; <xref rid="R11" ref-type="bibr">Eleftheriadis et al., 2015</xref>; <xref rid="R3" ref-type="bibr">Baltrusaitis et al., 2018</xref>) or geometric features (<xref rid="R36" ref-type="bibr">Lucey et al., 2007</xref>; <xref rid="R9" ref-type="bibr">Du et al., 2014</xref>) from the whole face and (ii) obtaining shallow representations as histograms of these features, thus ignoring the specificity of facial parts to AUs (<xref rid="R46" ref-type="bibr">Shojaeilangari et al., 2015</xref>). Deep approaches using whole face to train CNNs (<xref rid="R17" ref-type="bibr">Hammal et al., 2017</xref>; <xref rid="R39" ref-type="bibr">Onal Ertugrul et al., 2019a</xref>) also ignore the specificity of facial parts. More recent approaches focus on obtaining local representations using <italic>patch learning</italic>. Some of these approaches divide the face image into uniform grids (<xref rid="R34" ref-type="bibr">Liu et al., 2014</xref>; <xref rid="R60" ref-type="bibr">Zhong et al., 2015</xref>; <xref rid="R59" ref-type="bibr">Zhao et al., 2016b</xref>) while others define patches around facial parts (<xref rid="R8" ref-type="bibr">Corneanu et al., 2018</xref>) or facial landmarks (<xref rid="R57" ref-type="bibr">Zhao et al., 2016a</xref>). Among them, <xref rid="R34" ref-type="bibr">Liu et al. (2014)</xref> divide a face image into non-overlapping patches and categorize them into common and specific patches to describe different expressions. <xref rid="R60" ref-type="bibr">Zhong et al. (2015)</xref> identify active patches common to multiple expressions and specific to an individual expression using a multi-task sparse learning framework. <xref rid="R59" ref-type="bibr">Zhao et al. (2016b)</xref> use a regional connected convolutional layer that learns specific convolutional filters from sub-areas of the input. <xref rid="R8" ref-type="bibr">Corneanu et al. (2018)</xref> crop patches containing facial parts, train separate classifiers for each part and fuse the decisions of classifiers using structured learning. <xref rid="R57" ref-type="bibr">Zhao et al. (2016a)</xref> describe overlapping patches centered at facial landmarks, obtain shallow representations of patches and identify informative patches using a multi-label learning framework. These studies generally pre-process their frames to remove roll rotation. None of the aforementioned studies perform a 3D face registration to remove pitch and yaw rotation. Hence, patches cropped from different frames are likely to contain variable facial regions under pose. Only in a recent study, <xref rid="R40" ref-type="bibr">Onal Ertugrul et al. (2019b)</xref> cropped patches from 3D-registered faces for AU detection from static frames.</p></sec><sec id="S5"><label>2.3.</label><title>Regional Attention</title><p id="P20">As described in FACS (<xref rid="R10" ref-type="bibr">Ekman et al., 2002</xref>), AUs relate to specific regions of human faces. Motivated by this fact, recent studies aim to highlight information obtained from specific facial regions to detect specific AUs. <xref rid="R57" ref-type="bibr">Zhao et al. (2016a)</xref> employ patch regularization to eliminate the effect of non-informative shallow patch representations. <xref rid="R48" ref-type="bibr">Taheri et al. (2014)</xref> learn a dictionary per AU using local features extracted from predefined AU semantic regions on faces performing that AU. <xref rid="R21" ref-type="bibr">Jaiswal and Valstar (2016)</xref> use a pre-defined binary mask created to select a relevant region for a particular AU and pass it to a convolutional and bidirectional Long Short-Term Memory (LSTM) neural network. <xref rid="R31" ref-type="bibr">Li et al. (2018)</xref> design an attention map using the facial key points and AU centers to enforce their CNN-based architecture to focus more on these AU centers. <xref rid="R44" ref-type="bibr">Sanchez et al. (2018)</xref> generate heatmaps for a target AU, by estimating the facial landmarks and drawing a 2D Gaussian around the points where the AU is known to cause changes. They train Hourglass network to estimate AU intensity. <xref rid="R45" ref-type="bibr">Shao et al. (2018)</xref> employ an initial attention map, created based on AU centers and refine it to jointly perform AU detection and face alignment. These studies have mechanisms to enforce their models to focus on pre-defined regions. They do not have a learned attention mechanism, in which the network decides where to attend itself for each AU. In a recent work, <xref rid="R40" ref-type="bibr">Onal Ertugrul et al. (2019b)</xref> has proposed a mechanism which learns to attend to significant patches from their static encodings.</p></sec></sec><sec id="S6"><label>3.</label><title>METHODS</title><p id="P21"><xref rid="F1" ref-type="fig">Figure 1</xref> shows the components of the proposed dynamic patch-attentive network (D-PAttNet) architecture. First, we perform dense 3D registration from 2D videos (<xref rid="F1" ref-type="fig">Figure 1a</xref>). Then, we crop patches containing local facial parts. For each patch location, we use a separate 2D-CNN to encode local, static information and 3D-CNN to encode local, dynamic information. We concatenate static and dynamic encoding to obtain patch encoding (<xref rid="F1" ref-type="fig">Figure 1b</xref>). We employ a sigmoidal attention mechanism to weight the contribution of each patch to detect specific AUs (<xref rid="F1" ref-type="fig">Figure 1c</xref>). Finally, using the final face encoding, we detect 12 AUs (<xref rid="F1" ref-type="fig">Figure 1d</xref>). In the following, we describe in detail, the different components of the proposed D-PAttNet approach.</p><sec id="S7"><label>3.1.</label><title>3D Face Registration</title><p id="P22">We track and normalize videos using ZFace (<xref rid="R23" ref-type="bibr">Jeni et al., 2015</xref>, <xref rid="R24" ref-type="bibr">2017</xref>), a real-time face alignment software that accomplishes dense 3D registration from 2D videos and images without requiring person-specific training. ZFace performs a canonical 3D normalization that minimizes appearance changes from head movement and maximizes changes from expressions. First, it uses dense cascade-regression-based face alignment to estimate a dense set of 1,024 facial landmarks. Then a part-based 3D deformable model is applied to reconstruct a dense 3D mesh of the face. Face images are normalized in terms of pitch, yaw and roll rotation and scale and then centered. At the output of this step, video resolution is 512 &#x000d7; 512 with an interocular distance (IOD) of about 100 pixels.</p></sec><sec id="S8"><label>3.2.</label><title>Patch Cropping and Encoding</title><p id="P23">The 3D face registration step ensures that faces in all frames of all individuals are registered to the same template and that same landmarks (facial parts) in all frames are very close to each other. This step allows us to identify the locations of face parts and crop patches containing the same face parts for all frames.</p><p id="P24">Patch locations are identified using the domain knowledge of human FACS coders and based on the FACS manual (<xref rid="R10" ref-type="bibr">Ekman et al., 2002</xref>). We identify <italic>N</italic> = 9 patches given in <xref rid="F2" ref-type="fig">Figure 2</xref> with the aim to cover specific face parts that are deformed during the appearance of specific AUs, namely right eyebrow (<italic>P</italic><sub>1</sub>), left eyebrow (<italic>P</italic><sub>2</sub>), right eye (<italic>P</italic><sub>3</sub>), region between eyebrows and nose root (<italic>P</italic><sub>4</sub>), left eye (<italic>P</italic><sub>5</sub>), right cheek and lip corner (<italic>P</italic><sub>6</sub>), nose and upper mouth (<italic>P</italic><sub>7</sub>), left cheek and lip corner (<italic>P</italic><sub>8</sub>), and mouth and chin (<italic>P</italic><sub>9</sub>). Then, we crop <italic>N</italic> = 9 patches using the same identified locations from all frames in the dataset. The size of each RGB patch is 100 &#x000d7; 100 pixels.</p><sec id="S9"><label>3.2.1.</label><title>Static Patch Encoding</title><p id="P25">We use 2D-CNNs to encode static information. Input to each 2D-CNN is a single patch. We feed patches cropped from each of the nine locations to a different static encoder so that each encoder aims to learn representations of local face parts. Each of the nine static encoders has an identical architecture, which includes three convolutional layers and 1 fully connected layer. At the output of static encoders, we obtain <italic>M</italic>-dimensional vector representations of local patches.</p></sec><sec id="S10"><label>3.2.2.</label><title>Dynamic Patch Encoding</title><p id="P26">We use 3D-CNNs to encode dynamic information. We feed a patch sequence of length <italic>T</italic> as input to each 3D-CNN. Note that, each patch sequence contains the current patch fed to 2D-CNN and <italic>T</italic> &#x02212; 1 patches preceding the current patch. Similar to static encoders, we feed patch sequences cropped from each of the nine locations to a different dynamic encoder so that each encoder aims to learn dynamic representations of local face parts. 3D-CNNs have the same architectures as 2D-CNNs except 2D convolution layers are replaced by 3D convolution layers. At the output of dynamic encoders, we obtain <italic>M</italic>-dimensional vector representations of local patches.</p><p id="P27">After we obtain static and dynamic encoding of patches, we concatenate them and have a 2M-dimensional patch encoding.</p></sec></sec><sec id="S11"><label>3.3.</label><title>Patch Weighting by Sigmoidal Attention Mechanism</title><p id="P28">Different face patches contribute unequally to the face representation to predict AUs. In order to weight the contribution of patch encodings, we use an attention mechanism. An attention mechanism aggregates the representation of the informative patch encodings to form a face encoding. Let <italic>e</italic><sub><italic>p</italic></sub> be the encoding of patch <italic>p</italic> obtained by concatenating the outputs of 2D and 3D CNNs. First, patch encoding <italic>e</italic><sub><italic>p</italic></sub> is fed to a one-layer MLP to obtain hidden representation <italic>h</italic><sub><italic>p</italic></sub> of <italic>e</italic><sub><italic>p</italic></sub> as follows:
<disp-formula id="FD1"><label>(1)</label><mml:math display="block" id="M1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">tanh</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:msub><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>W</italic><sub><italic>f</italic></sub> and <italic>b</italic><sub><italic>f</italic></sub> are the weight and bias parameters of the MLP, respectively. Then, the importance of each patch is measured by the similarity between <italic>h</italic><sub><italic>p</italic></sub> and a patch level context vector <italic>c</italic><sub><italic>f</italic></sub>. In order to normalize the importance of patches to the range [0,1] and obtain attention weight <italic>&#x003b1;</italic><sub><italic>p</italic></sub>, we apply sigmoid function as follows:
<disp-formula id="FD2"><label>(2)</label><mml:math display="block" id="M2" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>c</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
If a patch representation is similar to context vector, their inner product will give a large value, and sigmoid output will be closer to 1. On the other hand, if a patch representation is very different from context vector, then their inner product will be close to zero, and the sigmoid output will also be close to zero (meaning that given patch is not important to detect the AU). Therefore, patch level context vector <italic>c</italic><sub><italic>f</italic></sub> can be interpreted as the high level representation of fixed query &#x0201c;What are the informative patches to predict a specific AU?&#x0201d; It is randomly initialized and learned during training. Finally, we obtain face encoding <italic>v</italic> as a weighted sum of patch encodings <italic>e</italic><sub><italic>p</italic></sub> as:
<disp-formula id="FD3"><label>(3)</label><mml:math display="block" id="M3" overflow="scroll"><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mstyle><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mi>p</mml:mi></mml:munder><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:msub><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>
Note that, it is typical to use softmax activation function for normalization in attention mechanisms employed in many NLP tasks. One such task is neural machine translation, where the network is trained to attend to one word (or a few words, but not to the others) to obtain the corresponding translation of the word. Output of softmax function can be used to represent a categorical distribution. In our case, we aim to allow multiple patches to contribute to predict a specific AU. Therefore, instead of softmax, we used sigmoid activation function which allows for multiple selection with a collection of Bernoulli random variables.</p></sec><sec id="S12"><label>3.4.</label><title>AU Detection</title><p id="P29">Face encoding <italic>v</italic> is a high level representation of the face that is used for AU detection. To <italic>v</italic> we apply ReLU for non-linearity and have a fully connected layer to predict the occurrence of AUs. We train individual networks for each AU. We apply sigmoid function and use weighted binary cross-entropy loss as follows:
<disp-formula id="FD4"><label>(4)</label><mml:math display="block" id="M4" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mtext mathvariant="italic">ylog</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext mathvariant="italic">pos</mml:mtext></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext mathvariant="italic">log</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>y</italic> denotes actual AU occurrence, &#x00177; denotes predicted AU occurrence. <italic>w</italic><sub><italic>pos</italic></sub> is the weight that is used for adjusting positive error relative to negative error.</p></sec></sec><sec id="S13"><label>4.</label><title>EXPERIMENTS</title><sec id="S14"><label>4.1.</label><title>Dataset</title><p id="P30">BP4D is a manually FACS annotated database of spontaneous behavior containing 2D and 3D videos of 41 subjects (23 female and 18 male). Following previous research in AU detection, only 2D videos are used here. In BP4D, well-designed tasks initiated by an experimenter are used to elicit varied spontaneous emotions. Each subject performs eight tasks. In total there are 328 videos of approximately 20 s each that have been FACS annotated manually. This results in about 140,000 valid, manually FACS annotated frames. We include 12 AUs that occurred in more than 5% of the frames. Positive samples are defined as ones with intensities equal to or higher than A-level, and the remaining ones are negative samples. We visualize the co-occurrence matrix of AUs computed using Jaccard index in <xref rid="F3" ref-type="fig">Figure 3</xref>. It can be observed that AU6, AU7, AU10, AU12, and AU14 co-occur frequently.</p></sec><sec id="S15"><label>4.2.</label><title>Network</title><p id="P31">In 2D-CNN, we employ 32, 64, and 64 filters of 5 &#x000d7; 5 pixels in three convolutional layers with a stride of 1. After convolution, rectified linear unit (ReLU) is applied to the output of the convolutional layers to add non-linearity to the model. We apply batch normalization to the outputs of all convolutional layers. The network contains three maxpooling layers that are applied after batch normalization. We apply max-pooling with a 2 &#x000d7; 2 window such that the output of max-pooling layer is downsampled with a factor of 2. At the output of the fully connected layer of static encoder, we obtain an encoding of size 1 &#x000d7; <italic>M</italic>, where <italic>M</italic> = 60.</p><p id="P32">In 3D-CNN, we select the patch sequence length <italic>T</italic> = 20. We employ 32, 64, and 64 filters of 5 &#x000d7; 5 &#x000d7; 5 pixels in the first two convolutional layers and 2 &#x000d7; 5 &#x000d7; 5 pixels in the final convolutional layer with a stride of 1. 3D convolutional layers are followed by ReLU and batch normalization layers. The first two batch normalization layers are followed by maxpooling layers with a 2 &#x000d7; 2 &#x000d7; 2 window, while the last batch normalization layer is followed by a maxpooling layer with a 1 &#x000d7; 2 &#x000d7; 2 window. At the output of the fully connected layer of dynamic encoder, we obtain an encoding of size 1 &#x000d7; <italic>M</italic>, where <italic>M</italic> = 60.</p><p id="P33">Temporal window length varies in the range [10, 24] in previous AU detection studies (<xref rid="R6" ref-type="bibr">Chu et al., 2017</xref>; <xref rid="R30" ref-type="bibr">Li et al., 2017</xref>). To be consistent with previous work, we selected patch sequences of length T = 20 within that range. The CNN architecture used in this study has been shown to be successful in previous studies (<xref rid="R7" ref-type="bibr">Cohn et al., 2018</xref>; <xref rid="R39" ref-type="bibr">Onal Ertugrul et al., 2019a</xref>,<xref rid="R41" ref-type="bibr">c</xref>). Two differences from previous work may be noted. One is the size of input images. Previously, we used holistic face images of size 200&#x000d7;200. Here we use local facial patches of size 100 &#x000d7; 100. The other difference results from the smaller input size. Because input size was reduced by 50%, we reduced the number of filters by 50% from 64, 128, and 128 filters to 32, 64, and 64 filters. The number of convolutional layers remained the same.</p><p id="P34">We obtain a patch encoding <italic>e</italic><sub><italic>p</italic></sub> of size 1 &#x000d7; 120, for each frame, which is obtained by concatenating 1&#x000d7;60 dimensional outputs of static and dynamic encoder outputs. In patch attention layer, we use the weight matrix <italic>W</italic><sub><italic>f</italic></sub> of size 120 &#x000d7; 120 and face level context vector <italic>c</italic><sub><italic>f</italic></sub> as 1 &#x000d7; 120. Attention layer output is a face encoding v of size 1 &#x000d7; 120, for each frame.</p></sec><sec id="S16"><label>4.3.</label><title>Training</title><p id="P35">We trained our architecture with mini-batches of 50 samples for 10 epochs. We used stochastic gradient descent (SGD) optimizer. Our models were initialized with learning rate of 1e-3, with a momentum of 0.9. In order to keep variability in the data, we used all of the available frames and did not subsample training frames to generate balanced dataset. For each AU, we assign <italic>w</italic><sub><italic>pos</italic></sub> to the ratio between the number of training frames excluding the AU and containing the AU. We perform a subject independent three-fold cross-validation for BP4D dataset. Our folds include the same subjects as in <xref rid="R57" ref-type="bibr">Zhao et al. (2016a)</xref>.</p></sec><sec id="S17"><label>4.4.</label><title>Evaluation Measures</title><p id="P36">We evaluate network performance on two metrics: F1-score and area under the receiver operator characteristics curve (AUC). F1-score is the harmonic mean of precision (P) and recall (R) <inline-formula><mml:math display="inline" id="M5" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mtext mathvariant="italic">RP</mml:mtext></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. It is widely used in the literature and therefore enables comparison with the many approaches that have used it to report their performance. Because F1-score is highly attenuated by imbalanced data (<xref rid="R22" ref-type="bibr">Jeni et al., 2013</xref>), however, results for less frequent AUs must be considered with caution. AUC has the advantage of being robust to imbalanced data but has been reported less frequently in the literature. It supports more limited comparisons with other approaches.</p></sec><sec id="S18"><label>4.5.</label><title>Threshold Tuning</title><p id="P37">For each AU, our model predicts a value between 0 and 1, denoting the probability that the specified AU is present in the frame. In order to binarize the output, we take threshold &#x003c4; = 0.5 and then evaluate the performance of D-PAttNet. Although during training we employed a weighted loss based on the baserates of AUs, it does not totally solve class imbalance problem. Optimal threshold &#x003c4; may be different for different AUs and may not be equal to 0.5. We optimized the threshold &#x003c4; &#x02208; [0.1, 0.9] on training set and evaluate the test performance in D-PAttNet<sup>tt</sup>.</p></sec></sec><sec id="S19"><label>5.</label><title>RESULTS</title><sec id="S20"><label>5.1.</label><title>Performance Comparison With the State-of-the-Art</title><p id="P38">We compare the performance of D-PAttNet with the following state-of-the-art approaches:
<list list-type="simple" id="L4"><list-item><p id="P39"><bold>Linear SVM (LSVM)</bold> is based on training an SVM classifier using the SIFT features obtained from the frames without considering patch learning.</p></list-item><list-item><p id="P40"><bold>Joint patch and multilabel learning (JPML)</bold> (<xref rid="R57" ref-type="bibr">Zhao et al., 2016a</xref>) simultaneously selects a discriminative set of patches and learn multi-AU classifiers. It uses SIFT features obtained from patches.</p></list-item><list-item><p id="P41"><bold>Deep region and multilabel learning (DRML)</bold> (<xref rid="R59" ref-type="bibr">Zhao et al., 2016b</xref>) combines region learning and multilabel learning for AU detection.</p></list-item><list-item><p id="P42"><bold>Network combining CNN and LSTM (LSTM)</bold> (<xref rid="R6" ref-type="bibr">Chu et al., 2017</xref>) employs CNN to model spatial information and LSTM to model temporal dynamics in a sequential way for multilabel AU detection.</p></list-item><list-item><p id="P43"><bold>Adversarial Training Framework (ATF)</bold> (<xref rid="R56" ref-type="bibr">Zhang et al., 2018</xref>) is a CNN-based framework in which AU loss is minimized and identity loss is maximized to learn subject invariant feature representations during the adversarial training.</p></list-item><list-item><p id="P44"><bold>Finetuned VGG Network (FVGG)</bold> (<xref rid="R31" ref-type="bibr">Li et al., 2018</xref>) is the model obtained after finetuning the pretrained VGG 19-layer model.</p></list-item><list-item><p id="P45"><bold>Network with enhancing layers (E-Net)</bold> (<xref rid="R31" ref-type="bibr">Li et al., 2018</xref>) is the finetuned VGG network with enhancing layer which forces the network to pay more attention to AU interest regions on face images.</p></list-item><list-item><p id="P46"><bold>Enhancing and Cropping Network (EAC Net)</bold> (<xref rid="R31" ref-type="bibr">Li et al., 2018</xref>) is a pretrained CNN model with enhancing (E-Net) and cropping (C-Net) layers. E-net forces the network to attend more to AU interest regions based on a predefined attention map while C-Net crops facial regions around detected landmarks and applies upscaling and convolutional layers in the cropped regions.</p></list-item><list-item><p id="P47"><bold>Deep Structured Inference Network (DSIN)</bold> (<xref rid="R8" ref-type="bibr">Corneanu et al., 2018</xref>) is a deep network which performs patch learning to learn local representations and structure inference to model AU correlations.</p></list-item><list-item><p id="P48"><bold>Joint AU detection and face alignment (JAA)</bold> (<xref rid="R45" ref-type="bibr">Shao et al., 2018</xref>) is a deep learning based joint AU detection and face alignment framework in which multi-scale shared features for the two tasks are learned firstly, and high-level features of face alignment are extracted and fed into AU detection.</p></list-item><list-item><p id="P151"><bold>Patch-attentive deep network (PAttNet)</bold> (<xref rid="R40" ref-type="bibr">Onal Ertugrul et al., 2019b</xref>) is a CNN-based approach which jointly learns local patch representations and weights them with a learned attention mechanism for AU detection.</p></list-item></list>
F1-score performances for the state-of-the-art approaches and D-PAttNet are given in <xref rid="T1" ref-type="table">Table 1</xref>. We also report results with Only3D-PAttNet, which includes only 3D CNN component of the D-PAttNet. Note that, for DSIN and D-PAttNet, superscript<sup><italic>tt</italic></sup> denotes the results after tuning the threshold. For fair comparison, we excluded the studies which do not follow three-fold protocol (<xref rid="R47" ref-type="bibr">T&#x00151;s&#x000e9;r et al., 2016</xref>).</p><p id="P50">Results reflect that, D-PAttNet and D-PAttNet<sup><italic>tt</italic></sup> give the best F1-score for 6 of 12 AUs (For D-PAttNet AU6, AU7, AU12, and AU23 and for D-PAttNet<sup><italic>tt</italic></sup> AU15 and AU24). For the remaining 6 AUs (AU1, AU2, AU4, AU10, AU14, and AU17), D-PAttNet<sup><italic>tt</italic></sup> gives the second best result. For four of the AUs (AU1, AU10, AU14, and AU17) for which D-PAttNet or D-PAttNet<sup><italic>tt</italic></sup> did not perform the best, DSIN<sup><italic>tt</italic></sup> show the best F1-score. On average, our method outperforms all of the comparison approaches and provides 2.1% absolute improvement over PAttNet.</p><p id="P51">Since F1-score is affected by the skew in the labels and some action units are highly skewed, we also compute AUC results, which are not affected by the skew. Only a few studies report AUC values. In <xref rid="T2" ref-type="table">Table 2</xref>, we compare the performance of D-PAttNet with the state of the art approaches using AUC. D-PAttNet gives an average AUC of 73.4% over all AUs. For each AU, AUC is above 64%. D-PAttNet gives superior performance compared to all of the approaches reporting AUC for 9 of the 12 AUs except for AU14, AU15, and AU24. For these three AUs, the maximum AUC is obtained for PAttNet.</p><p id="P52">Comparison of variants of PAttNet approach reflects that D-PAttNet which combines 2D CNN with 3D CNN outperforms PAttNet, which only has 2D CNN. Both variants give much better performance compared to using Only3D-PAttNet, which only has 3D CNN. D-PAttNet gives the best F1-scores for all AUs and the best AUC values for all but three AUs.</p><p id="P53">For the comparisons between D-PAttNet and other two variants (PAttNet and Only3D-PAttNet) we performed significance tests as given in <xref rid="T3" ref-type="table">Table 3</xref>. For each set of comparisons we controlled for Type I error using Bonferroni correction. With experiment-wise error of 0.05 and 12 comparisons in each set, a <italic>p</italic> of 0.004 is the critical value for significance. For AU7, AU10 and AU14 D-PAttNet significantly outperforms PAttNet when F1 scores are compared. When AUC values are compared, D-PAttNet performs significantly better for AU1, AU6, and AU7. Moreover, D-PAttNet outperforms Only3D-PAttNet for all AUs except for AU1 when F1 scores are compared. When AUC is used, it is significantly better for AU12, AU15, and AU24.</p></sec><sec id="S21"><label>5.2.</label><title>Performance Comparison of Using Sigmoid and Softmax Functions for Attention in Variants of Patch-Attentive Deep Networks</title><p id="P54">In this section, we compare the AU detection results of using our proposed attention function sigmoid and conventional activation function softmax to weight the contributions of patches. We compare these functions for (i) PAttNet approach which has 2D CNN to model static information, (ii) Only3D-PAttNet approach which has 3D CNN to model dynamic information, and (iii) D-PAttNet approach which combines static and dynamic information using 2D CNN and 3D CNN. We compare F1-scores and AUC values in <xref rid="T4" ref-type="table">Tables 4</xref>, <xref rid="T5" ref-type="table">5</xref>, respectively. We also performed significance tests for the comparisons between sigmoid &#x00026; softmax in given <xref rid="T6" ref-type="table">Table 6</xref>.</p><p id="P55">Comparison of the softmax and sigmoid rows of each approach in <xref rid="T4" ref-type="table">Table 4</xref> shows that using softmax instead of sigmoid for both PAttNet and D-PAttNet causes a drop in the F1-scores for all AUs. Decreases in F1 are significant for all AUs except for AU24. For Only3D-PAttNet, sigmoid function performs similarly to softmax. We observe similar results for AUC values in <xref rid="T5" ref-type="table">Table 5</xref>. Decreases in AUC are significant for four AUs namely, AU4, AU12, AU15, and AU17. When we force the network to attend one or a few patches, it cannot learn proper facial representation. These results are consistent with the assumption that even if AUs relate to specific facial regions, co-occurring nature of AUs causes the contribution of other facial regions to detect specific AUs. When softmax attention function is used, D-PAttNet leads to a 2.4% increase in the average F1-score (see <xref rid="T4" ref-type="table">Table 4</xref>), and a1.7% increase in the AUC (see <xref rid="T5" ref-type="table">Table 5</xref>). Similarly, using patch dynamics provides a 1.5% improvement in the average F1-score (see <xref rid="T4" ref-type="table">Table 4</xref>) and a 0.7% improvement in the average AUC (see <xref rid="T5" ref-type="table">Table 5</xref>).</p></sec><sec id="S22"><label>5.3.</label><title>Patch Attention Analysis</title><p id="P56">We visualize the attention maps formed using the learned attention weights of D-PAttNet with sigmoid attention, D-PAttNet with softmax attention, PAttNet with sigmoid attention, and PAttNet with softmax attention in <xref rid="F4" ref-type="fig">Figure 4</xref>. We obtain an attention map for each sample and then average these maps to obtain the presented attention maps. In all maps, entries can take values between [0,1]. Cells with black color denote that the corresponding patch has high attention weight (is significant) to detect the corresponding AU for all of these folds whereas cells with white color denote that the related patch is not significant to detect the corresponding AU in any of the folds. Multiple patches contribute with varying weights to detect AUs.</p><sec id="S23"><label>5.3.1.</label><title>Comparison of Sigmoid and Softmax Attention</title><p id="P57">We can compare the attention maps obtained using sigmoid (<xref rid="F4" ref-type="fig">Figures 4A</xref>,<xref rid="F4" ref-type="fig">C</xref>) and softmax (<xref rid="F4" ref-type="fig">Figures 4B</xref>,<xref rid="F4" ref-type="fig">D</xref>) attention. As expected, we obtain denser maps with sigmoid attention for both PAttNet and D-PAttNet since softmax tends to select sparse entries. Moreover, we observe larger number of black or dark gray entries in the attention maps obtained using sigmoid meaning that models learned for different folds agree on the significance of corresponding patches to detect related AUs. On the other hand, attention maps obtained using softmax attention do not have black entries and have a few dark gray entries. This indicates an inconsistency between the models trained for different folds, each of which learns to detect the same AU from different parts of the face.</p></sec><sec id="S24"><label>5.3.2.</label><title>Comparison of D-PAttNet and PAttNet</title><p id="P58">When we compare D-PAttNet with sigmoid (<xref rid="F4" ref-type="fig">Figure 4A</xref>) and PAttNet with sigmoid (<xref rid="F4" ref-type="fig">Figure 4C</xref>), we observe that for most of the AUs, the network learns to attend meaningful patches. In both maps, generally higher attention is observed in upper face patches to detect AUs of upper face region (AU1, AU2, and AU4). Similarly, higher attention is observed in mouth and lip corner patches to detect AUs of lower face region. In both maps, the highest attention is given to patches containing eyebrows (<italic>P</italic><sub>1</sub> for D-PAttNet and <italic>P</italic><sub>4</sub> for PAttNet) to detect AU1. AU12 is detected mainly from patches containing mouth and lip corner regions (<italic>P</italic><sub>7</sub>, <italic>P</italic><sub>8</sub>, and <italic>P</italic><sub>9</sub> for D-PAttNet and <italic>P</italic><sub>6</sub>, <italic>P</italic><sub>9</sub> for PAttNet).</p><p id="P59">AU6 (contraction of the orbicularis oculi) raises the cheeks, narrows the eye aperture, and in social contexts, such as BP4D, typically occurs together with AU12 (zygomatic major). AU12 stretches the lip corners obliquely. Because AU6 and AU12 frequently co-occur and lip-corner stretching often is a relatively prominent appearance change, it may not be surprising that PAttNet for AU6 (<xref rid="F4" ref-type="fig">Figure 4C</xref>) learns to attend more to patches containing lip corner, cheek, and mouth than to ones containing only the eyes. What is unexpected is that when patch dynamics are included for AU6 in PAttNet (<xref rid="F4" ref-type="fig">Figure 4A</xref>), eye features become more salient (<italic>P</italic><sub>1</sub>). The same effect may be seen with respect to AU7, which also is highly correlated with AU12 (<italic>P</italic><sub>6</sub> in <xref rid="F4" ref-type="fig">Figure 4A</xref> and <italic>P</italic><sub>8</sub> in <xref rid="F4" ref-type="fig">Figure 4C</xref>). The addition of dynamics in this way contributes to the detection of these AUs.</p><p id="P60">When we compare D-PAttNet with softmax (<xref rid="F4" ref-type="fig">Figure 4B</xref>) and PAttNet with softmax (<xref rid="F4" ref-type="fig">Figure 4D</xref>), we observe that forcing the classifier to attend sparse facial regions with softmax attention causes the network to attend irrelevant patches for some AUs in D-PAttNet. For example, to detect eye AUs, AU1 and AU2 the classifier does not attend to any of the eye patches. Recall that a black cell represents that the corresponding patch is significant to detect specific AUs for all or majority of the input frames. Neither maps for models with softmax attention contains black or dark cells. Contrary to the maps obtained with sigmoid atention, models with softmax attention do not attend to consistent patches to detect specific AUs for different images. Therefore, using softmax function for attention is not a good option for D-PAttNet and PAttNet.</p></sec></sec></sec><sec id="S25"><label>6.</label><title>DISCUSSION AND CONCLUSION</title><p id="P61">Inspired by the human perception of face and facial actions, we have proposed a dynamic patch-attentive deep network called D-PAttNet for AU detection. Analogous to OFA in human face perception, we encode local patches in an early stage of the network. Then, analogous to FFA, patch-based information is fused at a later stage by means of an attention mechanism. Analogous to STS, spatiotemporal dynamics are modeled by 3D-CNN.</p><p id="P62">In D-PAttNet, we first apply 3D face registration to remove the variation caused by the differences in pose and scale. Then, we crop patches containing important facial parts to detect specific AUs. We encode static patch information using 2D-CNN and patch dynamics using 3D-CNN and concatenate them to obtain patch encodings. After encoding each patch with CNN-based encoders, we weight the contribution of patch encodings using a patch attention mechanism. To allow multiple patches to contribute AU detection, we employ sigmoidal attention rather than the conventional softmax attention.</p><p id="P63">D-PAttNet outperforms state-of-the-art approaches on BP4D. Considering patch dynamics in D-PAttNet leads to an increase in the AU detection performance compared to its variants PAttNet and Only3D-PAttNet. Tuning the decision threshold of classifier further improves the detection performance. While D-PAttNet and PAttNet results are closer to each other, Only3D-PAttNet results are much worse than these two. Both PAttNet and D-PAttNet include a 2D CNN component. Current frame whose AUs are being detected is explicitly fed to these models through the 2D CNN component. However, in Only3D-PAttNet, 2D-CNN component does not exist. A sequence of frames is given as input to the 3D-CNN component but the task is to predict the AU occurrences of the last frame. Therefore, it may be more difficult for Only3D-PAttNet model to figure out the problem compared to the other variants.</p><p id="P64">Visualizing attention maps provides interpretation of the significant facial regions to detect AUs. Attention maps show that, with the help of sigmoidal attention D-PAttNet chooses to attend multiple patches and the most significant patches are meaningful. Softmax attention map is much sparser and leads to lower AU detection performance. While the facial regions attended in both D-PAttNet and PAttNet are similar, D-PAttNet is more successful to capture subtle appearance changes from the dynamics.</p><p id="P65">A limitation of our work is that we only tested our approach on a single database, BP4D, in which non-frontal variation in head pose is relatively limited. The 3D registration in D-PAttNet may be especially effective in databases that have larger non-frontal variation in head pose. More generally, generalizability of models and decision thresholds across databases or domains are open research questions. Decreases in classifier performance are common in cross-domain settings (<xref rid="R39" ref-type="bibr">Onal Ertugrul et al., 2019a</xref>) even when models are trained on large databases. Future work should explore cross-domain generalizability of models and thresholds in large databases that vary in pose characteristics. Another future direction would be modeling spatiotemporal patch dynamics for AU intensity estimation.</p></sec></body><back><ack id="S26"><title>FUNDING</title><p id="P66">This research was supported in part by NIH awards NS100549 and MH096951 and NSF award CNS 1629716.</p></ack><fn-group><fn id="FN3"><p id="P67"><bold>Specialty section:</bold> This article was submitted to Human-Media Interaction, a section of the journal Frontiers in Computer Science</p></fn><fn id="FN4"><p id="P68">DATA AVAILABILITY STATEMENT</p><p id="P69">BP4D dataset used for this study is available on request in <ext-link ext-link-type="uri" xlink:href="http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html">http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html</ext-link>.</p></fn><fn id="FN5"><p id="P70">ETHICS STATEMENT</p><p id="P71">The studies involving human participants were reviewed and approved by University of Pittsburgh IRB number IRB961170. Automated Facial Expression Analysis for Research and Clinical Use. The patients/participants provided their written informed consent to participate in this study. Written informed consent was obtained from the individual(s) for the publication of any potentially identifiable images or data included in this article.</p></fn><fn fn-type="COI-statement" id="FN6"><p id="P72"><bold>Conflict of Interest:</bold> The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></fn></fn-group><ref-list><title>REFERENCES</title><ref id="R1"><mixed-citation publication-type="journal"><name><surname>Ambadar</surname><given-names>Z</given-names></name>, <name><surname>Schooler</surname><given-names>JW</given-names></name>, and <name><surname>Cohn</surname><given-names>JF</given-names></name> (<year>2005</year>). <article-title>Deciphering the enigmatic face: the importance of facial dynamics in interpreting subtle facial expressions</article-title>. <source>Psychol. Sci</source>
<volume>16</volume>, <fpage>403</fpage>&#x02013;<lpage>410</lpage>. doi: <pub-id pub-id-type="doi">10.1111/j.0956-7976.2005.01548.x</pub-id><pub-id pub-id-type="pmid">15869701</pub-id></mixed-citation></ref><ref id="R2"><mixed-citation publication-type="journal"><name><surname>Arcurio</surname><given-names>LR</given-names></name>, <name><surname>Gold</surname><given-names>JM</given-names></name>, and <name><surname>James</surname><given-names>TW</given-names></name> (<year>2012</year>). <article-title>The response of face-selective cortex with single face parts and part combinations</article-title>. <source>Neuropsychologia</source>
<volume>50</volume>, <fpage>2454</fpage>&#x02013;<lpage>2459</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.06.016</pub-id><pub-id pub-id-type="pmid">22750118</pub-id></mixed-citation></ref><ref id="R3"><mixed-citation publication-type="book"><name><surname>Baltrusaitis</surname><given-names>T</given-names></name>, <name><surname>Zadeh</surname><given-names>A</given-names></name>, <name><surname>Lim</surname><given-names>YC</given-names></name>, and <name><surname>Morency</surname><given-names>L-P</given-names></name> (<year>2018</year>). &#x0201c;<chapter-title>Openface2.0: facial behavior analysis toolkit</chapter-title>,&#x0201d; in <source>FG</source> (<publisher-loc>Xian</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>59</fpage>&#x02013;<lpage>66</lpage>. doi: <pub-id pub-id-type="doi">10.1109/FG.2018.00019</pub-id></mixed-citation></ref><ref id="R4"><mixed-citation publication-type="journal"><name><surname>Bould</surname><given-names>E</given-names></name>, <name><surname>Morris</surname><given-names>N</given-names></name>, and <name><surname>Wink</surname><given-names>B</given-names></name> (<year>2008</year>). <article-title>Recognising subtle emotional expressions: the role of facial movements</article-title>. <source>Cogn. Emot</source>
<volume>22</volume>, <fpage>1569</fpage>&#x02013;<lpage>1587</lpage>. doi: <pub-id pub-id-type="doi">10.1080/02699930801921156</pub-id></mixed-citation></ref><ref id="R5"><mixed-citation publication-type="book"><name><surname>Chu</surname><given-names>W-S</given-names></name>, <name><surname>De la Torre</surname><given-names>F</given-names></name>, and <name><surname>Cohn</surname><given-names>JF</given-names></name> (<year>2013</year>). &#x0201c;<chapter-title>Selective transfer machine for personalized facial action unit detection</chapter-title>,&#x0201d; in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Portland, OR</publisher-loc>), <fpage>3515</fpage>&#x02013;<lpage>3522</lpage>. doi: <pub-id pub-id-type="doi">10.1109/CVPR.2013.451</pub-id></mixed-citation></ref><ref id="R6"><mixed-citation publication-type="book"><name><surname>Chu</surname><given-names>W-S</given-names></name>, <name><surname>De la Torre</surname><given-names>F</given-names></name>, and <name><surname>Cohn</surname><given-names>JF</given-names></name> (<year>2017</year>). &#x0201c;<chapter-title>Learning spatial and temporal cues for multi-label facial action unit detection</chapter-title>,&#x0201d; in <source>FG</source> (<publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>25</fpage>&#x02013;<lpage>32</lpage>. doi: <pub-id pub-id-type="doi">10.1109/FG.2017.13</pub-id></mixed-citation></ref><ref id="R7"><mixed-citation publication-type="book"><name><surname>Cohn</surname><given-names>JF</given-names></name>, <name><surname>Jeni</surname><given-names>LA</given-names></name>, <name><surname>Onal Ertugrul</surname><given-names>I</given-names></name>, <name><surname>Malone</surname><given-names>D</given-names></name>, <name><surname>Okun</surname><given-names>MS</given-names></name>, <name><surname>Borton</surname><given-names>D</given-names></name>, <etal/> (<year>2018</year>). &#x0201c;<chapter-title>Automated affect detection in deep brain stimulation for obsessive-compulsive disorder: a pilot study</chapter-title>,&#x0201d; in <source>ICMI</source> (<publisher-loc>Boulder, CO</publisher-loc>: <publisher-name>ACM</publisher-name>). doi: <pub-id pub-id-type="doi">10.1145/3242969.3243023</pub-id></mixed-citation></ref><ref id="R8"><mixed-citation publication-type="book"><name><surname>Corneanu</surname><given-names>C</given-names></name>, <name><surname>Madadi</surname><given-names>M</given-names></name>, and <name><surname>Escalera</surname><given-names>S</given-names></name> (<year>2018</year>). &#x0201c;<chapter-title>Deep structure inference network for facial action unit recognition</chapter-title>,&#x0201d; in <source>Proceedings of the European Conference on Computer Vision (ECCV)</source> (<publisher-loc>Munich</publisher-loc>), <fpage>298</fpage>&#x02013;<lpage>313</lpage>. doi: <pub-id pub-id-type="doi">10.1007/978-3-030-01258-8_19</pub-id></mixed-citation></ref><ref id="R9"><mixed-citation publication-type="journal"><name><surname>Du</surname><given-names>S</given-names></name>, <name><surname>Tao</surname><given-names>Y</given-names></name>, and <name><surname>Martinez</surname><given-names>AM</given-names></name> (<year>2014</year>). <article-title>Compound facial expressions of emotion</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>111</volume>, <fpage>E1454</fpage>&#x02013;<lpage>E1462</lpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1322355111</pub-id><pub-id pub-id-type="pmid">24706770</pub-id></mixed-citation></ref><ref id="R10"><mixed-citation publication-type="book"><name><surname>Ekman</surname><given-names>P</given-names></name>, <name><surname>Friesen</surname><given-names>W</given-names></name>, and <name><surname>Hager</surname><given-names>J</given-names></name> (<year>2002</year>). <source>Facial Action Coding System: Research Nexus Network Research Information</source>. <publisher-loc>Salt Lake City, UT</publisher-loc>: <publisher-name>Research Nexus</publisher-name>.</mixed-citation></ref><ref id="R11"><mixed-citation publication-type="book"><name><surname>Eleftheriadis</surname><given-names>S</given-names></name>, <name><surname>Rudovic</surname><given-names>O</given-names></name>, and <name><surname>Pantic</surname><given-names>M</given-names></name> (<year>2015</year>). &#x0201c;<chapter-title>Multi-conditional latent variable model for joint facial action unit detection</chapter-title>,&#x0201d; in <source>Proceedings of the IEEE International Conference on Computer Vision</source> (<publisher-loc>Santiago</publisher-loc>), <fpage>3792</fpage>&#x02013;<lpage>3800</lpage>. doi: <pub-id pub-id-type="doi">10.1109/ICCV.2015.432</pub-id></mixed-citation></ref><ref id="R12"><mixed-citation publication-type="journal"><name><surname>Fairhall</surname><given-names>SL</given-names></name>, and <name><surname>Ishai</surname><given-names>A</given-names></name> (<year>2006</year>). <article-title>Effective connectivity within the distributed cortical network for face perception</article-title>. <source>Cereb. Cortex</source>
<volume>17</volume>, <fpage>2400</fpage>&#x02013;<lpage>2406</lpage>. doi: <pub-id pub-id-type="doi">10.1093/cercor/bhl148</pub-id><pub-id pub-id-type="pmid">17190969</pub-id></mixed-citation></ref><ref id="R13"><mixed-citation publication-type="book"><name><surname>Fan</surname><given-names>Y</given-names></name>, <name><surname>Lu</surname><given-names>X</given-names></name>, <name><surname>Li</surname><given-names>D</given-names></name>, and <name><surname>Liu</surname><given-names>Y</given-names></name> (<year>2016</year>). &#x0201c;<chapter-title>Video-based emotion recognition using CNN-RNN and C3D hybrid networks</chapter-title>,&#x0201d; in <source>Proceedings of the 18th ACM International Conference on Multimodal Interaction</source> (<publisher-loc>Tokyo</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>445</fpage>&#x02013;<lpage>450</lpage>. doi: <pub-id pub-id-type="doi">10.1145/2993148.2997632</pub-id></mixed-citation></ref><ref id="R14"><mixed-citation publication-type="journal"><name><surname>George</surname><given-names>N</given-names></name>, <name><surname>Dolan</surname><given-names>RJ</given-names></name>, <name><surname>Fink</surname><given-names>GR</given-names></name>, <name><surname>Baylis</surname><given-names>GC</given-names></name>, <name><surname>Russell</surname><given-names>C</given-names></name>, and <name><surname>Driver</surname><given-names>J</given-names></name> (<year>1999</year>). <article-title>Contrast polarity and face recognition in the human fusiform gyrus</article-title>. <source>Nat. Neurosci</source>
<volume>2</volume>, <fpage>574</fpage>&#x02013;<lpage>580</lpage>. doi: <pub-id pub-id-type="doi">10.1038/9230</pub-id><pub-id pub-id-type="pmid">10448224</pub-id></mixed-citation></ref><ref id="R15"><mixed-citation publication-type="journal"><name><surname>Gonzalez</surname><given-names>I</given-names></name>, <name><surname>Cartella</surname><given-names>F</given-names></name>, <name><surname>Enescu</surname><given-names>V</given-names></name>, and <name><surname>Sahli</surname><given-names>H</given-names></name> (<year>2015</year>). <article-title>Recognition of facial actions and their temporal segments based on duration models</article-title>. <source>Multimedia Tools Appl</source>. <volume>74</volume>, <fpage>10001</fpage>&#x02013;<lpage>10024</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s11042-014-2320-8</pub-id></mixed-citation></ref><ref id="R16"><mixed-citation publication-type="journal"><name><surname>Grill-Spector</surname><given-names>K</given-names></name>, and <name><surname>Malach</surname><given-names>R</given-names></name> (<year>2004</year>). <article-title>The human visual cortex</article-title>. <source>Annu. Rev. Neurosci</source>
<volume>27</volume>, <fpage>649</fpage>&#x02013;<lpage>677</lpage>. doi: <pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144220</pub-id><pub-id pub-id-type="pmid">15217346</pub-id></mixed-citation></ref><ref id="R17"><mixed-citation publication-type="book"><name><surname>Hammal</surname><given-names>Z</given-names></name>, <name><surname>Chu</surname><given-names>W-S</given-names></name>, <name><surname>Cohn</surname><given-names>JF</given-names></name>, <name><surname>Heike</surname><given-names>C</given-names></name>, and <name><surname>Speltz</surname><given-names>ML</given-names></name> (<year>2017</year>). &#x0201c;<chapter-title>Automatic action unit detection in infants using convolutional neural network</chapter-title>,&#x0201d; in <source>2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)</source> (<publisher-loc>Xian</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>216</fpage>&#x02013;<lpage>221</lpage>. doi: <pub-id pub-id-type="doi">10.1109/ACII.2017.8273603</pub-id></mixed-citation></ref><ref id="R18"><mixed-citation publication-type="journal"><name><surname>Haxby</surname><given-names>JV</given-names></name>, <name><surname>Hoffman</surname><given-names>EA</given-names></name>, and <name><surname>Gobbini</surname><given-names>MI</given-names></name> (<year>2000</year>). <article-title>The distributed human neural system for face perception</article-title>. <source>Trends Cogn. Sci</source>
<volume>4</volume>, <fpage>223</fpage>&#x02013;<lpage>233</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S1364-6613(00)01482-0</pub-id><pub-id pub-id-type="pmid">10827445</pub-id></mixed-citation></ref><ref id="R19"><mixed-citation publication-type="journal"><name><surname>Hoffman</surname><given-names>EA</given-names></name>, and <name><surname>Haxby</surname><given-names>JV</given-names></name> (<year>2000</year>). <article-title>Distinct representations of eye gaze and identity in the distributed human neural system for face perception</article-title>. <source>Nat. Neurosci</source>
<volume>3</volume>, <fpage>80</fpage>&#x02013;<lpage>84</lpage>. doi: <pub-id pub-id-type="doi">10.1038/71152</pub-id><pub-id pub-id-type="pmid">10607399</pub-id></mixed-citation></ref><ref id="R20"><mixed-citation publication-type="journal"><name><surname>Horstmann</surname><given-names>G</given-names></name>, and <name><surname>Ansorge</surname><given-names>U</given-names></name> (<year>2009</year>). <article-title>Visual search for facial expressions of emotions: a comparison of dynamic and static faces</article-title>. <source>Emotion</source>
<volume>9</volume>, <fpage>29</fpage>&#x02013;<lpage>38</lpage>. doi: <pub-id pub-id-type="doi">10.1037/a0014147</pub-id><pub-id pub-id-type="pmid">19186914</pub-id></mixed-citation></ref><ref id="R21"><mixed-citation publication-type="book"><name><surname>Jaiswal</surname><given-names>S</given-names></name>, and <name><surname>Valstar</surname><given-names>M</given-names></name> (<year>2016</year>). &#x0201c;<chapter-title>Deep learning the dynamic appearance and shape of facial action units</chapter-title>,&#x0201d; in <source>2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</source> (<publisher-loc>Lake Placid, NY</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1</fpage>&#x02013;<lpage>8</lpage>. doi: <pub-id pub-id-type="doi">10.1109/WACV.2016.7477625</pub-id></mixed-citation></ref><ref id="R22"><mixed-citation publication-type="book"><name><surname>Jeni</surname><given-names>LA</given-names></name>, <name><surname>Cohn</surname><given-names>JF</given-names></name>, and <name><surname>De La Torre</surname><given-names>F</given-names></name> (<year>2013</year>). &#x0201c;<chapter-title>Facing imbalanced data&#x02013;recommendations for the use of performance metrics</chapter-title>,&#x0201d; in <source>ACII</source> (<publisher-loc>Geneva</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>245</fpage>&#x02013;<lpage>251</lpage>. doi: <pub-id pub-id-type="doi">10.1109/ACII.2013.47</pub-id></mixed-citation></ref><ref id="R23"><mixed-citation publication-type="book"><name><surname>Jeni</surname><given-names>LA</given-names></name>, <name><surname>Cohn</surname><given-names>JF</given-names></name>, and <name><surname>Kanade</surname><given-names>T</given-names></name> (<year>2015</year>). &#x0201c;<chapter-title>Dense 3D face alignment from 2D videos in real-time</chapter-title>,&#x0201d; in <source>2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</source>, Vol. <volume>1</volume> (<publisher-loc>Ljubljana</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1</fpage>&#x02013;<lpage>8</lpage>. doi: <pub-id pub-id-type="doi">10.1109/FG.2015.7163142</pub-id></mixed-citation></ref><ref id="R24"><mixed-citation publication-type="journal"><name><surname>Jeni</surname><given-names>LA</given-names></name>, <name><surname>Cohn</surname><given-names>JF</given-names></name>, and <name><surname>Kanade</surname><given-names>T</given-names></name> (<year>2017</year>). <article-title>Dense 3D face alignment from 2D video for real-time use</article-title>. <source>Image Vis. Comput</source>
<volume>58</volume>, <fpage>13</fpage>&#x02013;<lpage>24</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.imavis.2016.05.009</pub-id><pub-id pub-id-type="pmid">29731533</pub-id></mixed-citation></ref><ref id="R25"><mixed-citation publication-type="book"><name><surname>Jeni</surname><given-names>LA</given-names></name>, <name><surname>L&#x00151;rincz</surname><given-names>A</given-names></name>, <name><surname>Szab&#x000f3;</surname><given-names>Z</given-names></name>, <name><surname>Cohn</surname><given-names>JF</given-names></name>, and <name><surname>Kanade</surname><given-names>T</given-names></name> (<year>2014</year>). &#x0201c;<chapter-title>Spatiotemporal event classification using time-series kernel based structured sparsity</chapter-title>,&#x0201d; in <source>European Conference on Computer Vision</source> (<publisher-loc>Zurich</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>135</fpage>&#x02013;<lpage>150</lpage>. doi: <pub-id pub-id-type="doi">10.1007/978-3-319-10593-2_10</pub-id></mixed-citation></ref><ref id="R26"><mixed-citation publication-type="book"><name><surname>Jiang</surname><given-names>B</given-names></name>, <name><surname>Valstar</surname><given-names>MF</given-names></name>, and <name><surname>Pantic</surname><given-names>M</given-names></name> (<year>2011</year>). &#x0201c;<chapter-title>Action unit detection using sparse appearance descriptors in space-time video volumes</chapter-title>,&#x0201d; in <source>Face and Gesture 2011</source> (<publisher-loc>Santa Barbara, CA</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>314</fpage>&#x02013;<lpage>321</lpage>. doi: <pub-id pub-id-type="doi">10.1109/FG.2011.5771416</pub-id></mixed-citation></ref><ref id="R27"><mixed-citation publication-type="journal"><name><surname>K&#x000e4;tsyri</surname><given-names>J</given-names></name>, and <name><surname>Sams</surname><given-names>M</given-names></name> (<year>2008</year>). <article-title>The effect of dynamics on identifying basic emotions from synthetic and natural faces</article-title>. <source>Int. J. Hum. Comput. Stud</source>
<volume>66</volume>, <fpage>233</fpage>&#x02013;<lpage>242</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.ijhcs.2007.10.001</pub-id></mixed-citation></ref><ref id="R28"><mixed-citation publication-type="journal"><name><surname>Koelstra</surname><given-names>S</given-names></name>, <name><surname>Pantic</surname><given-names>M</given-names></name>, and <name><surname>Patras</surname><given-names>I</given-names></name> (<year>2010</year>). <article-title>A dynamic texture-based approach to recognition of facial actions and their temporal models</article-title>. <source>IEEE TPAMI</source>
<volume>32</volume>, <fpage>1940</fpage>&#x02013;<lpage>1954</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TPAMI.2010.50</pub-id></mixed-citation></ref><ref id="R29"><mixed-citation publication-type="book"><name><surname>Kollias</surname><given-names>D</given-names></name>, and <name><surname>Zafeiriou</surname><given-names>S</given-names></name> (<year>2018</year>). &#x0201c;<chapter-title>Training deep neural networks with different datasets in-the-wild: the emotion recognition paradigm</chapter-title>,&#x0201d; in <source>2018 International Joint Conference on Neural Networks (IJCNN)</source> (<publisher-loc>Rio de Janeiro</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1</fpage>&#x02013;<lpage>8</lpage>. doi: <pub-id pub-id-type="doi">10.1109/IJCNN.2018.8489340</pub-id></mixed-citation></ref><ref id="R30"><mixed-citation publication-type="book"><name><surname>Li</surname><given-names>W</given-names></name>, <name><surname>Abtahi</surname><given-names>F</given-names></name>, and <name><surname>Zhu</surname><given-names>Z</given-names></name> (<year>2017</year>). &#x0201c;<chapter-title>Action unit detection with region adaptation, multi-labeling learning and optimal temporal fusing</chapter-title>,&#x0201d; in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Honolulu, HI</publisher-loc>), <fpage>1841</fpage>&#x02013;<lpage>1850</lpage>. doi: <pub-id pub-id-type="doi">10.1109/CVPR.2017.716</pub-id></mixed-citation></ref><ref id="R31"><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>W</given-names></name>, <name><surname>Abtahi</surname><given-names>F</given-names></name>, <name><surname>Zhu</surname><given-names>Z</given-names></name>, and <name><surname>Yin</surname><given-names>L</given-names></name> (<year>2018</year>). <article-title>EAC-net: Deep nets with enhancing and cropping for facial action unit detection</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>
<volume>40</volume>, <fpage>2583</fpage>&#x02013;<lpage>2596</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TPAMI.2018.2791608</pub-id><pub-id pub-id-type="pmid">29994168</pub-id></mixed-citation></ref><ref id="R32"><mixed-citation publication-type="book"><name><surname>Liu</surname><given-names>C</given-names></name>, <name><surname>Tang</surname><given-names>T</given-names></name>, <name><surname>Lv</surname><given-names>K</given-names></name>, and <name><surname>Wang</surname><given-names>M</given-names></name> (<year>2018</year>). &#x0201c;<chapter-title>Multi-feature based emotion recognition for video clips</chapter-title>,&#x0201d; in <source>Proceedings of the 2018 on International Conference on Multimodal Interaction</source> (<publisher-loc>Boulder, CO</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>630</fpage>&#x02013;<lpage>634</lpage>. doi: <pub-id pub-id-type="doi">10.1145/3242969.3264989</pub-id></mixed-citation></ref><ref id="R33"><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>J</given-names></name>, <name><surname>Harris</surname><given-names>A</given-names></name>, and <name><surname>Kanwisher</surname><given-names>N</given-names></name> (<year>2010</year>). <article-title>Perception of face parts and face configurations: an FMRI study</article-title>. <source>J. Cogn. Neurosci</source>
<volume>22</volume>, <fpage>203</fpage>&#x02013;<lpage>211</lpage>. doi: <pub-id pub-id-type="doi">10.1162/jocn.2009.21203</pub-id><pub-id pub-id-type="pmid">19302006</pub-id></mixed-citation></ref><ref id="R34"><mixed-citation publication-type="book"><name><surname>Liu</surname><given-names>P</given-names></name>, <name><surname>Zhou</surname><given-names>JT</given-names></name>, <name><surname>Tsang</surname><given-names>IW-H</given-names></name>, <name><surname>Meng</surname><given-names>Z</given-names></name>, <name><surname>Han</surname><given-names>S</given-names></name>, and <name><surname>Tong</surname><given-names>Y</given-names></name> (<year>2014</year>). &#x0201c;<chapter-title>Feature disentangling machine-a novel approach of feature selection and disentangling in facial expression analysis</chapter-title>,&#x0201d; in <source>European Conference on Computer Vision</source> (<publisher-loc>Zurich</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>151</fpage>&#x02013;<lpage>166</lpage>. doi: <pub-id pub-id-type="doi">10.1007/978-3-319-10593-2_11</pub-id></mixed-citation></ref><ref id="R35"><mixed-citation publication-type="book"><name><surname>Lu</surname><given-names>C</given-names></name>, <name><surname>Zheng</surname><given-names>W</given-names></name>, <name><surname>Li</surname><given-names>C</given-names></name>, <name><surname>Tang</surname><given-names>C</given-names></name>, <name><surname>Liu</surname><given-names>S</given-names></name>, <name><surname>Yan</surname><given-names>S</given-names></name>, and <name><surname>Zong</surname><given-names>Y</given-names></name> (<year>2018</year>). &#x0201c;<chapter-title>Multiple spatio-temporal feature learning for video-based emotion recognition in the wild</chapter-title>,&#x0201d; in <source>Proceedings of the 2018 on International Conference on Multimodal Interaction</source> (<publisher-loc>Boulder, CO</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>646</fpage>&#x02013;<lpage>652</lpage>. doi: <pub-id pub-id-type="doi">10.1145/3242969.3264992</pub-id></mixed-citation></ref><ref id="R36"><mixed-citation publication-type="journal"><name><surname>Lucey</surname><given-names>S</given-names></name>, <name><surname>Ashraf</surname><given-names>AB</given-names></name>, and <name><surname>Cohn</surname><given-names>JF</given-names></name> (<year>2007</year>). &#x0201c;<article-title>Investigating spontaneous facial action recognition through aam representations of the face</article-title>,&#x0201d; in <source>Face Recognition (IntechOpen)</source>.</mixed-citation></ref><ref id="R37"><mixed-citation publication-type="book"><name><surname>Luong</surname><given-names>T</given-names></name>, <name><surname>Pham</surname><given-names>H</given-names></name>, and <name><surname>Manning</surname><given-names>CD</given-names></name> (<year>2015</year>). &#x0201c;<chapter-title>Effective approaches to attention-based neural machine translation</chapter-title>,&#x0201d; in <source>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</source> (<publisher-loc>Lisbon</publisher-loc>), <fpage>1412</fpage>&#x02013;<lpage>1421</lpage>. doi: <pub-id pub-id-type="doi">10.18653/v1/D15-1166</pub-id></mixed-citation></ref><ref id="R38"><mixed-citation publication-type="journal"><name><surname>Nichols</surname><given-names>DF</given-names></name>, <name><surname>Betts</surname><given-names>LR</given-names></name>, and <name><surname>Wilson</surname><given-names>HR</given-names></name> (<year>2010</year>). <article-title>Decoding of faces and face components in face-sensitive human visual cortex</article-title>. <source>Front. Psychol</source>
<volume>1</volume>:<fpage>28</fpage>. doi: <pub-id pub-id-type="doi">10.3389/fpsyg.2010.00028</pub-id><pub-id pub-id-type="pmid">21833198</pub-id></mixed-citation></ref><ref id="R39"><mixed-citation publication-type="book"><name><surname>Onal Ertugrul</surname><given-names>I</given-names></name>, <name><surname>Cohn</surname><given-names>JF</given-names></name>, <name><surname>Jeni</surname><given-names>LA</given-names></name>, <name><surname>Zhang</surname><given-names>Z</given-names></name>, <name><surname>Yin</surname><given-names>L</given-names></name>, and <name><surname>Ji</surname><given-names>Q</given-names></name> (<year>2019a</year>). &#x0201c;<chapter-title>Cross-domain AU detection: domains, learning approaches, and measures</chapter-title>,&#x0201d; in <source>2019 14th IEEE International Conference on Automatic Face &#x00026; Gesture Recognition</source> (<publisher-loc>Lille</publisher-loc>). doi: <pub-id pub-id-type="doi">10.1109/FG.2019.8756543</pub-id></mixed-citation></ref><ref id="R40"><mixed-citation publication-type="book"><name><surname>Onal Ertugrul</surname><given-names>I</given-names></name>, <name><surname>Jeni</surname><given-names>LA</given-names></name>, and <name><surname>Cohn</surname><given-names>JF</given-names></name> (<year>2019b</year>). &#x0201c;<chapter-title>Pattnet: Patch-attentive deep network for action unit detection</chapter-title>,&#x0201d; in <source>Proceedings of the British Machine Vision Conference (BMVC)</source> (<publisher-loc>Cardiff</publisher-loc>).</mixed-citation></ref><ref id="R41"><mixed-citation publication-type="book"><name><surname>Onal Ertugrul</surname><given-names>I</given-names></name>, <name><surname>Jeni</surname><given-names>LA</given-names></name>, <name><surname>Ding</surname><given-names>W</given-names></name>, and <name><surname>Cohn</surname><given-names>JF</given-names></name> (<year>2019c</year>). &#x0201c;<chapter-title>Afar: a deep learning based tool for automated facial affect recognition</chapter-title>,&#x0201d; in <source>2019 14th IEEE International Conference on Automatic Face &#x00026; Gesture Recognition</source> (<publisher-loc>Lille</publisher-loc>).</mixed-citation></ref><ref id="R42"><mixed-citation publication-type="journal"><name><surname>Pitcher</surname><given-names>D</given-names></name>, <name><surname>Walsh</surname><given-names>V</given-names></name>, and <name><surname>Duchaine</surname><given-names>B</given-names></name> (<year>2011</year>). <article-title>The role of the occipital face area in the cortical face perception network</article-title>. <source>Exp. Brain Res</source>
<volume>209</volume>, <fpage>481</fpage>&#x02013;<lpage>493</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s00221-011-2579-1</pub-id><pub-id pub-id-type="pmid">21318346</pub-id></mixed-citation></ref><ref id="R43"><mixed-citation publication-type="book"><name><surname>Rodr&#x000ed;guez</surname><given-names>P</given-names></name>, <name><surname>Gonfaus</surname><given-names>JM</given-names></name>, <name><surname>Cucurull</surname><given-names>G</given-names></name>, <name><surname>XavierRoca</surname><given-names>F</given-names></name>, and <name><surname>Gonz&#x000e0;lez</surname><given-names>J</given-names></name> (<year>2018</year>). &#x0201c;<chapter-title>Attend and rectify: a gated attention mechanism for fine-grained recovery</chapter-title>,&#x0201d; in <source>Proceedings of the European Conference on Computer Vision (ECCV)</source> (<publisher-loc>Munich</publisher-loc>), <fpage>349</fpage>&#x02013;<lpage>364</lpage>.</mixed-citation></ref><ref id="R44"><mixed-citation publication-type="book"><name><surname>Sanchez</surname><given-names>E</given-names></name>, <name><surname>Tzimiropoulos</surname><given-names>G</given-names></name>, and <name><surname>Valstar</surname><given-names>M</given-names></name> (<year>2018</year>). &#x0201c;<chapter-title>Joint action unit localisation and intensity estimation through heatmap regression</chapter-title>,&#x0201d; in <source>BMVC</source> (<publisher-loc>Newcastle</publisher-loc>).</mixed-citation></ref><ref id="R45"><mixed-citation publication-type="book"><name><surname>Shao</surname><given-names>Z</given-names></name>, <name><surname>Liu</surname><given-names>Z</given-names></name>, <name><surname>Cai</surname><given-names>J</given-names></name>, and <name><surname>Ma</surname><given-names>L</given-names></name> (<year>2018</year>). &#x0201c;<chapter-title>Deep adaptive attention for joint facial action unit detection and face alignment</chapter-title>,&#x0201d; in <source>Proceedings of the European Conference on Computer Vision (ECCV)</source> (<publisher-loc>Munich</publisher-loc>), <fpage>705</fpage>&#x02013;<lpage>720</lpage>.</mixed-citation></ref><ref id="R46"><mixed-citation publication-type="journal"><name><surname>Shojaeilangari</surname><given-names>S</given-names></name>, <name><surname>Yau</surname><given-names>W-Y</given-names></name>, <name><surname>Nandakumar</surname><given-names>K</given-names></name>, <name><surname>Li</surname><given-names>J</given-names></name>, and <name><surname>Teoh</surname><given-names>EK</given-names></name> (<year>2015</year>). <article-title>Robust representation and recognition of facial emotions using extreme sparse learning</article-title>. <source>IEEE Trans. Image Process</source>. <volume>24</volume>, <fpage>2140</fpage>&#x02013;<lpage>2152</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TIP.2015.2416634</pub-id><pub-id pub-id-type="pmid">25823034</pub-id></mixed-citation></ref><ref id="R47"><mixed-citation publication-type="book"><name><surname>T&#x00151;s&#x000e9;r</surname><given-names>Z</given-names></name>, <name><surname>Jeni</surname><given-names>LA</given-names></name>, <name><surname>L&#x00151;rincz</surname><given-names>A</given-names></name>, and <name><surname>Cohn</surname><given-names>JF</given-names></name> (<year>2016</year>). &#x0201c;<chapter-title>Deep learning for facial action unit detection under large head poses</chapter-title>,&#x0201d; in <source>European Conference on Computer Vision</source> (<publisher-loc>Amsterdam</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>359</fpage>&#x02013;<lpage>371</lpage>.</mixed-citation></ref><ref id="R48"><mixed-citation publication-type="journal"><name><surname>Taheri</surname><given-names>S</given-names></name>, <name><surname>Qiu</surname><given-names>Q</given-names></name>, and <name><surname>Chellappa</surname><given-names>R</given-names></name> (<year>2014</year>). <article-title>Structure-preserving sparse decomposition for facial expression analysis</article-title>. <source>IEEE Trans. Image Process</source>
<volume>23</volume>, <fpage>3590</fpage>&#x02013;<lpage>3603</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TIP.2014.2331141</pub-id><pub-id pub-id-type="pmid">24956366</pub-id></mixed-citation></ref><ref id="R49"><mixed-citation publication-type="journal"><name><surname>Tian</surname><given-names>Y-I</given-names></name>, <name><surname>Kanade</surname><given-names>T</given-names></name>, and <name><surname>Cohn</surname><given-names>JF</given-names></name> (<year>2001</year>). <article-title>Recognizing action units for facial expression analysis</article-title>. <source>IEEE TPAMI</source>
<volume>23</volume>, <fpage>97</fpage>&#x02013;<lpage>115</lpage>. doi: <pub-id pub-id-type="doi">10.1109/34.908962</pub-id></mixed-citation></ref><ref id="R50"><mixed-citation publication-type="book"><name><surname>Valstar</surname><given-names>MF</given-names></name>, and <name><surname>Pantic</surname><given-names>M</given-names></name> (<year>2007</year>). &#x0201c;<chapter-title>Combined support vector machines and hidden markov models for modeling facial action temporal dynamics</chapter-title>,&#x0201d; in <source>International Workshop on Human-Computer Interaction</source> (<publisher-loc>Rio de Janeiro</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>118</fpage>&#x02013;<lpage>127</lpage>. doi: <pub-id pub-id-type="doi">10.1007/978-3-540-75773-3_13</pub-id></mixed-citation></ref><ref id="R51"><mixed-citation publication-type="book"><name><surname>Vielzeuf</surname><given-names>V</given-names></name>, <name><surname>Pateux</surname><given-names>S</given-names></name>, and <name><surname>Jurie</surname><given-names>F</given-names></name> (<year>2017</year>). &#x0201c;<chapter-title>Temporal multimodal fusion for video emotion classification in the wild</chapter-title>,&#x0201d; in <source>Proceedings of the 19th ACM International Conference on Multimodal Interaction</source> (<publisher-loc>Glasgow</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>569</fpage>&#x02013;<lpage>576</lpage>.</mixed-citation></ref><ref id="R52"><mixed-citation publication-type="book"><name><surname>Yang</surname><given-names>L</given-names></name>, <name><surname>Onal Ertugrul</surname><given-names>I</given-names></name>, <name><surname>Cohn</surname><given-names>JF</given-names></name>, <name><surname>Hammal</surname><given-names>Z</given-names></name>, <name><surname>Jiang</surname><given-names>D</given-names></name>, and <name><surname>Sahli</surname><given-names>H</given-names></name> (<year>2019</year>). &#x0201c;<chapter-title>FACS3D-net: 3D convolution based spatiotemporal representation for action unit detection</chapter-title>,&#x0201d; in <source>International Conference on Affective Computing and Intelligent Interaction</source> (<publisher-loc>Cambridge</publisher-loc>).</mixed-citation></ref><ref id="R53"><mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>P</given-names></name>, <name><surname>Liu</surname><given-names>Q</given-names></name>, and <name><surname>Metaxas</surname><given-names>DN</given-names></name> (<year>2009</year>). <article-title>Boosting encoded dynamic features for facial expression recognition</article-title>. <source>Pattern Recogn. Lett</source>
<volume>30</volume>, <fpage>132</fpage>&#x02013;<lpage>139</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.patrec.2008.03.014</pub-id></mixed-citation></ref><ref id="R54"><mixed-citation publication-type="book"><name><surname>Yang</surname><given-names>Z</given-names></name>, <name><surname>Yang</surname><given-names>D</given-names></name>, <name><surname>Dyer</surname><given-names>C</given-names></name>, <name><surname>He</surname><given-names>X</given-names></name>, <name><surname>Smola</surname><given-names>A</given-names></name>, and <name><surname>Hovy</surname><given-names>E</given-names></name> (<year>2016</year>). &#x0201c;<chapter-title>Hierarchical attention networks for document classification</chapter-title>,&#x0201d; in <source>Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</source> (<publisher-loc>San Diego, CA</publisher-loc>), <fpage>1480</fpage>&#x02013;<lpage>1489</lpage>.</mixed-citation></ref><ref id="R55"><mixed-citation publication-type="book"><name><surname>Zeng</surname><given-names>J</given-names></name>, <name><surname>Chu</surname><given-names>W-S</given-names></name>, <name><surname>De la Torre</surname><given-names>F</given-names></name>, <name><surname>Cohn</surname><given-names>JF</given-names></name>, and <name><surname>Xiong</surname><given-names>Z</given-names></name> (<year>2015</year>). &#x0201c;<chapter-title>Confidence preserving machine for facial action unit detection</chapter-title>,&#x0201d; in <source>Proceedings of the IEEE international conference on computer vision</source> (<publisher-loc>Nice</publisher-loc>), <fpage>3622</fpage>&#x02013;<lpage>3630</lpage>. doi: <pub-id pub-id-type="doi">10.1109/ICCV.2015.413</pub-id></mixed-citation></ref><ref id="R56"><mixed-citation publication-type="book"><name><surname>Zhang</surname><given-names>Z</given-names></name>, <name><surname>Zhai</surname><given-names>S</given-names></name>, and <name><surname>Yin</surname><given-names>L</given-names></name> (<year>2018</year>). &#x0201c;<chapter-title>Identity-based adversarial training of deep CNNs for facial action unit recognition</chapter-title>,&#x0201d; in <source>BMVC</source> (<publisher-loc>Newcastle</publisher-loc>).</mixed-citation></ref><ref id="R57"><mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>K</given-names></name>, <name><surname>Chu</surname><given-names>W-S</given-names></name>, <name><surname>De la Torre</surname><given-names>F</given-names></name>, <name><surname>Cohn</surname><given-names>JF</given-names></name>, and <name><surname>Zhang</surname><given-names>H</given-names></name> (<year>2016a</year>). <article-title>Joint patch and multi-label learning for facial action unit and holistic expression recognition</article-title>. <source>IEEE Trans. Image Process</source>
<volume>25</volume>, <fpage>3931</fpage>&#x02013;<lpage>3946</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TIP.2016.2570550</pub-id><pub-id pub-id-type="pmid">28113424</pub-id></mixed-citation></ref><ref id="R58"><mixed-citation publication-type="book"><name><surname>Zhao</surname><given-names>K</given-names></name>, <name><surname>Chu</surname><given-names>W-S</given-names></name>, and <name><surname>Martinez</surname><given-names>AM</given-names></name> (<year>2018</year>). &#x0201c;<chapter-title>Learning facial action units from web images with scalable weakly supervised clustering</chapter-title>,&#x0201d; in <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Salt Lake, UT</publisher-loc>), <fpage>2090</fpage>&#x02013;<lpage>2099</lpage>. doi: <pub-id pub-id-type="doi">10.1109/CVPR.2018.00223</pub-id></mixed-citation></ref><ref id="R59"><mixed-citation publication-type="book"><name><surname>Zhao</surname><given-names>K</given-names></name>, <name><surname>Chu</surname><given-names>W-S</given-names></name>, and <name><surname>Zhang</surname><given-names>H</given-names></name> (<year>2016b</year>). &#x0201c;<chapter-title>Deep region and multi-label learning for facial action unit detection</chapter-title>,&#x0201d; in <source>CVPR</source> (<publisher-loc>Las Vegas, NV</publisher-loc>), <fpage>3391</fpage>&#x02013;<lpage>3399</lpage>. doi: <pub-id pub-id-type="doi">10.1109/CVPR.2016.369</pub-id></mixed-citation></ref><ref id="R60"><mixed-citation publication-type="journal"><name><surname>Zhong</surname><given-names>L</given-names></name>, <name><surname>Liu</surname><given-names>Q</given-names></name>, <name><surname>Yang</surname><given-names>P</given-names></name>, <name><surname>Huang</surname><given-names>J</given-names></name>, and <name><surname>Metaxas</surname><given-names>DN</given-names></name> (<year>2015</year>). <article-title>Learning multiscale active facial patches for expression analysis</article-title>. <source>IEEE Trans. Cybern</source>
<volume>45</volume>, <fpage>1499</fpage>&#x02013;<lpage>1510</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TCYB.2014.2354351</pub-id><pub-id pub-id-type="pmid">25291808</pub-id></mixed-citation></ref></ref-list></back><floats-group><fig id="F1" orientation="portrait" position="float"><label>FIGURE 1 |</label><caption><p id="P73">Proposed D-PAttNet approach. <bold>(a)</bold> A dense set of facial landmarks is estimated and a dense 3D mesh of the face is reconstructed. <bold>(b)</bold> Patches containing facial regions related to specific AUs are cropped and fed to different CNNs for encoding. For each patch, 2D-CNN is used to encode static frame-level information and 3D-CNN is used to encode dynamic, segment-level information. Patch encoding is obtained by concatenating static and dynamic encoding. <bold>(c)</bold> Patches are weighted by sigmoidal attention mechanism to detect specific AUs. <bold>(d)</bold> Face encodings are fed to a fully connected layer (FC) to detect AUs.</p></caption><graphic xlink:href="nihms-1061808-f0001"/></fig><fig id="F2" orientation="portrait" position="float"><label>FIGURE 2 |</label><caption><p id="P74">Cropped patches from 3D registered face images.</p></caption><graphic xlink:href="nihms-1061808-f0002"/></fig><fig id="F3" orientation="portrait" position="float"><label>FIGURE 3 |</label><caption><p id="P75">Co-occurrence matrix of AUs computed with Jaccard index.</p></caption><graphic xlink:href="nihms-1061808-f0003"/></fig><fig id="F4" orientation="portrait" position="float"><label>FIGURE 4 |</label><caption><p id="P76">Average attention maps for PAttNet with sigmoid attention <bold>(A)</bold>, PAttNet with softmax attention <bold>(B)</bold>, D-PAttNet with sigmoid attention <bold>(C)</bold>, and D-PAttNet with softmax attention <bold>(D)</bold>. Attention maps are obtained by averaging attention weights of all samples. Attention weights are in [0,1]. White color represents no attention (0) and black color represents the maximum attention (1).</p></caption><graphic xlink:href="nihms-1061808-f0004"/></fig><table-wrap id="T1" position="float" orientation="landscape"><label>TABLE 1 |</label><caption><p id="P77">AU detection performances (F1-scores) on BP4D dataset.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" valign="middle" rowspan="1" colspan="1">AU</th><th align="center" valign="middle" rowspan="1" colspan="1">1</th><th align="center" valign="middle" rowspan="1" colspan="1">2</th><th align="center" valign="middle" rowspan="1" colspan="1">4</th><th align="center" valign="middle" rowspan="1" colspan="1">6</th><th align="center" valign="middle" rowspan="1" colspan="1">7</th><th align="center" valign="middle" rowspan="1" colspan="1">10</th><th align="center" valign="middle" rowspan="1" colspan="1">12</th><th align="center" valign="middle" rowspan="1" colspan="1">14</th><th align="center" valign="middle" rowspan="1" colspan="1">15</th><th align="center" valign="middle" rowspan="1" colspan="1">17</th><th align="center" valign="middle" rowspan="1" colspan="1">23</th><th align="center" valign="middle" rowspan="1" colspan="1">24</th><th align="center" valign="middle" rowspan="1" colspan="1">Avg.</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">LSVM</td><td align="center" valign="middle" rowspan="1" colspan="1">23.2</td><td align="center" valign="middle" rowspan="1" colspan="1">22.8</td><td align="center" valign="middle" rowspan="1" colspan="1">23.1</td><td align="center" valign="middle" rowspan="1" colspan="1">27.2</td><td align="center" valign="middle" rowspan="1" colspan="1">47.1</td><td align="center" valign="middle" rowspan="1" colspan="1">77.2</td><td align="center" valign="middle" rowspan="1" colspan="1">63.7</td><td align="center" valign="middle" rowspan="1" colspan="1">64.3</td><td align="center" valign="middle" rowspan="1" colspan="1">18.4</td><td align="center" valign="middle" rowspan="1" colspan="1">33.0</td><td align="center" valign="middle" rowspan="1" colspan="1">19.4</td><td align="center" valign="middle" rowspan="1" colspan="1">20.7</td><td align="center" valign="middle" rowspan="1" colspan="1">36.7</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">JPML</td><td align="center" valign="middle" rowspan="1" colspan="1">32.6</td><td align="center" valign="middle" rowspan="1" colspan="1">25.6</td><td align="center" valign="middle" rowspan="1" colspan="1">37.4</td><td align="center" valign="middle" rowspan="1" colspan="1">42.3</td><td align="center" valign="middle" rowspan="1" colspan="1">50.5</td><td align="center" valign="middle" rowspan="1" colspan="1">72.2</td><td align="center" valign="middle" rowspan="1" colspan="1">74.1</td><td align="center" valign="middle" rowspan="1" colspan="1">65.7</td><td align="center" valign="middle" rowspan="1" colspan="1">38.1</td><td align="center" valign="middle" rowspan="1" colspan="1">40.0</td><td align="center" valign="middle" rowspan="1" colspan="1">30.4</td><td align="center" valign="middle" rowspan="1" colspan="1">42.3</td><td align="center" valign="middle" rowspan="1" colspan="1">45.9</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DRML</td><td align="center" valign="middle" rowspan="1" colspan="1">36.4</td><td align="center" valign="middle" rowspan="1" colspan="1">41.8</td><td align="center" valign="middle" rowspan="1" colspan="1">43.0</td><td align="center" valign="middle" rowspan="1" colspan="1">55.0</td><td align="center" valign="middle" rowspan="1" colspan="1">67.0</td><td align="center" valign="middle" rowspan="1" colspan="1">66.3</td><td align="center" valign="middle" rowspan="1" colspan="1">65.8</td><td align="center" valign="middle" rowspan="1" colspan="1">54.1</td><td align="center" valign="middle" rowspan="1" colspan="1">33.2</td><td align="center" valign="middle" rowspan="1" colspan="1">48.0</td><td align="center" valign="middle" rowspan="1" colspan="1">31.7</td><td align="center" valign="middle" rowspan="1" colspan="1">30.0</td><td align="center" valign="middle" rowspan="1" colspan="1">47.7</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">31.4</td><td align="center" valign="middle" rowspan="1" colspan="1">31.1</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>71.4</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">63.3</td><td align="center" valign="middle" rowspan="1" colspan="1">77.1</td><td align="center" valign="middle" rowspan="1" colspan="1">45.0</td><td align="center" valign="middle" rowspan="1" colspan="1">82.6</td><td align="center" valign="middle" rowspan="1" colspan="1">72.9</td><td align="center" valign="middle" rowspan="1" colspan="1">34.0</td><td align="center" valign="middle" rowspan="1" colspan="1">53.9</td><td align="center" valign="middle" rowspan="1" colspan="1">38.6</td><td align="center" valign="middle" rowspan="1" colspan="1">37.0</td><td align="center" valign="middle" rowspan="1" colspan="1">53.2</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ATF</td><td align="center" valign="middle" rowspan="1" colspan="1">39.2</td><td align="center" valign="middle" rowspan="1" colspan="1">35.2</td><td align="center" valign="middle" rowspan="1" colspan="1">45.9</td><td align="center" valign="middle" rowspan="1" colspan="1">71.6</td><td align="center" valign="middle" rowspan="1" colspan="1">71.9</td><td align="center" valign="middle" rowspan="1" colspan="1">79.0</td><td align="center" valign="middle" rowspan="1" colspan="1">83.7</td><td align="center" valign="middle" rowspan="1" colspan="1">65.5</td><td align="center" valign="middle" rowspan="1" colspan="1">33.8</td><td align="center" valign="middle" rowspan="1" colspan="1">60.0</td><td align="center" valign="middle" rowspan="1" colspan="1">37.3</td><td align="center" valign="middle" rowspan="1" colspan="1">41.8</td><td align="center" valign="middle" rowspan="1" colspan="1">55.4</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FVGG</td><td align="center" valign="middle" rowspan="1" colspan="1">27.8</td><td align="center" valign="middle" rowspan="1" colspan="1">27.6</td><td align="center" valign="middle" rowspan="1" colspan="1">18.3</td><td align="center" valign="middle" rowspan="1" colspan="1">69.7</td><td align="center" valign="middle" rowspan="1" colspan="1">69.1</td><td align="center" valign="middle" rowspan="1" colspan="1">78.1</td><td align="center" valign="middle" rowspan="1" colspan="1">63.2</td><td align="center" valign="middle" rowspan="1" colspan="1">36.4</td><td align="center" valign="middle" rowspan="1" colspan="1">26.1</td><td align="center" valign="middle" rowspan="1" colspan="1">50.7</td><td align="center" valign="middle" rowspan="1" colspan="1">22.8</td><td align="center" valign="middle" rowspan="1" colspan="1">35.9</td><td align="center" valign="middle" rowspan="1" colspan="1">43.8</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">E-Net</td><td align="center" valign="middle" rowspan="1" colspan="1">37.6</td><td align="center" valign="middle" rowspan="1" colspan="1">32.1</td><td align="center" valign="middle" rowspan="1" colspan="1">44.2</td><td align="center" valign="middle" rowspan="1" colspan="1">75.6</td><td align="center" valign="middle" rowspan="1" colspan="1">74.5</td><td align="center" valign="middle" rowspan="1" colspan="1">80.8</td><td align="center" valign="middle" rowspan="1" colspan="1">85.1</td><td align="center" valign="middle" rowspan="1" colspan="1">56.8</td><td align="center" valign="middle" rowspan="1" colspan="1">31.6</td><td align="center" valign="middle" rowspan="1" colspan="1">55.6</td><td align="center" valign="middle" rowspan="1" colspan="1">21.9</td><td align="center" valign="middle" rowspan="1" colspan="1">29.1</td><td align="center" valign="middle" rowspan="1" colspan="1">52.1</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">EAC-Net</td><td align="center" valign="middle" rowspan="1" colspan="1">39.0</td><td align="center" valign="middle" rowspan="1" colspan="1">35.2</td><td align="center" valign="middle" rowspan="1" colspan="1">48.6</td><td align="center" valign="middle" rowspan="1" colspan="1">76.1</td><td align="center" valign="middle" rowspan="1" colspan="1">72.9</td><td align="center" valign="middle" rowspan="1" colspan="1">81.9</td><td align="center" valign="middle" rowspan="1" colspan="1">86.2</td><td align="center" valign="middle" rowspan="1" colspan="1">58.8</td><td align="center" valign="middle" rowspan="1" colspan="1">37.5</td><td align="center" valign="middle" rowspan="1" colspan="1">59.1</td><td align="center" valign="middle" rowspan="1" colspan="1">35.9</td><td align="center" valign="middle" rowspan="1" colspan="1">35.8</td><td align="center" valign="middle" rowspan="1" colspan="1">55.6</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">JAA</td><td align="center" valign="middle" rowspan="1" colspan="1">47.2</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>44.0</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">54.9</td><td align="center" valign="middle" rowspan="1" colspan="1">77.5</td><td align="center" valign="middle" rowspan="1" colspan="1">74.6</td><td align="center" valign="middle" rowspan="1" colspan="1">84.0</td><td align="center" valign="middle" rowspan="1" colspan="1">86.9</td><td align="center" valign="middle" rowspan="1" colspan="1">61.9</td><td align="center" valign="middle" rowspan="1" colspan="1">43.6</td><td align="center" valign="middle" rowspan="1" colspan="1">60.3</td><td align="center" valign="middle" rowspan="1" colspan="1">42.7</td><td align="center" valign="middle" rowspan="1" colspan="1">41.9</td><td align="center" valign="middle" rowspan="1" colspan="1">60.0</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DSIN</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>51.7</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">40.4</td><td align="center" valign="middle" rowspan="1" colspan="1">56.0</td><td align="center" valign="middle" rowspan="1" colspan="1">76.1</td><td align="center" valign="middle" rowspan="1" colspan="1">73.5</td><td align="center" valign="middle" rowspan="1" colspan="1">79.9</td><td align="center" valign="middle" rowspan="1" colspan="1">85.4</td><td align="center" valign="middle" rowspan="1" colspan="1">62.7</td><td align="center" valign="middle" rowspan="1" colspan="1">37.3</td><td align="center" valign="middle" rowspan="1" colspan="1">62.9</td><td align="center" valign="middle" rowspan="1" colspan="1">38.8</td><td align="center" valign="middle" rowspan="1" colspan="1">41.6</td><td align="center" valign="middle" rowspan="1" colspan="1">58.9</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DSIN<sup><italic>tt</italic></sup></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>51.7</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">41.6</td><td align="center" valign="middle" rowspan="1" colspan="1">58.1</td><td align="center" valign="middle" rowspan="1" colspan="1">76.6</td><td align="center" valign="middle" rowspan="1" colspan="1">74.1</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>85.5</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">87.4</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>72.6</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">40.4</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>66.5</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">38.6</td><td align="center" valign="middle" rowspan="1" colspan="1">46.9</td><td align="center" valign="middle" rowspan="1" colspan="1">61.7</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PAttNet</td><td align="center" valign="middle" rowspan="1" colspan="1">46.1</td><td align="center" valign="middle" rowspan="1" colspan="1">41.4</td><td align="center" valign="middle" rowspan="1" colspan="1">57.1</td><td align="center" valign="middle" rowspan="1" colspan="1">77.9</td><td align="center" valign="middle" rowspan="1" colspan="1">76.1</td><td align="center" valign="middle" rowspan="1" colspan="1">83.8</td><td align="center" valign="middle" rowspan="1" colspan="1">88.4</td><td align="center" valign="middle" rowspan="1" colspan="1">66.5</td><td align="center" valign="middle" rowspan="1" colspan="1">51.2</td><td align="center" valign="middle" rowspan="1" colspan="1">61.6</td><td align="center" valign="middle" rowspan="1" colspan="1">44.1</td><td align="center" valign="middle" rowspan="1" colspan="1">57.3</td><td align="center" valign="middle" rowspan="1" colspan="1">62.6</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Only3D-PAttNet</td><td align="center" valign="middle" rowspan="1" colspan="1">36.8</td><td align="center" valign="middle" rowspan="1" colspan="1">33.9</td><td align="center" valign="middle" rowspan="1" colspan="1">47.9</td><td align="center" valign="middle" rowspan="1" colspan="1">74.6</td><td align="center" valign="middle" rowspan="1" colspan="1">72.2</td><td align="center" valign="middle" rowspan="1" colspan="1">81.7</td><td align="center" valign="middle" rowspan="1" colspan="1">84.0</td><td align="center" valign="middle" rowspan="1" colspan="1">62.0</td><td align="center" valign="middle" rowspan="1" colspan="1">41.9</td><td align="center" valign="middle" rowspan="1" colspan="1">58.1</td><td align="center" valign="middle" rowspan="1" colspan="1">40.0</td><td align="center" valign="middle" rowspan="1" colspan="1">45.7</td><td align="center" valign="middle" rowspan="1" colspan="1">56.6</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">D-PAttNet</td><td align="center" valign="middle" rowspan="1" colspan="1">50.4</td><td align="center" valign="middle" rowspan="1" colspan="1">41.1</td><td align="center" valign="middle" rowspan="1" colspan="1">58.4</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>78.6</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>77.5</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">84.6</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>89.0</underline></td><td align="center" valign="middle" rowspan="1" colspan="1">66.7</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>52.6</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">64.5</td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>49.0</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>57.6</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>64.1</underline></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">D-PAttNet<sup><italic>tt</italic></sup></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>50.7</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>42.5</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>59.0</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>79.4</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>79.0</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>85.0</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>89.3</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>67.6</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>51.6</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>65.3</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>49.6</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><underline>57.5</underline></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>64.7</bold></td></tr></tbody></table><table-wrap-foot><fn id="TFN1"><p id="P78">The best results are shown in bold and the second best results are shown underlined. For methods DSIN and D-PAttNet, <sup><italic>tt</italic></sup> denotes the use of threshold tuning.</p></fn></table-wrap-foot></table-wrap><table-wrap id="T2" position="float" orientation="landscape"><label>TABLE 2 |</label><caption><p id="P79">AU detection performances (AUC) on BP4D dataset.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" valign="middle" rowspan="1" colspan="1">AU</th><th align="center" valign="middle" rowspan="1" colspan="1">1</th><th align="center" valign="middle" rowspan="1" colspan="1">2</th><th align="center" valign="middle" rowspan="1" colspan="1">4</th><th align="center" valign="middle" rowspan="1" colspan="1">6</th><th align="center" valign="middle" rowspan="1" colspan="1">7</th><th align="center" valign="middle" rowspan="1" colspan="1">10</th><th align="center" valign="middle" rowspan="1" colspan="1">12</th><th align="center" valign="middle" rowspan="1" colspan="1">14</th><th align="center" valign="middle" rowspan="1" colspan="1">15</th><th align="center" valign="middle" rowspan="1" colspan="1">17</th><th align="center" valign="middle" rowspan="1" colspan="1">23</th><th align="center" valign="middle" rowspan="1" colspan="1">24</th><th align="center" valign="middle" rowspan="1" colspan="1">Avg.</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">LSVM</td><td align="center" valign="middle" rowspan="1" colspan="1">20.7</td><td align="center" valign="middle" rowspan="1" colspan="1">17.7</td><td align="center" valign="middle" rowspan="1" colspan="1">22.9</td><td align="center" valign="middle" rowspan="1" colspan="1">20.3</td><td align="center" valign="middle" rowspan="1" colspan="1">44.8</td><td align="center" valign="middle" rowspan="1" colspan="1">73.4</td><td align="center" valign="middle" rowspan="1" colspan="1">55.3</td><td align="center" valign="middle" rowspan="1" colspan="1">46.8</td><td align="center" valign="middle" rowspan="1" colspan="1">18.3</td><td align="center" valign="middle" rowspan="1" colspan="1">36.4</td><td align="center" valign="middle" rowspan="1" colspan="1">19.2</td><td align="center" valign="middle" rowspan="1" colspan="1">11.7</td><td align="center" valign="middle" rowspan="1" colspan="1">32.3</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">JPML</td><td align="center" valign="middle" rowspan="1" colspan="1">40.7</td><td align="center" valign="middle" rowspan="1" colspan="1">42.1</td><td align="center" valign="middle" rowspan="1" colspan="1">46.2</td><td align="center" valign="middle" rowspan="1" colspan="1">40.0</td><td align="center" valign="middle" rowspan="1" colspan="1">50.0</td><td align="center" valign="middle" rowspan="1" colspan="1">75.2</td><td align="center" valign="middle" rowspan="1" colspan="1">60.5</td><td align="center" valign="middle" rowspan="1" colspan="1">53.6</td><td align="center" valign="middle" rowspan="1" colspan="1">50.1</td><td align="center" valign="middle" rowspan="1" colspan="1">42.5</td><td align="center" valign="middle" rowspan="1" colspan="1">51.9</td><td align="center" valign="middle" rowspan="1" colspan="1">53.2</td><td align="center" valign="middle" rowspan="1" colspan="1">50.5</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DRML</td><td align="center" valign="middle" rowspan="1" colspan="1">55.7</td><td align="center" valign="middle" rowspan="1" colspan="1">54.5</td><td align="center" valign="middle" rowspan="1" colspan="1">58.8</td><td align="center" valign="middle" rowspan="1" colspan="1">56.6</td><td align="center" valign="middle" rowspan="1" colspan="1">61.0</td><td align="center" valign="middle" rowspan="1" colspan="1">53.6</td><td align="center" valign="middle" rowspan="1" colspan="1">60.8</td><td align="center" valign="middle" rowspan="1" colspan="1">57.0</td><td align="center" valign="middle" rowspan="1" colspan="1">56.2</td><td align="center" valign="middle" rowspan="1" colspan="1">50.0</td><td align="center" valign="middle" rowspan="1" colspan="1">53.9</td><td align="center" valign="middle" rowspan="1" colspan="1">53.9</td><td align="center" valign="middle" rowspan="1" colspan="1">56.0</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PAttNet</td><td align="center" valign="middle" rowspan="1" colspan="1">66.5</td><td align="center" valign="middle" rowspan="1" colspan="1">65.6</td><td align="center" valign="middle" rowspan="1" colspan="1">74.4</td><td align="center" valign="middle" rowspan="1" colspan="1">78.6</td><td align="center" valign="middle" rowspan="1" colspan="1">71.8</td><td align="center" valign="middle" rowspan="1" colspan="1">78.4</td><td align="center" valign="middle" rowspan="1" colspan="1">86.4</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>65.4</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>72.1</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">70.1</td><td align="center" valign="middle" rowspan="1" colspan="1">68.0</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>74.8</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">72.7</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Only3D-PAttNet</td><td align="center" valign="middle" rowspan="1" colspan="1">59.5</td><td align="center" valign="middle" rowspan="1" colspan="1">59.6</td><td align="center" valign="middle" rowspan="1" colspan="1">67.6</td><td align="center" valign="middle" rowspan="1" colspan="1">75.9</td><td align="center" valign="middle" rowspan="1" colspan="1">66.1</td><td align="center" valign="middle" rowspan="1" colspan="1">75.9</td><td align="center" valign="middle" rowspan="1" colspan="1">81.5</td><td align="center" valign="middle" rowspan="1" colspan="1">63.0</td><td align="center" valign="middle" rowspan="1" colspan="1">65.6</td><td align="center" valign="middle" rowspan="1" colspan="1">67.1</td><td align="center" valign="middle" rowspan="1" colspan="1">64.6</td><td align="center" valign="middle" rowspan="1" colspan="1">68.1</td><td align="center" valign="middle" rowspan="1" colspan="1">67.9</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">D-PAttNet</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>68.3</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>66.0</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>75.6</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>79.1</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>73.0</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>79.0</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>87.0</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">64.9</td><td align="center" valign="middle" rowspan="1" colspan="1">72.0</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>71.9</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>69.5</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">74.5</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>73.4</bold></td></tr></tbody></table><table-wrap-foot><fn id="TFN2"><p id="P80">The best results are shown in bold.</p></fn></table-wrap-foot></table-wrap><table-wrap id="T3" position="float" orientation="landscape"><label>TABLE 3 |</label><caption><p id="P81">Significance of differences between D-PAttNet and the two other variants (PAttNet and Only3D-PAttNet) by <italic>t</italic>-test.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" valign="middle" rowspan="1" colspan="1"/><th align="center" valign="middle" rowspan="1" colspan="1"/><th align="center" valign="middle" rowspan="1" colspan="1">1</th><th align="center" valign="middle" rowspan="1" colspan="1">2</th><th align="center" valign="middle" rowspan="1" colspan="1">4</th><th align="center" valign="middle" rowspan="1" colspan="1">6</th><th align="center" valign="middle" rowspan="1" colspan="1">7</th><th align="center" valign="middle" rowspan="1" colspan="1">10</th><th align="center" valign="middle" rowspan="1" colspan="1">12</th><th align="center" valign="middle" rowspan="1" colspan="1">14</th><th align="center" valign="middle" rowspan="1" colspan="1">15</th><th align="center" valign="middle" rowspan="1" colspan="1">17</th><th align="center" valign="middle" rowspan="1" colspan="1">23</th><th align="center" valign="middle" rowspan="1" colspan="1">24</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">D-PAttNet &#x0003e;</td><td align="center" valign="middle" rowspan="1" colspan="1">F1</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" style="background-color:#939598" rowspan="1" colspan="1"><xref rid="TFN3" ref-type="table-fn">*</xref></td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN3" ref-type="table-fn">*</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN3" ref-type="table-fn">*</xref></td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PAttNet</td><td align="center" valign="middle" rowspan="1" colspan="1">AUC</td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" style="background-color:#939598" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN3" ref-type="table-fn">*</xref></td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">D-PAttNet &#x0003e;</td><td align="center" valign="middle" rowspan="1" colspan="1">F1</td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN3" ref-type="table-fn">*</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Only3D-PAttNet</td><td align="center" valign="middle" rowspan="1" colspan="1">AUC</td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN3" ref-type="table-fn">*</xref></td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN3" ref-type="table-fn">*</xref></td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN4" ref-type="table-fn">**</xref></td></tr></tbody></table><table-wrap-foot><fn id="TFN3"><label>*</label><p id="P82"><italic>p</italic> &#x0003c; <italic>0.05</italic>,</p></fn><fn id="TFN4"><label>**</label><p id="P83"><italic>p</italic> &#x0003c; <italic>0.05/12</italic>.</p></fn><fn id="TFN5"><p id="P84">The latter are significant after correcting for multiple comparisons. n.s., not significant. Cells denoted with gray color indicates cases where the results for PAttNet are greater than the ones for D-PAttNet.</p></fn></table-wrap-foot></table-wrap><table-wrap id="T4" position="float" orientation="landscape"><label>TABLE 4 |</label><caption><p id="P85">Comparison of sigmoid and softmax attention functions in PAttNet, Only3D-PAttNet, and D-PAttNet (F1-scores).</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" valign="middle" rowspan="1" colspan="1">AU</th><th align="center" valign="middle" rowspan="1" colspan="1"/><th align="center" valign="middle" rowspan="1" colspan="1">1</th><th align="center" valign="middle" rowspan="1" colspan="1">2</th><th align="center" valign="middle" rowspan="1" colspan="1">4</th><th align="center" valign="middle" rowspan="1" colspan="1">6</th><th align="center" valign="middle" rowspan="1" colspan="1">7</th><th align="center" valign="middle" rowspan="1" colspan="1">10</th><th align="center" valign="middle" rowspan="1" colspan="1">12</th><th align="center" valign="middle" rowspan="1" colspan="1">14</th><th align="center" valign="middle" rowspan="1" colspan="1">15</th><th align="center" valign="middle" rowspan="1" colspan="1">17</th><th align="center" valign="middle" rowspan="1" colspan="1">23</th><th align="center" valign="middle" rowspan="1" colspan="1">24</th><th align="center" valign="middle" rowspan="1" colspan="1">Avg</th></tr></thead><tbody><tr><td rowspan="2" align="left" valign="middle" colspan="1">PAttNet (2D)</td><td align="center" valign="middle" rowspan="1" colspan="1">Softmax</td><td align="center" valign="middle" rowspan="1" colspan="1">37.2</td><td align="center" valign="middle" rowspan="1" colspan="1">28.4</td><td align="center" valign="middle" rowspan="1" colspan="1">41.4</td><td align="center" valign="middle" rowspan="1" colspan="1">73.4</td><td align="center" valign="middle" rowspan="1" colspan="1">69.8</td><td align="center" valign="middle" rowspan="1" colspan="1">79.3</td><td align="center" valign="middle" rowspan="1" colspan="1">81.9</td><td align="center" valign="middle" rowspan="1" colspan="1">58.7</td><td align="center" valign="middle" rowspan="1" colspan="1">32.7</td><td align="center" valign="middle" rowspan="1" colspan="1">58.5</td><td align="center" valign="middle" rowspan="1" colspan="1">39.8</td><td align="center" valign="middle" rowspan="1" colspan="1">49.2</td><td align="center" valign="middle" rowspan="1" colspan="1">54.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sigmoid</td><td align="center" valign="middle" rowspan="1" colspan="1">46.1</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>41.4</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">57.1</td><td align="center" valign="middle" rowspan="1" colspan="1">77.9</td><td align="center" valign="middle" rowspan="1" colspan="1">76.1</td><td align="center" valign="middle" rowspan="1" colspan="1">83.8</td><td align="center" valign="middle" rowspan="1" colspan="1">88.4</td><td align="center" valign="middle" rowspan="1" colspan="1">66.5</td><td align="center" valign="middle" rowspan="1" colspan="1">51.2</td><td align="center" valign="middle" rowspan="1" colspan="1">61.6</td><td align="center" valign="middle" rowspan="1" colspan="1">44.1</td><td align="center" valign="middle" rowspan="1" colspan="1">57.3</td><td align="center" valign="middle" rowspan="1" colspan="1">62.6</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Only3D</td><td align="center" valign="middle" rowspan="1" colspan="1">Softmax</td><td align="center" valign="middle" rowspan="1" colspan="1">46.5</td><td align="center" valign="middle" rowspan="1" colspan="1">33.8</td><td align="center" valign="middle" rowspan="1" colspan="1">41.3</td><td align="center" valign="middle" rowspan="1" colspan="1">74.5</td><td align="center" valign="middle" rowspan="1" colspan="1">71.4</td><td align="center" valign="middle" rowspan="1" colspan="1">81.9</td><td align="center" valign="middle" rowspan="1" colspan="1">85.9</td><td align="center" valign="middle" rowspan="1" colspan="1">57.6</td><td align="center" valign="middle" rowspan="1" colspan="1">33.4</td><td align="center" valign="middle" rowspan="1" colspan="1">55.2</td><td align="center" valign="middle" rowspan="1" colspan="1">43.1</td><td align="center" valign="middle" rowspan="1" colspan="1">46.1</td><td align="center" valign="middle" rowspan="1" colspan="1">55.9</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PAttNet</td><td align="center" valign="middle" rowspan="1" colspan="1">Sigmoid</td><td align="center" valign="middle" rowspan="1" colspan="1">36.8</td><td align="center" valign="middle" rowspan="1" colspan="1">33.9</td><td align="center" valign="middle" rowspan="1" colspan="1">47.9</td><td align="center" valign="middle" rowspan="1" colspan="1">74.6</td><td align="center" valign="middle" rowspan="1" colspan="1">72.2</td><td align="center" valign="middle" rowspan="1" colspan="1">81.7</td><td align="center" valign="middle" rowspan="1" colspan="1">84.0</td><td align="center" valign="middle" rowspan="1" colspan="1">62.0</td><td align="center" valign="middle" rowspan="1" colspan="1">41.9</td><td align="center" valign="middle" rowspan="1" colspan="1">58.1</td><td align="center" valign="middle" rowspan="1" colspan="1">40.0</td><td align="center" valign="middle" rowspan="1" colspan="1">45.7</td><td align="center" valign="middle" rowspan="1" colspan="1">56.6</td></tr><tr><td rowspan="2" align="left" valign="middle" colspan="1">D-PAttNet (2D + 3D)</td><td align="center" valign="middle" rowspan="1" colspan="1">Softmax</td><td align="center" valign="middle" rowspan="1" colspan="1">42.5</td><td align="center" valign="middle" rowspan="1" colspan="1">41.2</td><td align="center" valign="middle" rowspan="1" colspan="1">42.0</td><td align="center" valign="middle" rowspan="1" colspan="1">72.1</td><td align="center" valign="middle" rowspan="1" colspan="1">72.2</td><td align="center" valign="middle" rowspan="1" colspan="1">82.6</td><td align="center" valign="middle" rowspan="1" colspan="1">86.7</td><td align="center" valign="middle" rowspan="1" colspan="1">62.1</td><td align="center" valign="middle" rowspan="1" colspan="1">32.2</td><td align="center" valign="middle" rowspan="1" colspan="1">54.9</td><td align="center" valign="middle" rowspan="1" colspan="1">37.8</td><td align="center" valign="middle" rowspan="1" colspan="1">52.4</td><td align="center" valign="middle" rowspan="1" colspan="1">56.6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sigmoid</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>50.4</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">41.1</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>58.4</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>78.6</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>77.5</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>84.6</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>89.0</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>66.7</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>52.6</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>64.5</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>49.0</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>57.6</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>64.1</bold></td></tr></tbody></table><table-wrap-foot><fn id="TFN6"><p id="P86">The best results are shown in bold.</p></fn></table-wrap-foot></table-wrap><table-wrap id="T5" position="float" orientation="landscape"><label>TABLE 5 |</label><caption><p id="P87">Comparison of sigmoid and softmax attention functions in PAttNet, Only3D-PAttNet, and D-PAttNet (AUC).</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" valign="middle" rowspan="1" colspan="1">AU</th><th align="center" valign="middle" rowspan="1" colspan="1"/><th align="center" valign="middle" rowspan="1" colspan="1">1</th><th align="center" valign="middle" rowspan="1" colspan="1">2</th><th align="center" valign="middle" rowspan="1" colspan="1">4</th><th align="center" valign="middle" rowspan="1" colspan="1">6</th><th align="center" valign="middle" rowspan="1" colspan="1">7</th><th align="center" valign="middle" rowspan="1" colspan="1">10</th><th align="center" valign="middle" rowspan="1" colspan="1">12</th><th align="center" valign="middle" rowspan="1" colspan="1">14</th><th align="center" valign="middle" rowspan="1" colspan="1">15</th><th align="center" valign="middle" rowspan="1" colspan="1">17</th><th align="center" valign="middle" rowspan="1" colspan="1">23</th><th align="center" valign="middle" rowspan="1" colspan="1">24</th><th align="center" valign="middle" rowspan="1" colspan="1">Avg</th></tr></thead><tbody><tr><td rowspan="2" align="left" valign="middle" colspan="1">PAttNet (2D)</td><td align="center" valign="middle" rowspan="1" colspan="1">Softmax</td><td align="center" valign="middle" rowspan="1" colspan="1">59.2</td><td align="center" valign="middle" rowspan="1" colspan="1">54.7</td><td align="center" valign="middle" rowspan="1" colspan="1">63.4</td><td align="center" valign="middle" rowspan="1" colspan="1">74.7</td><td align="center" valign="middle" rowspan="1" colspan="1">65.9</td><td align="center" valign="middle" rowspan="1" colspan="1">72.9</td><td align="center" valign="middle" rowspan="1" colspan="1">76.9</td><td align="center" valign="middle" rowspan="1" colspan="1">58.8</td><td align="center" valign="middle" rowspan="1" colspan="1">59.6</td><td align="center" valign="middle" rowspan="1" colspan="1">67.3</td><td align="center" valign="middle" rowspan="1" colspan="1">65.3</td><td align="center" valign="middle" rowspan="1" colspan="1">71.4</td><td align="center" valign="middle" rowspan="1" colspan="1">65.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sigmoid</td><td align="center" valign="middle" rowspan="1" colspan="1">66.5</td><td align="center" valign="middle" rowspan="1" colspan="1">65.6</td><td align="center" valign="middle" rowspan="1" colspan="1">74.4</td><td align="center" valign="middle" rowspan="1" colspan="1">78.6</td><td align="center" valign="middle" rowspan="1" colspan="1">71.8</td><td align="center" valign="middle" rowspan="1" colspan="1">78.4</td><td align="center" valign="middle" rowspan="1" colspan="1">86.4</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>65.4</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>72.1</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">70.1</td><td align="center" valign="middle" rowspan="1" colspan="1">68.0</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>74.8</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">72.7</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Only3D</td><td align="center" valign="middle" rowspan="1" colspan="1">Softmax</td><td align="center" valign="middle" rowspan="1" colspan="1">66.4</td><td align="center" valign="middle" rowspan="1" colspan="1">60.2</td><td align="center" valign="middle" rowspan="1" colspan="1">62.4</td><td align="center" valign="middle" rowspan="1" colspan="1">74.8</td><td align="center" valign="middle" rowspan="1" colspan="1">65.1</td><td align="center" valign="middle" rowspan="1" colspan="1">78.8</td><td align="center" valign="middle" rowspan="1" colspan="1">83.6</td><td align="center" valign="middle" rowspan="1" colspan="1">59.5</td><td align="center" valign="middle" rowspan="1" colspan="1">59.3</td><td align="center" valign="middle" rowspan="1" colspan="1">64.5</td><td align="center" valign="middle" rowspan="1" colspan="1">67.1</td><td align="center" valign="middle" rowspan="1" colspan="1">69.4</td><td align="center" valign="middle" rowspan="1" colspan="1">67.6</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PAttNet</td><td align="center" valign="middle" rowspan="1" colspan="1">Sigmoid</td><td align="center" valign="middle" rowspan="1" colspan="1">59.5</td><td align="center" valign="middle" rowspan="1" colspan="1">59.6</td><td align="center" valign="middle" rowspan="1" colspan="1">67.6</td><td align="center" valign="middle" rowspan="1" colspan="1">76.0</td><td align="center" valign="middle" rowspan="1" colspan="1">66.1</td><td align="center" valign="middle" rowspan="1" colspan="1">75.9</td><td align="center" valign="middle" rowspan="1" colspan="1">81.5</td><td align="center" valign="middle" rowspan="1" colspan="1">63.0</td><td align="center" valign="middle" rowspan="1" colspan="1">65.6</td><td align="center" valign="middle" rowspan="1" colspan="1">67.1</td><td align="center" valign="middle" rowspan="1" colspan="1">64.6</td><td align="center" valign="middle" rowspan="1" colspan="1">68.1</td><td align="center" valign="middle" rowspan="1" colspan="1">67.9</td></tr><tr><td rowspan="2" align="left" valign="middle" colspan="1">D-PAttNet (2D + 3D)</td><td align="center" valign="middle" rowspan="1" colspan="1">Softmax</td><td align="center" valign="middle" rowspan="1" colspan="1">63.2</td><td align="center" valign="middle" rowspan="1" colspan="1">63.9</td><td align="center" valign="middle" rowspan="1" colspan="1">60.7</td><td align="center" valign="middle" rowspan="1" colspan="1">73.2</td><td align="center" valign="middle" rowspan="1" colspan="1">63.5</td><td align="center" valign="middle" rowspan="1" colspan="1">77.0</td><td align="center" valign="middle" rowspan="1" colspan="1">84.4</td><td align="center" valign="middle" rowspan="1" colspan="1">61.4</td><td align="center" valign="middle" rowspan="1" colspan="1">59.1</td><td align="center" valign="middle" rowspan="1" colspan="1">63.2</td><td align="center" valign="middle" rowspan="1" colspan="1">63.4</td><td align="center" valign="middle" rowspan="1" colspan="1">77.4</td><td align="center" valign="middle" rowspan="1" colspan="1">67.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Sigmoid</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>68.3</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>66.0</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>75.6</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>79.1</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>73.0</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>79.0</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>87.0</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">64.9</td><td align="center" valign="middle" rowspan="1" colspan="1">72.0</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>71.9</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>69.5</bold></td><td align="center" valign="middle" rowspan="1" colspan="1">74.5</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>73.4</bold></td></tr></tbody></table><table-wrap-foot><fn id="TFN7"><p id="P88">The best results are shown in bold.</p></fn></table-wrap-foot></table-wrap><table-wrap id="T6" position="float" orientation="landscape"><label>TABLE 6 |</label><caption><p id="P89">Significance of differences between classifiers (sigmoid and softmax) by <italic>t</italic>-test.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" valign="middle" rowspan="1" colspan="1"/><th align="center" valign="middle" rowspan="1" colspan="1"/><th align="center" valign="middle" rowspan="1" colspan="1">1</th><th align="center" valign="middle" rowspan="1" colspan="1">2</th><th align="center" valign="middle" rowspan="1" colspan="1">4</th><th align="center" valign="middle" rowspan="1" colspan="1">6</th><th align="center" valign="middle" rowspan="1" colspan="1">7</th><th align="center" valign="middle" rowspan="1" colspan="1">10</th><th align="center" valign="middle" rowspan="1" colspan="1">12</th><th align="center" valign="middle" rowspan="1" colspan="1">14</th><th align="center" valign="middle" rowspan="1" colspan="1">15</th><th align="center" valign="middle" rowspan="1" colspan="1">17</th><th align="center" valign="middle" rowspan="1" colspan="1">23</th><th align="center" valign="middle" rowspan="1" colspan="1">24</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Sigmoid &#x0003e;</td><td align="center" valign="middle" rowspan="1" colspan="1">F1</td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN9" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN9" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN9" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN9" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN9" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN9" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN9" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN9" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN9" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN9" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN9" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Softmax</td><td align="center" valign="middle" rowspan="1" colspan="1">AUC</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN9" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN9" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1">n.s.</td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN9" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN9" ref-type="table-fn">**</xref></td><td align="center" valign="middle" rowspan="1" colspan="1"><xref rid="TFN8" ref-type="table-fn">*</xref></td><td align="center" valign="middle" style="background-color:#939598" rowspan="1" colspan="1"><xref rid="TFN8" ref-type="table-fn">*</xref></td></tr></tbody></table><table-wrap-foot><fn id="TFN8"><label>*</label><p id="P90"><italic>p</italic> &#x0003c; <italic>0.05</italic>,</p></fn><fn id="TFN9"><label>**</label><p id="P91"><italic>p</italic> &#x0003c; <italic>0.05/12</italic>.</p></fn><fn id="TFN10"><p id="P92">The latter are significant after correcting for multiple comparisons. n.s., not significant. Cells denoted with gray color indicates cases where the results for softmax are greater than the ones for sigmoid.</p></fn></table-wrap-foot></table-wrap></floats-group></article>