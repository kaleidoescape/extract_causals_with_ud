<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">31924811</article-id><article-id pub-id-type="pmc">6954180</article-id><article-id pub-id-type="publisher-id">56437</article-id><article-id pub-id-type="doi">10.1038/s41598-019-56437-4</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Humans judge faces in incomplete photographs as physically more attractive</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Orghian</surname><given-names>Diana</given-names></name><address><email>dorghian@psicologia.ulisboa.pt</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6031-5982</contrib-id><name><surname>Hidalgo</surname><given-names>C&#x000e9;sar A.</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff4">4</xref><xref ref-type="aff" rid="Aff5">5</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2181 4263</institution-id><institution-id institution-id-type="GRID">grid.9983.b</institution-id><institution>CICPSI, Faculdade de Psicologia, Universidade de Lisboa, Alameda da Universidade, </institution></institution-wrap>Lisboa, 1649-013 Portugal </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2353 1689</institution-id><institution-id institution-id-type="GRID">grid.11417.32</institution-id><institution>ANITI Chair, University of Toulouse, 41 All&#x000e9;e Jules Guesde, </institution></institution-wrap>Toulouse, 31000 France </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000000121662407</institution-id><institution-id institution-id-type="GRID">grid.5379.8</institution-id><institution>Alliance Business School, </institution><institution>University of Manchester, </institution></institution-wrap>Booth St W, Manchester, M15 6PB United Kingdom </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution>School of Engineering and Applied Sciences, </institution><institution>Harvard University, </institution></institution-wrap>29 Oxford St, Cambridge, MA 02138 USA </aff><aff id="Aff5"><label>5</label>Datawheel, 1299 Cambridge Street, Cambridge, MA 02139 USA </aff></contrib-group><pub-date pub-type="epub"><day>10</day><month>1</month><year>2020</year></pub-date><pub-date pub-type="pmc-release"><day>10</day><month>1</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>10</volume><elocation-id>110</elocation-id><history><date date-type="received"><day>3</day><month>4</month><year>2019</year></date><date date-type="accepted"><day>5</day><month>12</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2020</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Attractive people are perceived to be healthier, wealthier, and more sociable. Yet, people often judge the attractiveness of others based on incomplete and inaccurate facial information. Here, we test the hypothesis that people fill in the missing information with positive inferences when judging others&#x02019; facial beauty. To test this hypothesis, we conducted seven experiments where participants judged the attractiveness of human faces in complete and incomplete photographs. Our data shows that&#x02014;relative to complete photographs&#x02014;participants judge faces in incomplete photographs as physically more attractive. This positivity bias is replicated for different types of incompleteness; is mostly specific to aesthetic judgments; is stronger for male participants; is specific to human faces when compared to pets, flowers, and landscapes; seems to involve a holistic processing; and is stronger for atypical faces. These findings contribute to our understanding of how people perceive and make inferences about others&#x02019; beauty.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Social behaviour</kwd><kwd>Human behaviour</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2020</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">Often, people judge the appearance of others using incomplete information. Such is the case when we see someone for the first time from far away or in poor light conditions. Similarly, most online encounters involve aesthetic judgements based on small, incomplete, or partly occluded profile pictures<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. These online personas are important. For example organizations are increasingly using social media (e.g., Facebook, LinkedIn) to gather information about job candidates and the inherent incompleteness of online images can therefore bias the first impressions of employers about potential employees<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Here, we conducted a series of experiments to explore how people infer facial attractiveness from incomplete, small, and blurry pictures. The results suggest that, under information shortage, people are positively biased when judging others&#x02019; facial attractiveness. This suggests that people fill in the missing information with optimistic inferences.</p><p id="Par3">Positive biased are common in the human cognition. People are known to perceive themselves in unrealistically positive ways<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. People believe they have more control over the environment than they in fact do<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, expect a better future than the one predicted from base-rates<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, and overestimate the prevalence of their own opinions<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. When comparing themselves with others, the so called &#x0201c;better-than-average&#x0201d; effect suggests that people perceive themselves as kinder, warmer, and sincerer than the average person<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. These positive illusions, or biases, have a self-serving role: promoting psychological well-being by creating a positive self-image. Yet, all of these effects describe biases that inflate people&#x02019;s self-perception. Could similar biases also affect people&#x02019;s perception of others?</p><p id="Par4">There are good reasons to believe that people may have unrealistic expectations when perceiving others as well. The literature has shown that people tend to have optimistic impression about others&#x02019; personalities when they have limited information about them<sup><xref ref-type="bibr" rid="CR7">7</xref>&#x02013;<xref ref-type="bibr" rid="CR9">9</xref></sup>. Moreover, the excitement of anticipating a first encounter can further amplify these positive expectations<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>. But do these biases also apply to physical appearance? Do we perceive others as better-looking when we are presented with incomplete information about their faces?</p><p id="Par5">Such a positivity bias can have profound implications on our social interactions. People that we perceive as more attractive are also perceived as more sociable<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, healthier and wealthier<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>, academically brighter<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, and as having more expertise<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> and better job qualifications<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. This halo-effect plays an important role not only on how we perceive others but also on how we behave towards them. Indeed, we tend to offer more help<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, imitate<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, and offer more attention and care<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> to people that we perceive as more attractive. This bias can have practical implication, such as getting milder court sentences<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>.</p><p id="Par6">In this manuscript we explore the following questions:<list list-type="order"><list-item><p id="Par7">Are people positively biased in their inferential mechanisms when judging other people&#x02019;s facial beauty?</p></list-item><list-item><p id="Par8">If such a positivity bias exists, is it specific to human faces or does it apply to other entities that people judge aesthetically, such as landscapes, flowers, and pets?</p></list-item><list-item><p id="Par9">Is it specific to aesthetic judgements, or does it carry into other evaluative dimensions (e.g., perception of warmness)?</p></list-item><list-item><p id="Par10">Is the bias stronger for one gender?</p></list-item><list-item><p id="Par11">Can this bias be disrupted? And</p></list-item><list-item><p id="Par12">What is the mechanism underlying such a positivity bias?</p></list-item></list></p><p id="Par13">In our first experiment, participants were instructed to judge the attractiveness of 96 human faces while being randomly assigned to one of four conditions (each corresponding to a different manipulation of facial photographs): small photographs (Small condition); photographs with only one-third of the face visible (One-third condition); blurred photographs (Blurred condition); or photographs with complete faces (Original condition). The first three conditions share the fact that they are missing information. We then compared the attractiveness ratings of the 96 faces across the four conditions, finding that participants judged, on average, small, one-third, and blurred faces as more attractive than their original counterparts. The version depicting one-third of the faces led to the largest positivity bias and the small size version led to the smallest (but still significant) bias. Moreover, we also show that the bias spills over &#x02014; albeit weakened &#x02014; to warmness and knowledgeableness judgments when the positivity bias for attractiveness is strong (One-third condition). In this experiment we also measured the mood of the participants as a way to show that the differences found between conditions cannot be attributed to differences in mood.</p><p id="Par14">In the second experiment we replicate the effect with two new modifications: half faces and a manipulation in which groups of pixels, accounting for a third of the total image, were randomly removed from the photographs. Also, we included perfectly symmetric faces by using mirror-reversed halves to create complete faces. We find that the positivity bias replicates for the half and the randomly incomplete versions, whereas perfectly symmetric faces were rated as less attractive than their original and half counterparts. This suggests that the positivity bias is not based on people assuming perfect symmetry.</p><p id="Par15">In the third experiment we show that the bias does not replicate for incomplete photographs of dog faces, landscapes, or flowers, which might suggest that the effect is specific to human faces.</p><p id="Par16">In the last four experiments we investigate the mechanism underlying the reported positivity bias. In experiment four we show that while the attractiveness judgements are sensitive to expectation, positive expectations do not lead to an increase in the bias. We also rule out expectations of similarity with the self by showing that participants do not perceive themselves as more similar to the people in the incomplete photographs (experiment five). We propose that typicality (i.e., the use of a prototypical face to fill in the missing information) can be one of the mechanisms responsible for this positivity bias (experiment six). Finally, we show that in situations where human faces are more difficult to be recognized as a face (i.e., when presented upside down) the use of a prototypical face to fill in the missing information is less likely and thus, the effect is disrupted (experiment seven).</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Positivity bias effect</title><p id="Par17">In the first experiment, 420 Mechanical Turk participants were presented with 96 photographs modified in one of the following ways: (1) photographs were kept in their Original format (400&#x02009;&#x000d7;&#x02009;400 pixels); (2) photographs were Blurred through the application of a 15 pixels radius Gaussian filter; (3) photographs had only One-third (the left side) of the faces visible; and (4) photographs were reduced to a Small size (50&#x02009;&#x000d7;&#x02009;50 pixels; see Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1A</xref>).<fig id="Fig1"><label>Figure 1</label><caption><p>Examples of stimuli used in Experiments 1 and 2. Examples of the four manipulations used in experiment one (Original, Blurred, One-third, and Small versions-<bold>A</bold>) and experiment two (Original, Incomplete, Half, and Mirror&#x02013;reversed-<bold>B</bold>). To satisfy the copyright policies of the journal, in this illustration we use an artificially generated face from the website <ext-link ext-link-type="uri" xlink:href="https://www.thispersondoesnotexist.com">https://www.thispersondoesnotexist.com</ext-link>, which uses generative adversarial networks or GANs (credited to Nvidia Corporation). However, in the experiments, we used real human faces from the website <ext-link ext-link-type="uri" xlink:href="https://www.facity.com">https://www.facity.com</ext-link>.</p></caption><graphic xlink:href="41598_2019_56437_Fig1_HTML" id="d29e426"/></fig></p><p id="Par18">All the participants were presented with the same 96 faces but they were randomly assignment to one of the four modifications. In each condition, for each photograph, participants were asked to judge how physically attractive, warm, and knowledgeable (always in this order) the people portrayed in the 96 photographs were. To give their answers, participants used a scale ranging from 1 (not at all) to 10 (very much). The response was self-paced and the mouse was used to indicate the corresponding number on the scale. At the end of the experiment participants, completed a mood scale &#x02013; the Positive and Negative Affect Scale<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>.</p><p id="Par19">For each of the 96 target-faces, the responses were aggregated across participants. To test if there is a positivity bias across the different modifications (Small, Blurred, and One-third) and different judgements (attractiveness, warmness, and knowledgeableness), we conducted a repeated measures ANOVA, using the average ratings of the faces as the dependent variable, and the type of judgement (attractiveness versus warmness vs. knowledgeableness) and the type of modification of the photograph (Original vs. One-third vs. Blurred vs. Small) as the two independent variables. The significant interaction found between the two independent variables, <italic>F</italic>(6, 90)&#x02009;=&#x02009;55.07, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, suggests that the three judgements were differently affected by the modification manipulation (see Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> for descriptive statistics).<table-wrap id="Tab1"><label>Table 1</label><caption><p>Descriptive statistics for all experiments.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Experiment</th><th>Modification</th><th><italic>M</italic></th><th><italic>SD</italic></th><th><italic>M difference</italic></th><th><italic>SE</italic></th><th><italic>95% CI</italic></th></tr></thead><tbody><tr><td rowspan="21">1 - Positivity Bias effect</td><td colspan="6"><bold>Aggregated Participants: attractiveness judgements</bold></td></tr><tr><td>Original</td><td>4.81</td><td>1.07</td><td/><td/><td/></tr><tr><td>Small</td><td>5.06</td><td>1.01</td><td>0.25***</td><td>0.06</td><td>[0.13, 0.36]</td></tr><tr><td>Blurred</td><td>5.27</td><td>1.07</td><td>0.46***</td><td>0.06</td><td>[0.35, 0.57]</td></tr><tr><td>One-third</td><td>5.73</td><td>1.19</td><td>0.92***</td><td>0.05</td><td>[0.81, 1.02]</td></tr><tr><td colspan="6"><bold>Male Participants: attractiveness judgements</bold></td></tr><tr><td>Original</td><td>4.77</td><td>1.09</td><td/><td/><td/></tr><tr><td>One-third</td><td>5.72</td><td>1.14</td><td>0.95***</td><td>0.05</td><td>[0.85, 1.06]</td></tr><tr><td colspan="6"><bold>Female Participants: attractiveness judgements</bold></td></tr><tr><td>Original</td><td>4.87</td><td>1.09</td><td/><td/><td/></tr><tr><td>One-third</td><td>5.74</td><td>1.24</td><td>0.87***</td><td>0.06</td><td>[0.75, 0.98]</td></tr><tr><td colspan="6"><bold>Aggregated Participants: warmness judgements</bold></td></tr><tr><td>Original</td><td>5.19</td><td>0.83</td><td/><td/><td/></tr><tr><td>Small</td><td>5.01</td><td>0.72</td><td>&#x02212;0.18***</td><td>0.05</td><td>[&#x02212;0.28, &#x02212;0.09]</td></tr><tr><td>Blurred</td><td>5.09</td><td>0.88</td><td>&#x02212;0.10**</td><td>0.04</td><td>[&#x02212;0.18, &#x02212;0.01]</td></tr><tr><td>One-third</td><td>5.58</td><td>0.88</td><td>0.38***</td><td>0.04</td><td>[0.30, 0.47]</td></tr><tr><td colspan="6"><bold>Aggregated Participants: knowledgeableness judgements</bold></td></tr><tr><td>Original</td><td>5.89</td><td>0.54</td><td/><td/><td/></tr><tr><td>Small</td><td>5.79</td><td>0.47</td><td>&#x02212;0.10**</td><td>0.04</td><td>[&#x02212;0.17, &#x02212;0.03]</td></tr><tr><td>Blurred</td><td>5.7</td><td>0.53</td><td>&#x02212;0.18**</td><td>0.03</td><td>[&#x02212;0.25, &#x02212;0.12]</td></tr><tr><td>One-third</td><td>6.3</td><td>0.53</td><td>0.41***</td><td>0.03</td><td>[0.36, 0.47]</td></tr><tr><td rowspan="4">2 - Replication</td><td>Original</td><td>48.4</td><td>11.42</td><td/><td/><td/></tr><tr><td>Half</td><td>50.45</td><td>11.83</td><td>2.05***</td><td>0.45</td><td>[1.15, 2.94]</td></tr><tr><td>Incomplete</td><td>51.31</td><td>10.34</td><td>2.91***</td><td>0.4</td><td>[2.11, 3.70]</td></tr><tr><td>Mirror-reversed</td><td>38.28</td><td>12.66</td><td>&#x02212;10.12***</td><td>0.62</td><td>[&#x02212;11.34, &#x02212;8.90]</td></tr><tr><td rowspan="9">3 - Specific to Human Faces</td><td colspan="6"><bold>Dog Faces</bold></td></tr><tr><td>Original</td><td>71.03</td><td>8.47</td><td/><td/><td/></tr><tr><td>Incomplete</td><td>67</td><td>7.18</td><td>&#x02212;4.02***</td><td>0.82</td><td>[&#x02212;5.65, &#x02212;2.40]</td></tr><tr><td colspan="6"><bold>Landscapes</bold></td></tr><tr><td>Original</td><td>55.62</td><td>15.44</td><td/><td/><td/></tr><tr><td>Incomplete</td><td>54.44</td><td>14.65</td><td>&#x02212;1.18</td><td>0.82</td><td>[&#x02212;2.88, 0.44]</td></tr><tr><td colspan="6"><bold>Flowers</bold></td></tr><tr><td>Original</td><td>66.5</td><td>10.68</td><td/><td/><td/></tr><tr><td>Incomplete</td><td>62.7</td><td>10.61</td><td>&#x02212;3.8***</td><td>0.82</td><td>[&#x02212;5.43, &#x02212;2.18]</td></tr><tr><td>4 &#x02013; Sensitivity to</td><td colspan="6"><bold>No Expectations</bold></td></tr><tr><td>Expectations</td><td>Original</td><td>41.2</td><td>11.61</td><td/><td/><td/></tr><tr><td/><td>Incomplete</td><td>48.28</td><td>10.71</td><td>7.08***</td><td>0.45</td><td>[6.18, 7.98]</td></tr><tr><td/><td colspan="6"><bold>High Expectations</bold></td></tr><tr><td/><td>Original</td><td>47.51</td><td>11.87</td><td/><td/><td/></tr><tr><td/><td>Incomplete</td><td>50.51</td><td>11.16</td><td>3***</td><td>0.39</td><td>[2.21, 3.78]</td></tr><tr><td/><td colspan="6"><bold>Low Expectations</bold></td></tr><tr><td/><td>Original</td><td>43.01</td><td>12.41</td><td/><td/><td/></tr><tr><td/><td>Incomplete</td><td>44.98</td><td>9.84</td><td>1.97***</td><td>0.49</td><td>[1.00, 2.93]</td></tr><tr><td rowspan="2">5 &#x02013; Ruling out Similarity</td><td>Original</td><td>33.8</td><td>4.14</td><td/><td/><td/></tr><tr><td>Incomplete</td><td>34.06</td><td>2.76</td><td>0.25</td><td>0.23</td><td>[&#x02212;0.20, 0.71]</td></tr><tr><td rowspan="6">6 &#x02013; The role of Typicality</td><td colspan="6"><bold>Aggregated typical (experiments two and four)</bold></td></tr><tr><td>Original</td><td>52.95</td><td>8.86</td><td/><td/><td/></tr><tr><td>Incomplete</td><td>55.17</td><td>8.47</td><td>2.22***</td><td>0.47</td><td>[1.28, 3.15]</td></tr><tr><td colspan="6"><bold>Aggregated atypical (experiments two and four)</bold></td></tr><tr><td>Original</td><td>39.55</td><td>9.52</td><td/><td/><td/></tr><tr><td>Incomplete</td><td>44.97</td><td>9.82</td><td>5.42***</td><td>0.47</td><td>[4.48, 6.35]</td></tr><tr><td rowspan="9">7 &#x02013; Disrupting the positivity bias</td><td colspan="6"><bold>Upright</bold></td></tr><tr><td>Original</td><td>45.18</td><td>11.44</td><td/><td/><td/></tr><tr><td>Incomplete</td><td>48.72</td><td>10.64</td><td>3.54***</td><td>0.4</td><td>[2.73, 4.34]</td></tr><tr><td colspan="6"><bold>90-degree-rotated</bold></td></tr><tr><td>Original</td><td>49.63</td><td>11.48</td><td/><td/><td/></tr><tr><td>Incomplete</td><td>49.94</td><td>8.53</td><td>0.31</td><td>0.48</td><td>[&#x02212;0.63, 1.26]</td></tr><tr><td colspan="6"><bold>Inverted</bold></td></tr><tr><td>Original</td><td>50.11</td><td>11.37</td><td/><td/><td/></tr><tr><td>Incomplete</td><td>50.09</td><td>10.39</td><td>&#x02212;0.02</td><td>0.53</td><td>[&#x02212;1.06, 1.02]</td></tr></tbody></table><table-wrap-foot><p>Means, Standard Deviations, Means of differences, Standard Error on the Means, and Confidence Intervals on the Means, as a function of the conditions in all seven experiments. **Stands for p value&#x02009;=&#x02009;&#x0003c;0.05 and ***p value&#x02009;=&#x02009;&#x0003c;0.001.</p></table-wrap-foot></table-wrap></p><p id="Par20">Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> illustrates the positivity bias found for the One-third condition. In this condition, incomplete faces were rated&#x02014;on average&#x02014;almost an entire point higher on the ten points scale than their respective original versions (<italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;0.92, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001). In the figure, we plotted the difference between the ratings in each of the three incomplete conditions and the ratings in the Original condition as our measure of attractiveness bias. The figure also shows that the bias is as large as two points on the scale in the strongest cases and non-existent in a handful of cases.<fig id="Fig2"><label>Figure 2</label><caption><p>Positivity bias found in Experiment 1. The ratings for the Original faces (x axis) are plotted against the magnitude of the bias (y axis). Each dot represents one of the 96 faces.</p></caption><graphic xlink:href="41598_2019_56437_Fig2_HTML" id="d29e1357"/></fig></p><p id="Par21">Participants also rated faces as less attractive in the Original condition than in the Small conditions, <italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;0.25, p&#x02009;&#x0003c;&#x02009;0.001, or in the Blurred condition, <italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;0.46, p&#x02009;&#x0003c;&#x02009;0.001. Among the three conditions, the One-third condition led to the largest positivity bias and the Small modification led to the smallest bias.</p><p id="Par22">For warmness and knowledgeableness judgements, a negativity bias was found in the Small and Blurred conditions, since the ratings were larger for the Original faces than for the Small faces (warmness: <italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;&#x02212;0.18, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001; knowledgeableness: <italic>M</italic><sub><italic>differenc</italic>e</sub>&#x02009;=&#x02009;&#x02212;0.10, <italic>p</italic>&#x02009;=&#x02009;0.008) or the Blurred faces (warmness: <italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;&#x02212;0.10, <italic>p</italic>&#x02009;=&#x02009;0.024; knowledgeableness: <italic>M</italic><sub><italic>differenc</italic>e</sub>&#x02009;=&#x02009;&#x02212;0.18, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001). However, the ratings in the One-third condition were larger than in the Original condition, (warmness: <italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;0.38, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001; knowledgeableness: <italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;0.41, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001), meaning that the positivity bias found for attractiveness generalizes to warmness and knowledgeableness in this case.</p><p id="Par23">Although we know, from previous studies, that men and women usually agree on attractiveness evaluations<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, we asked whether the positivity bias is stronger for male or female participants and whether it is affected by the gender of the person being evaluated. To answer this question, we calculated the average attractiveness ratings provided by male and female participants to faces of women and men in the Original and One-third conditions. We found a small but significant interaction between the modification of the face and the gender of the participants, <italic>F</italic>(1,94)&#x02009;=&#x02009;4.87, <italic>p</italic>&#x02009;=&#x02009;0.03. This interaction suggests that male participants exhibit a slightly stronger positivity bias, <italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;0.95, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, than female participants, <italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;0.87, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001. No effect of the gender of the face being evaluated was found.</p><p id="Par24">We also compared the scores on the mood scale for the four conditions to assure that the differences found are not due to differences in the participants&#x02019; mood. One could argue that the effect could be a consequence of participants in the incomplete conditions enjoying more the task which could lead to more positive evaluations of the faces. Such an argument is consistent with the literature that shows hedonic states following interruptions or uncertain situations<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>.</p><p id="Par25">Two mixed effects ANOVAs were conducted, with the modification being the independent variable and the ratings to the Positive and the Negative Affect Scales being the two dependent variables. For the Positive Scale, there was no significant effect of the type of modification, <italic>F</italic>(3, 413)&#x02009;=&#x02009;0.853, <italic>p</italic>&#x02009;=&#x02009;0.466, and the same is true for the Negative Scale, <italic>F</italic>(3, 416)&#x02009;=&#x02009;0.691, <italic>p</italic>&#x02009;=&#x02009;0.588. This result suggests that there is no reason to believe that the incompleteness of the photographs led to differences in participants&#x02019; mood.</p><p id="Par26">The results of this first experiment support our hypothesis that people are positively biased when judging other people&#x02019;s facial attractiveness under information shortage. Yet, this first experiment has limitations. The Blurred and the Small versions are likely to lead to objectively more attractive faces since facial imperfections, such as pimples or wrinkles, are less visible. In Experiment 2 we try to overcome this limitation by creating a new incomplete version of the photographs in which groups of pixels are eliminated at random.</p><p id="Par27">291 Mechanical Turk workers took part in the second experiment. To create the material for the new incomplete condition, we divided each original photograph (400&#x02009;&#x000d7;&#x02009;400 pixels) in 400 squares of 20 by 20 pixels each and eliminated randomly a set of 150 squares from the total of 400 squares (this modification will be called Incomplete from now on). This process was repeated 100 times for each face. Two other versions were created for this experiment: Half-faces (as opposed to the One-third from Experiment 1) and Mirror-reversed symmetric faces. For the Half-face condition, as the name indicates, we cut the faces in two halves. This was done by using the equidistant point between the eyes, the central axis of the nose, and the upper lip as references. Additionally, for each face, we used these halves to create symmetric faces by combining one half face with its mirror-reversed version (see Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1B</xref> for an example).</p><p id="Par28">Participants were assigned to one of four conditions: Original, Half, Mirror-reversed, and Incomplete. In the Incomplete condition, for each face (and individually for each participant), an incomplete version of the face was drawn at random from the set of 100 different incomplete versions. This procedure ensures that the obtained results are not an artifact of occluding a specific facial feature in the incomplete version, because the features shown or hidden vary at random across participants. This time, participants made only attractiveness judgements and, for that, they used a scale ranging from zero (very unattractive) to 100 (very attractive).</p><p id="Par29">Again, we conducted a repeated measures ANOVA to test for differences across the multiple conditions (Original vs. Incomplete vs. Half vs. Mirror-reversed). We found a main effect of modification, <italic>F</italic>(3, 93)&#x02009;=&#x02009;243.17, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, meaning the attractiveness ratings varied significantly across conditions. The Original faces received lower ratings than Half-faces, <italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;2.05, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, and Incomplete faces, <italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;2.91, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, meaning the positivity bias was replicated for these new incomplete conditions. Perfectly symmetric faces, on the other hand, received ratings that were significantly lower than their Original (<italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;&#x02212;10.12, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001; see Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3A</xref>) and their Half-face counterparts (<italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;&#x02212;12.65, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001). The fact that participants rated differently perfectly symmetric faces and half-faces suggests that the process taking place in the Half-face condition is probably not based on inferring perfect symmetry (inferring the missing half from the half provided; see Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> for means and standard deviations).<fig id="Fig3"><label>Figure 3</label><caption><p>Examples of the stimuli and the manipulations in Experiment 7.</p></caption><graphic xlink:href="41598_2019_56437_Fig3_HTML" id="d29e1556"/></fig><table-wrap id="Tab2"><label>Table 2</label><caption><p>Description of the sample in each experiment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Exp. 1</th><th>Exp. 2</th><th>Exp. 3</th><th>Exp. 4</th><th>Exp. 5</th><th>Exp. 6</th><th>Exp. 7</th></tr></thead><tbody><tr><td>Sample size</td><td>417</td><td>289</td><td>205</td><td>406</td><td>223</td><td>145</td><td>413</td></tr><tr><td>Average age</td><td>33.4</td><td>32.02</td><td>32.04</td><td>32</td><td>32.01</td><td>31.39</td><td>32.12</td></tr><tr><td>SD age</td><td>7.39</td><td>6.97</td><td>6.54</td><td>7.61</td><td>6.61</td><td>6.01</td><td>7.59</td></tr><tr><td>Females</td><td>217</td><td>126</td><td>97</td><td>202</td><td>77</td><td>56</td><td>199</td></tr><tr><td>White-Americans</td><td>310</td><td>213</td><td>157</td><td>296</td><td>149</td><td>95</td><td>293</td></tr><tr><td>African-Americans</td><td>45</td><td>30</td><td>20</td><td>38</td><td>33</td><td>20</td><td>44</td></tr><tr><td>Asian-Americans</td><td>31</td><td>16</td><td>12</td><td>35</td><td>9</td><td>16</td><td>33</td></tr><tr><td>Hispanic-Americans</td><td>24</td><td>25</td><td>12</td><td>29</td><td>24</td><td>7</td><td>33</td></tr><tr><td>Native-Americans</td><td>3</td><td>0</td><td>0</td><td>1</td><td>5</td><td>5</td><td>3</td></tr><tr><td>Others</td><td>4</td><td>5</td><td>4</td><td>9</td><td>3</td><td>2</td><td>7</td></tr><tr><td>Eliminated (attention-check)</td><td>3</td><td>2</td><td>2</td><td>16</td><td>6</td><td>6</td><td>11</td></tr><tr><td>Compensations (in dollars)</td><td>4</td><td>2.50</td><td>2.50</td><td>2.50</td><td>1.70</td><td>1.70</td><td>2.50</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec4"><title>Specific to human faces</title><p id="Par30">In the third experiment we used photographs of dog faces, flowers, and landscapes to test whether the positivity bias observed in Experiments 1 and 2 is also observed in these categories or whether it is specific to human faces. Dog faces are especially relevant because they are structurally similar to human faces in the sense that they have similar elements (eyes, nose, and mouth).</p><p id="Par31">We had 28 photographs for each of the three categories (dogs, flowers, and landscapes) and we also generated 100 incomplete versions for each photograph through a procedure equivalent to the one used in Experiment 2. Dog faces were collected from Google using the key words: &#x0201c;dog faces on white background&#x0201d;. The landscapes and flowers were collected from McGill Calibrated Color Image Database<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. The photos were then cropped to preserve only the area of interest (the face for the dogs and the flower for the plants). The photographs were centered and resize to 350 by 350 pixels.</p><p id="Par32">207 Mechanical Turk participants were assigned to one of two conditions: Original or Incomplete photographs. For dog faces, participants were asked &#x0201c;how cute is the dog?&#x0201d;, for flowers &#x0201c;how beautiful is the flower?&#x0201d;, and for landscapes &#x0201c;how attractive is the scenery?&#x0201d;. All participants rated the dogs, the flowers, and the landscapes, in blocks. The orders of the blocks and the photographs within each block were randomized for each participant. To give their answers, participants rated the photographs on a scale from zero (not at all) to 100 (very much).</p><p id="Par33">A mixed effects ANOVA revealed an interaction between the category of the stimulus and the modification, <italic>F</italic>(2, 81)&#x02009;=&#x02009;3.73, <italic>p</italic>&#x02009;=&#x02009;0.028, indicating that the bias was different for the three categories. For dog faces, the ratings given to the Incomplete photographs were lower than the ratings given to the Original photographs (<italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;&#x02212;4.02, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001) and a similar negativity bias was detected for flowers (<italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;&#x02212;3.80, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001). No bias was found for landscapes (<italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;&#x02212;1.18, <italic>p</italic>&#x02009;=&#x02009;0.151). These results show that the positivity bias found for human faces does not generalize to dog faces, landscapes, and flowers. This result also agrees with past research, including Sear&#x02019;s seminal paper<sup><xref ref-type="bibr" rid="CR9">9</xref></sup> about person-positivity bias, where the author argues that stimuli are evaluated more favorably the more they resemble individual human beings.</p></sec><sec id="Sec5"><title>Sensitivity to expectation</title><p id="Par34">In the fourth experiment we measure whether the positivity bias is sensitive to the perceiver&#x02019;s expectation regarding the target-faces that are being evaluated. If the positivity bias occurs due to positive expectations in the incomplete condition, by telling participants that other participants evaluated the target-faces as highly attractive should enlarge the positive expectations in incomplete photographs and increase the effect. Similarly, telling participants that the target-faces were previously evaluated by others as less attractive should decrease the use of positive expectations and thus disrupt the effect.</p><p id="Par35">424 Mechanical Turk participants evaluated photographs either in the Original or the Incomplete condition (with the random elimination of pixels as described in Experiment 2). The expectation manipulation consisted of three levels: High-Expectation, No-Expectation, and Low-Expectation. In the No-Expectation condition, no information was given regarding the beauty of the target. In the other two conditions, participants were told that only faces rated as above average (or below average) by other workers would be presented to them. Participants were assigned to one of six conditions: Incomplete or Original faces, with high, low, or no expectations.</p><p id="Par36">The repeated measures ANOVA suggests that the effect of expectation was significant, <italic>F</italic>(2, 94)&#x02009;=&#x02009;484.01, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, meaning that the ratings are overall higher in the High-Expectation condition (<italic>M</italic><sub><italic>High-Expectation</italic></sub>&#x02009;=&#x02009;49.01, <italic>SD</italic><sub><italic>High-Expectation</italic></sub>&#x02009;=&#x02009;11.36) than in the No-Expectation condition (<italic>M</italic><sub><italic>No-Expectation</italic></sub>&#x02009;=&#x02009;44.74, <italic>SD</italic><italic>No-Expectation</italic>&#x02009;=&#x02009;10.94), <italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;4.27, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, and they are higher in the No-Expectation condition in comparison to the Low-Expectation condition, <italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;0.74, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001 (<italic>M</italic><sub><italic>Low-Expectation</italic></sub>&#x02009;=&#x02009;44.00, <italic>SD</italic><sub><italic>Low-Expectation</italic></sub>&#x02009;=&#x02009;10.94). These results suggest that participants&#x02019; judgements were sensitive to the expectation manipulation. The positivity bias was also replicated in this experiment. It was the strongest in the No-Expectation condition, <italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;7.08, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, reduced in the High-Expectation condition, <italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;3.00, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, and reduced even further in the Low-Expectation condition, <italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;1.97, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001. The differences in positivity bias across conditions were also significant (<italic>M</italic><sub><italic>difference between no-expectation and high-expectation</italic></sub>&#x02009;=&#x02009;4.09, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, and <italic>M</italic><sub><italic>difference between high-expectation and low-expectation</italic></sub>&#x02009;=&#x02009;1.03, <italic>p</italic>&#x02009;=&#x02009;0.013).</p><p id="Par37">These results show that positive expectations, while increasing the overall evaluations of the faces, do not increase the bias, instead they decrease the bias. Low expectations also did not eliminate the effect, only reduced it. Hence, we conclude that expectations are not the main explaining mechanism underlying the positivity bias.</p><p id="Par38">This procedure of priming expectations also reduces the ambiguity that is experienced by participants in the incomplete condition and that might have contributed to the reduction of the bias. Reducing ambiguity is expected to reduce the effect (i.e., the difference between the Incomplete and the Original faces) through a recalibration of the ratings towards the expectation induced. Our rationale is that in the condition with no-expectation, no external information is given about the attractiveness of the targets and thus, the magnitude of the bias can be freely expressed in participants&#x02019; evaluations. In other words, expectations restricted the amplitude within which the cognitive bias is operating period.</p></sec><sec id="Sec6"><title>Ruling out similarity</title><p id="Par39">In the fifth experiment we test the hypothesis of whether similarity to the self could be the mechanism underlying the positivity bias. Similarity has been shown to account for positivity biases towards others in some contexts; such is the case of the research conducted by Sear<sup><xref ref-type="bibr" rid="CR9">9</xref></sup> and Norton <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. When the information about a target is ambiguous or incomplete, people erroneously perceive the targets as more similar to themselves, causing an increase in liking. If a similar mechanism is happening in the condition with incomplete faces, then we should observe higher ratings of perceived similarity in the incomplete than in the original photographs.</p><p id="Par40">223 Mechanical Turk participants evaluated the 96 faces after being assigned to one of two conditions: Original or Incomplete condition. For each photograph they were instructed to indicate how similar is the person&#x02019;s face to their own. To give their answers, participants rated the photographs on a scale from zero (not similar at all) to 100 (very similar).</p><p id="Par41">The similarity ratings for faces in the incomplete condition were not significantly different (<italic>M</italic>&#x02009;=&#x02009;34.06, <italic>SD</italic>&#x02009;=&#x02009;2.75) from the ratings of the original photographs (<italic>M</italic>&#x02009;=&#x02009;33.80, <italic>SD</italic>&#x02009;=&#x02009;4.14), <italic>t</italic>(95)&#x02009;=&#x02009;1.11, <italic>p</italic>&#x02009;=&#x02009;0.271. Although, this conclusion is based on a null effect, the result suggests that the two conditions do not vary in how similar participants rate the targets to the self.</p></sec><sec id="Sec7"><title>The role of typicality</title><p id="Par42">When presented with incomplete information, people infer the missing pieces based on a combination of contextual inputs and knowledge from similar past experiences. When reconstructing information regarding an acquaintance, people can fill in the blanks with memories of past interactions with that person. But, how do people fill in the missing information of a stranger that they meet for the first time? In such situations, the inference will rely on a more general visual representation. One possibility is that this representation is a typical face that people have stored in their memories as a result of their extensive exposure to human faces. If that is the case, since average/typical faces are perceived to be more attractive<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>, the resulting inference will reflect a positivity bias (the incomplete faces will be perceived as more attractive than the complete faces).</p><p id="Par43">If typicality does play a role in the positivity bias, then the magnitude of the positivity bias (i.e., the differences in the attractiveness ratings between original and incomplete photographs) is expected to be larger for atypical faces, since they are being completed based on a more attractive typical internal representation, than for incomplete typical faces, for which the rating will be more similar to attractiveness ratings attributed to the original versions. In other words, by completing the missing information of the incomplete untypical faces based on a prototypical representation, participants are sourcing elements from a face that is known to be on average more attractive. Thus, in the sixth experiment we explore the role of typicality in the positivity bias.</p><p id="Par44">145 Mechanical Turk participants were asked to rate the typicality/distinctiveness of the 96 original photographs used in the previous experiments. The photographs were paired with the question &#x0201c;How much does this face deviate from a typical face?&#x0201d; Participants provided their answer on a scale from zero (does not deviate at all) to 100 (deviates very much). Lower rating on this scale mean the face is considered more typical.</p><p id="Par45">These ratings were then used to investigate the positivity bias in typical versus untypical faces, which we did by comparing the perceived attractiveness of the original versus the incomplete faces given their typicality level.</p><p id="Par46">We used the median of the distinctiveness ratings to split the faces into two groups: typical and atypical. These groups were used as an independent variable together with the modification (original versus incomplete photograph) and the experiment (Experiments 2 and 4) in a mixed effects ANOVA. The dependent variable was the attractiveness ratings of the 96 target photographs. In this analysis, we used the attractiveness ratings of the original and incomplete faces from the experiments 2 and 4. These were experiments with similar design and identical modification of the photographs (from Experiment 2 only the original and the incomplete conditions were used and from Experiment 4 only the no-expectation condition was included in the analysis).</p><p id="Par47">A significant effect of modification was found, <italic>F</italic>(1,94)&#x02009;=&#x02009;130.869, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, with higher attractiveness ratings for the incomplete (<italic>M</italic>&#x02009;=&#x02009;50.07, <italic>SD</italic>&#x02009;=&#x02009;10.46) than for the original photographs (<italic>M</italic>&#x02009;=&#x02009;46.35, <italic>SD</italic>&#x02009;=&#x02009;11.36). This result replicates the patterns found in previous experiments. A strong interaction between the modification of the faces and the typicality variable was also observed, <italic>F</italic>(1, 94)&#x02009;=&#x02009;23.00, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001. As expected, a larger positivity bias was found for the atypical faces (<italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;5.416, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001) than for typical faces (<italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;2.216, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001). We also conducted a partial correlation between typicality and the attractiveness of the incomplete faces while controlling for the attractiveness of the original faces. A significant moderate correlation was found, <italic>r</italic>(93)&#x02009;=&#x02009;&#x02212;0.407, <italic>n</italic>&#x02009;=&#x02009;96, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001. These results are indicative of the role of typicality in the positivity bias effect.</p></sec><sec id="Sec8"><title>Disrupting the positivity effect</title><p id="Par48">In the seventh and last experiment, we test whether the positive bias can be disrupted. There is evidence in the literature that judgments of facial attractiveness rely on holistic representations of human faces<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Thus, we hypothesized that the positivity bias found for attractiveness judgements of incomplete faces will also depend on holistic processing. If this is true, then, we should be able to disrupt the positivity bias by disrupting the holistic processing of faces. Inverted (up-side-down) faces have been shown to disrupt holistic processing<sup><xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup> (but see<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>), so we created conditions with inverted faces to test this hypothesis. Moreover, disrupting the holistic processing is known to affect other types of face processing tasks such as face recognition<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, race categorization<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, and emotional expression recognition<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, among others. One possibility is that, by disrupting the holistic processing of the target-faces, participants are less successful in using the typical face to fill in the missing information, and as such, the positivity effect will not be observed anymore. In agreement with this hypothesis, judgements of distinctiveness or typicality were shown to be highly affected when the faces are inverted<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. On the same note, Dimond and Carey<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> proposed in 1989 that with experience, people develop fine-tuned prototypes of faces (or any other stimuli as long as a certain level of expertise is reached) that help them to encode configurational information in faces. If that is the case, then inverting the faces might disrupt the use of this prototypical spatial configuration.</p><p id="Par49">422 Mechanical Turk participants took part in this experiment. The material was the same material as in the previous experiment (Original and randomly generated Incomplete versions) plus four additional versions of the 96 faces: photographs rotated 90 degrees clockwise and their corresponding Incomplete versions (100 randomly incomplete photographs for each rotated face), and 96 Inverted photographs (180 degrees rotation) and their corresponding Incomplete versions (see Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3B</xref> for an example).</p><p id="Par50">Participants judged the attractiveness of the faces on a scale from zero (not attractive at all) to 100 (very attractive). The two independent variables in this experiment were the modification with two levels (Original vs. Incomplete) and the orientation of the photographs with three levels (Upright vs. 90-degree-rotated vs. Inverted).</p><p id="Par51">The interaction found between modification and rotation, <italic>F</italic>(2, 94)&#x02009;=&#x02009;36.45, <italic>p</italic>&#x02009;=&#x02009;0.028, reflects the presence of the positivity bias for the Upright photographs (<italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;3.54, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001), and the lack of bias for the 90-degree rotated (<italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;0.31, <italic>p</italic>&#x02009;=&#x02009;0.511) and Inverted photographs (<italic>M</italic><sub><italic>difference</italic></sub>&#x02009;=&#x02009;&#x02212;0.02, <italic>p</italic>&#x02009;=&#x02009;0.968; see Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>).<fig id="Fig4"><label>Figure 4</label><caption><p>Positivity bias in Experiment 7. The positivity bias in the Upright condition (<bold>A</bold>) and the absence of the bias in the Inverted condition (<bold>B</bold>). The histograms correspond to the differences between the incomplete and the original versions in Experiment 7.</p></caption><graphic xlink:href="41598_2019_56437_Fig4_HTML" id="d29e2290"/></fig></p><p id="Par52">This experiment shows that by inverting the faces the positivity bias is disrupted, which support our hypothesis that in the inverted condition the typicality is less likely to be used to fill in the missing information.</p></sec></sec><sec id="Sec9" sec-type="discussion"><title>Discussion</title><p id="Par53">We often judge others based on their physical appearance. Such judgments are driven by inferential mechanisms that help us fill in missing information. Here, we showed that (i) the inferential mechanism that we use to judge the physical appearance of human faces is positively biased, (ii) the bias is more pronounced in male participants, (iii) is specific to aesthetic judgments, but generalizes to other dimensions when the bias is strong enough, (iv) seems to be specific to human faces when compared to dog faces, landscapes, and flowers, and (v) is driven by the use of a holistic representation of what is a typical/average face. We also ruled out similarity to the self, positive expectations, and mood differences as explanatory mechanisms for the effect.</p><p id="Par54">Presented with an incomplete human faces and instructed to judge their attractiveness, participants resort to what they know about faces (structure and features) and their representation of a prototypical face to generate new holistic representations. An inferential process that stems from matching the type of stimuli &#x02013; i.e., human faces &#x02013; with a prototype already existent in their memories. While incomplete human faces lead to an overall positive bias effect, stimuli such landscapes, pets, and flowers showed not positivity bias, which is likely due to the absence of a clear&#x000a0;prototypical representations of these stimuli in people&#x02019;s memories. Although our experiments suggest that typicality may have a role in the attractiveness positivity effect, further and more direct evidence is necessary to prove the robustness of this relationship. If typicality does play a relevant role, is also important to better understand how is this prototypical representation created and what are exactly the past experiences that shape it.</p><p id="Par55">While the hypothesis that people fill in the missing pieces with positive inferences was never explicitly raised and tested, Saegusa and Watanabe stumbled on similar findings while investigating other phenomena. In their research on how information from individual facial parts contributes to the judgements of whole-face attractiveness over time, they found that attractiveness was higher for independent facial parts (e.g., eye, mouth) than for whole-faces<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. Another study found that, on average, back-view photographs were rated as more attractive than front-view photographs<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. The back-view condition can be seen as an extreme case of our incomplete treatments, in which the only information provided about the person is the shape of the head and the hair type, color, cut, and length. On a similar note, Miyazaki and Kawahara<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> in an attempt to look into how the use of sanitary-masks by the Japanese women affects people&#x02019;s perception of their beauty and health, found that certain types of occlusions also lead to higher perceived attractiveness, but only for originally unattractive faces judgements. Finally, Lu and collaborators<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> manipulated the amount of information and attractiveness of cartoon characters (computer generated, gouache, and stick-figures), with the purpose of studying gender difference in attractiveness judgements. However, no significant differences were found between attractiveness judgements of the three types of cartoons. Overall, these findings support our hypothesis: when perceiving incomplete faces people fill in the missing information with positive details. Also, noteworthy, but in a domain different from that of facial perception, the work by Norton and colleagues<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> showed that people perceive others&#x02019; personalities more favorably when they are provided with fewer personality traits as opposed to many.</p><p id="Par56">Being positively biased about the attractiveness of strangers might have been a mechanism evolutionarily selected, as it might have facilitated social and reproductive events. However, the impact of this bias might only apply to impressions and interactions in first encounters. It is known that first impressions get diluted as we get to know and acquire more information about a person<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Thus, an interesting question for future research is the influence of the positivity bias on subsequent interactions with the target-person.</p><p id="Par57">Whether the effect is unique to human faces also requires further research. More homogeneous categories than the ones we used need to be submitted to the same analysis to reach a more robust conclusion regarding the specificity of the positivity bias effect.</p><p id="Par58">The contribution of face symmetry should also be studies in more detail. A meta-analysis performed by Rhodes in 2006<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> tells us that symmetric faces are perceived as more attractive when they result from blending the original and mirror-reversed images, but they are not when they are &#x0201c;chimeras&#x0201d; (pure mirror-reversed with no blending). Pure mirror-reversed photographs lead to less attractive exemplars due to enlargement or reduction of the mid-line features<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. In our second experiment, we used chimeras because we wanted to understand if one half of the face is used to infer the missing half, but it would be interesting to test whether using a blended symmetric face (and thus more naturally looking) would lead to a similar conclusion.</p><p id="Par59">One limitation of our work is that all experiments were performed online with Mechanical Turk participants. While there is research showing that data from online experiments is comparable to data from lab-based experiments<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup>, these conclusions need to be replicated in the laboratory and in contexts where the implications of the research might be directly relevant (e.g., social media, recruitment, fashion industry, entertainment, advertisement, and marketing).</p></sec><sec id="Sec10"><title>Methods</title><p id="Par60">This research was approved by the MIT Committee of the Use of Humans as Experimental Subjects (Protocol # 1701822572). All the reported experiments were performed in accordance with the Federal regulations, 45 CFR Part 46.101(b)(2).</p><sec id="Sec11"><title>Participants</title><p id="Par61">The participants were all Mechanical Turk workers. They were compensated for their participations accordingly to the duration of the experiments (see Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>) and all of them singed an informed consent form before starting. The samples sizes in all the experiments were defined a priori by using an arbitrary minimum of 70 participants per condition, and the data collection was only stopped when this number was reached for each condition of the experimental design. During the experiments, participants had to answer an attention check question, which allowed us to eliminate workers that did not pay attention to the instruction.</p></sec><sec id="Sec12"><title>Human faces stimuli</title><p id="Par62">A pilot study was conducted to select the material for six of the seven experiments (1, 2, 4, 5, 6, and 7). 14 people took part in the pilot study, from which five were women. All 14 were participating in a summer school that the first author was also attending and all of them were blind to the goal of the research. The average age of this sample is 29.21 (SD&#x02009;=&#x02009;4.46). Participants were given 200 colorful unmodified photographs (400&#x02009;&#x000d7;&#x02009;400 pixels resolution in printed format). 100 photographs depicted adult women and the other 100 depicted adult men. The photographs were downloaded from <ext-link ext-link-type="uri" xlink:href="http://www.facity.com">http://www.facity.com</ext-link>, a website that contains 4265 faces shot under similar conditions (frontal position, open eyes, natural expression &#x02013; no smile, hair pulled back, none or minimum make-up, no glasses, jewelry or clothing visible, daylight, clear background, aperture 2.8 with 50&#x02009;mm lens and square format). Participants were instructed to sort them in five piles: (1) very unattractive; (2) unattractive; (3) medium; (4) attractive; and (5) very attractive (ordered from very unattractive on the left to very attractive on the right on a big table). They were given 30&#x02009;minutes to perform the sorting. Importantly, all participants were given only one exemplar of each face, meaning that a face could only be assigned to one of the five categories. Next, to each category we attributed an attractiveness score from to 2 to &#x02212;2 (very attractive&#x02009;=&#x02009;2, attractive&#x02009;=&#x02009;1, medium&#x02009;=&#x02009;0, unattractive&#x02009;=&#x02009;&#x02212;1, very unattractive&#x02009;=&#x02009;&#x02212;2). To obtain a single attractiveness index for each face, we calculated the weighted average of the scores, that is, the sum of the five products between the assigned value to each category and the proportion of people that attributed the face to that category:<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${A}_{i}=\mathop{\sum }\limits_{i}^{n}{a}_{i}{f}_{i}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><graphic xlink:href="41598_2019_56437_Article_Equa.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${a}_{i}=\{-2,-1,0,1,2\}$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2019_56437_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${f}_{i}=\{\frac{{n}_{-2}}{N},\frac{{n}_{-1}}{N},\frac{{n}_{0}}{N},\frac{{n}_{1}}{N},\frac{{n}_{2}}{N}\}$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2019_56437_Article_IEq2.gif"/></alternatives></inline-formula>. A low value on this index means that the face is considered very unattractive by this group of participants and a high value means that the face is considered very attractive. From the 200 faces we selected a smaller set that would contain faces well distributed across the five categories. To do that, the faces were ordered in terms of their attractiveness index and then divided in five equal size groups (20 female and 20 male faces in each group). From these 20, we selected the 11 faces with the highest attractiveness agreement between the participants. To quantify the categorization agreement between participants, we computed the Shannon Entropy for each photograph individually:<disp-formula id="Equb"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S=\sum _{i}{P}_{i}lo{g}_{2}{P}_{i}$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><graphic xlink:href="41598_2019_56437_Article_Equb.gif" position="anchor"/></alternatives></disp-formula>where <italic>P</italic><sub><italic>i</italic></sub>&#x02009;=&#x02009;<italic>f</italic><sub><italic>i</italic></sub> (see <italic>f</italic><sub><italic>i</italic></sub> above) and <inline-formula id="IEq3"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sum _{i}{P}_{i}=1$$\end{document}</tex-math><mml:math id="M10"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="41598_2019_56437_Article_IEq3.gif"/></alternatives></inline-formula>. A low entropy, in this context, means that people agreed with each other, and high entropy means people did not agree on the categorization of the face.</p><p id="Par63">This procedure allowed us to select a final set of 106 photographs (53 female and 53 male faces) that were used in Experiments 1, 2, 4, 5, 6, and 7. Ten of these faces (five males and five females) were used in the calibrations phase described below and the remaining 96 were used in the experimental trials.</p></sec><sec id="Sec13"><title>Experimental design</title><p id="Par64">In all experiments, after signing the consent form, participants answered demographic questions. Next, they completed the calibration phase, during which they evaluated the attractiveness of ten faces (in their original format) to become familiar with the range of attractiveness used in the experiment.</p><p id="Par65">The Experimental Design in each experiment was as follows:</p><p id="Par66"><italic>Experiments 1 and 2</italic>: 4 modifications (Original vs. Blurred vs. One-third vs. Small)&#x02009;&#x000d7;&#x02009;3 types of judgements (attractiveness vs. knowledgeableness vs. warmness). The first variable was manipulated between-Subjects and the second within-Subject. In the second experiment, we had 4 modifications (Original vs. Incomplete vs. Half-faces vs. Mirror-reversed) manipulated between-Subjects and the judgement performed was on physical attractiveness.</p><p id="Par67"><italic>Experiment 3</italic>: 2 modifications (Original vs. Incomplete) x 3 categories of stimuli (Dogs vs. Flowers vs. Landscapes), the judgement being aesthetic. The first variable was between-Subjects and the second was within-Subjects.</p><p id="Par68"><italic>Experiment 4</italic>: 2 modifications (Original vs. Incomplete) x 3 types of expectations (No Expectation vs. High Expectation vs. Low Expectation), the judgement being about attractiveness. Both variables were between-Subjects.</p><p id="Par69"><italic>Experiments 5 and 6</italic>: 2 modifications (Original vs. Incomplete) with the judgement being about attractiveness. The variable was between-Subjects.</p><p id="Par70"><italic>Experiment 7</italic>: 2 modifications (Original vs. Incomplete) x 3 types of rotations (Upright vs. 90-degree vs. Inverted), the judgement being about attractiveness. Both variables were between-Subjects.</p></sec></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>We thank Sanjay Guruprasad and Kevin Hu for their assistance, and Daniel Gilbert and T&#x000e2;nia Ramos for their comments.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>Both authors contributed to the Conceptualization, the Methodology (designing the experiments), and to the Writing of the paper (to the original draft and also to the reviewing and the editing). Diana Orghian conducted the experiments and the data analyses.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>All the data and material used will be made available upon publication.</p></notes><notes notes-type="COI-statement"><title>Competing interests</title><p id="Par71">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Utz</surname><given-names>S</given-names></name></person-group><article-title>Show me your friends and I will tell you what type of person you are: How one&#x02019;s profile, number of friends, and type of friends influence impression formation on social network sites</article-title><source>Journal of Computer-Mediated Communication</source><year>2010</year><volume>15</volume><fpage>314</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1111/j.1083-6101.2010.01522.x</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>PL</given-names></name><name><surname>Bobko</surname><given-names>P</given-names></name><name><surname>Van Iddekinge</surname><given-names>CH</given-names></name><name><surname>Thatcher</surname><given-names>JB</given-names></name></person-group><article-title>Social media in employee-selection-related decisions: A research agenda for uncharted territory</article-title><source>Journal of Management</source><year>2016</year><volume>42</volume><fpage>269</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1177/0149206313503018</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunning</surname><given-names>D</given-names></name><name><surname>Meyerowitz</surname><given-names>JA</given-names></name><name><surname>Holzberg</surname><given-names>AD</given-names></name></person-group><article-title>Ambiguity and self-evaluation: The role of idiosyncratic trait definitions in self-serving assessments of ability</article-title><source>Journal of Personality and Social Psychology</source><year>1989</year><volume>57</volume><fpage>1082</fpage><lpage>1090</lpage><pub-id pub-id-type="doi">10.1037/0022-3514.57.6.1082</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>SE</given-names></name><name><surname>Brown</surname><given-names>JD</given-names></name></person-group><article-title>Positive illusions and well-being revisited: Separating fact from fiction</article-title><source>Psychological Bulletin</source><year>1994</year><volume>116</volume><fpage>21</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.116.1.21</pub-id><pub-id pub-id-type="pmid">8078971</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilovich</surname><given-names>T</given-names></name></person-group><article-title>Differential construal and the false consensus effect</article-title><source>Journal of Personality and Social Psychology</source><year>1990</year><volume>59</volume><fpage>623</fpage><lpage>634</lpage><pub-id pub-id-type="doi">10.1037/0022-3514.59.4.623</pub-id><pub-id pub-id-type="pmid">2254848</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sedikides</surname><given-names>C</given-names></name><name><surname>Gregg</surname><given-names>AP</given-names></name></person-group><article-title>Self-Enhancement: Food for Thought</article-title><source>Perspectives on Psychological Science</source><year>2008</year><volume>3</volume><fpage>102</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1111/j.1745-6916.2008.00068.x</pub-id><pub-id pub-id-type="pmid">26158877</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nisbett</surname><given-names>RE</given-names></name><name><surname>Zukier</surname><given-names>H</given-names></name><name><surname>Lemley</surname><given-names>RE</given-names></name></person-group><article-title>The dilution effect: Nondiagnostic information weakens the implications of diagnostic information</article-title><source>Cognitive Psychology</source><year>1981</year><volume>13</volume><fpage>248</fpage><lpage>277</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(81)90010-4</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norton</surname><given-names>MI</given-names></name><name><surname>Frost</surname><given-names>JH</given-names></name><name><surname>Ariely</surname><given-names>D</given-names></name></person-group><article-title>Less is more: The lure of ambiguity, or why familiarity breeds contempt</article-title><source>Journal of Personality and Social Psychology</source><year>2007</year><volume>92</volume><fpage>97</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1037/0022-3514.92.1.97</pub-id><pub-id pub-id-type="pmid">17201545</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sears</surname><given-names>DO</given-names></name></person-group><article-title>The person-positivity bias</article-title><source>Journal of Personality and Social Psychology</source><year>1983</year><volume>44</volume><fpage>233</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1037/0022-3514.44.2.233</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Darley</surname><given-names>JM</given-names></name><name><surname>Berscheid</surname><given-names>E</given-names></name></person-group><article-title>Increased liking caused by the anticipation of personal contact</article-title><source>Human Relations</source><year>1967</year><volume>20</volume><fpage>29</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.1177/001872676702000103</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodwin</surname><given-names>SA</given-names></name><name><surname>Fiske</surname><given-names>ST</given-names></name><name><surname>Rosen</surname><given-names>LD</given-names></name><name><surname>Rosenthal</surname><given-names>AM</given-names></name></person-group><article-title>The Eye of the Beholder: Romantic Goals and Impression Biases</article-title><source>Journal of Experimental Social Psychology</source><year>2002</year><volume>38</volume><fpage>232</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1006/jesp.2001.1508</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langlois</surname><given-names>JH</given-names></name><etal/></person-group><article-title>Maxims or myths of beauty? A meta-analytic and theoretical review</article-title><source>Psychological Bulletin</source><year>2000</year><volume>126</volume><fpage>390</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.126.3.390</pub-id><pub-id pub-id-type="pmid">10825783</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Leeuwen</surname><given-names>ML</given-names></name><name><surname>Macrae</surname><given-names>CN</given-names></name></person-group><article-title>Is Beautiful Always Good? Implicit Benefits of Facial Attractiveness</article-title><source>Social Cognition</source><year>2004</year><volume>22</volume><fpage>637</fpage><lpage>649</lpage><pub-id pub-id-type="doi">10.1521/soco.22.6.637.54819</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kenealy</surname><given-names>P</given-names></name><name><surname>Frude</surname><given-names>N</given-names></name><name><surname>Shaw</surname><given-names>W</given-names></name></person-group><article-title>Influence of Children&#x02019;s Physical Attractiveness on Teacher Expectations</article-title><source>The Journal of Social Psychology</source><year>1988</year><volume>128</volume><fpage>373</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1080/00224545.1988.9713754</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>CL</given-names></name><name><surname>Peterson</surname><given-names>RD</given-names></name></person-group><article-title>Halo Effects and the Attractiveness Premium in Perceptions of Political Expertise</article-title><source>American Politics Research</source><year>2016</year><volume>44</volume><fpage>353</fpage><lpage>382</lpage><pub-id pub-id-type="doi">10.1177/1532673X15600517</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hosoda</surname><given-names>M</given-names></name><name><surname>Stone-Romero</surname><given-names>EF</given-names></name><name><surname>Coats</surname><given-names>G</given-names></name></person-group><article-title>The Effects of Physical Attractiveness on Job-Related Outcomes: A Meta-Analysis of Experimental Studies</article-title><source>Personnel Psychology</source><year>2003</year><volume>56</volume><fpage>431</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1111/j.1744-6570.2003.tb00157.x</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname><given-names>PL</given-names></name><name><surname>Karabenick</surname><given-names>SA</given-names></name><name><surname>Lerner</surname><given-names>RM</given-names></name></person-group><article-title>Pretty pleases: The effects of physical attractiveness, race, and sex on receiving help</article-title><source>Journal of Experimental Social Psychology</source><year>1976</year><volume>12</volume><fpage>409</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1016/0022-1031(76)90073-1</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Leeuwen</surname><given-names>ML</given-names></name><name><surname>Veling</surname><given-names>H</given-names></name><name><surname>van Baaren</surname><given-names>RB</given-names></name><name><surname>Dijksterhuis</surname><given-names>A</given-names></name></person-group><article-title>The Influence of Facial Attractiveness on Imitation</article-title><source>Journal of Experimental Social Psychology</source><year>2009</year><volume>45</volume><fpage>1295</fpage><lpage>1298</lpage><pub-id pub-id-type="doi">10.1016/j.jesp.2009.07.008</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langlois</surname><given-names>JH</given-names></name><name><surname>Ritter</surname><given-names>JM</given-names></name><name><surname>Casey</surname><given-names>RJ</given-names></name><name><surname>Sawin</surname><given-names>DB</given-names></name></person-group><article-title>Infant attractiveness predicts maternal behaviors and attitudes</article-title><source>Developmental Psychology</source><year>1995</year><volume>31</volume><fpage>464</fpage><lpage>472</lpage><pub-id pub-id-type="doi">10.1037/0012-1649.31.3.464</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erian</surname><given-names>M</given-names></name><name><surname>Lin</surname><given-names>C</given-names></name><name><surname>Patel</surname><given-names>N</given-names></name><name><surname>Neal</surname><given-names>A</given-names></name><name><surname>Geiselman</surname><given-names>RE</given-names></name></person-group><article-title>Juror verdicts as a function of victim and defendant attractiveness in sexual assault cases</article-title><source>American Journal of Forensic Psychology</source><year>1998</year><volume>16</volume><fpage>25</fpage><lpage>40</lpage></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname><given-names>D</given-names></name><name><surname>Clark</surname><given-names>LA</given-names></name><name><surname>Tellegen</surname><given-names>A</given-names></name></person-group><article-title>Development and validation of brief measures of positive and negative affect: The PANAS scales</article-title><source>Journal of Personality and Social Psychology</source><year>1988</year><volume>54</volume><fpage>1063</fpage><lpage>1070</lpage><pub-id pub-id-type="doi">10.1037/0022-3514.54.6.1063</pub-id><pub-id pub-id-type="pmid">3397865</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cartwright</surname><given-names>D</given-names></name></person-group><article-title>The effect of interruption, completion and failure upon the attractiveness of activity</article-title><source>Journal of Experimental Psychology</source><year>1942</year><volume>31</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1037/h0060243</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>TD</given-names></name><name><surname>Centerbar</surname><given-names>DB</given-names></name><name><surname>Kermer</surname><given-names>DA</given-names></name><name><surname>Gilbert</surname><given-names>DT</given-names></name></person-group><article-title>The pleasures of uncertainty: prolonging positive moods in ways people do not anticipate</article-title><source>Journal of Personality and Social Psychology</source><year>2005</year><volume>88</volume><fpage>5</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1037/0022-3514.88.1.5</pub-id><pub-id pub-id-type="pmid">15631571</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olmos</surname><given-names>A</given-names></name><name><surname>Kingdom</surname><given-names>FAA</given-names></name></person-group><article-title>A Biologically Inspired Algorithm for the Recovery of Shading and Reflectance Images</article-title><source>Perception</source><year>2004</year><volume>33</volume><fpage>1463</fpage><lpage>1473</lpage><pub-id pub-id-type="doi">10.1068/p5321</pub-id><pub-id pub-id-type="pmid">15729913</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langlois</surname><given-names>J</given-names></name><name><surname>Roggman</surname><given-names>LA</given-names></name></person-group><article-title>Attractive faces are only average</article-title><source>Psychological Science</source><year>1990</year><volume>1</volume><fpage>115</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.1990.tb00079.x</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rhodes</surname><given-names>G</given-names></name><name><surname>Sumich</surname><given-names>A</given-names></name><name><surname>Byatt</surname><given-names>G</given-names></name></person-group><article-title>Are average facial configurations attractive only because of their symmetry?</article-title><source>Psychological Science</source><year>1999</year><volume>10</volume><fpage>52</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00106</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbas</surname><given-names>ZA</given-names></name><name><surname>Duchaine</surname><given-names>B</given-names></name></person-group><article-title>The Role of Holistic Processing in Judgments of Facial Attractiveness</article-title><source>Perception</source><year>2008</year><volume>37</volume><fpage>1187</fpage><lpage>1196</lpage><pub-id pub-id-type="doi">10.1068/p5984</pub-id><pub-id pub-id-type="pmid">18853555</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barton</surname><given-names>JJS</given-names></name><name><surname>Keenan</surname><given-names>JP</given-names></name><name><surname>Bass</surname><given-names>T</given-names></name></person-group><article-title>Discrimination of spatial relations and features in faces: Effects of inversion and viewing duration</article-title><source>British Journal of Psychology</source><year>2001</year><volume>92</volume><fpage>527</fpage><lpage>549</lpage><pub-id pub-id-type="doi">10.1348/000712601162329</pub-id><pub-id pub-id-type="pmid">11802888</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yovel</surname><given-names>G</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>Face Perception: Domain Specific, Not Process Specific</article-title><source>Neuron</source><year>2004</year><volume>44</volume><fpage>889</fpage><lpage>898</lpage><pub-id pub-id-type="pmid">15572118</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richler</surname><given-names>JJ</given-names></name><name><surname>Cheung</surname><given-names>OS</given-names></name><name><surname>Gauthier</surname><given-names>I</given-names></name></person-group><article-title>Beliefs alter holistic face processing&#x02026; if response bias is not taken into account</article-title><source>Journal of Vision</source><year>2011</year><volume>11</volume><fpage>17</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1167/11.13.17</pub-id><pub-id pub-id-type="pmid">22101018</pub-id></element-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>JW</given-names></name><name><surname>Farah</surname><given-names>MJ</given-names></name></person-group><article-title>Parts and wholes in face recognition</article-title><source>Quarterly Journal of Experimental Psychology</source><year>1993</year><volume>46</volume><fpage>225</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1080/14640749308401045</pub-id><pub-id pub-id-type="pmid">8316637</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michel</surname><given-names>C</given-names></name><name><surname>Corneille</surname><given-names>O</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Race categorization modulates holistic face encoding</article-title><source>Cognitive Science</source><year>2007</year><volume>31</volume><fpage>911</fpage><lpage>924</lpage><pub-id pub-id-type="doi">10.1080/03640210701530805</pub-id><pub-id pub-id-type="pmid">21635322</pub-id></element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calder</surname><given-names>AJ</given-names></name><name><surname>Young</surname><given-names>AW</given-names></name><name><surname>Keane</surname><given-names>J</given-names></name><name><surname>Dean</surname><given-names>M</given-names></name></person-group><article-title>Configural information in facial expression perception</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2000</year><volume>26</volume><fpage>527</fpage><lpage>551</lpage><pub-id pub-id-type="pmid">10811161</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leder</surname><given-names>H</given-names></name><name><surname>Bruce</surname><given-names>V</given-names></name></person-group><article-title>Local and relational aspects of face distinctiveness</article-title><source>The Quarterly Journal of Experimental Psychology</source><year>1998</year><volume>51</volume><fpage>449</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1080/713755777</pub-id><pub-id pub-id-type="pmid">9745377</pub-id></element-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diamond</surname><given-names>R</given-names></name><name><surname>Carey</surname><given-names>S</given-names></name></person-group><article-title>Why faces are and are not special: an effect of expertise</article-title><source>Journal of Experimental Psychology: General</source><year>1986</year><volume>115</volume><fpage>107</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.115.2.107</pub-id><pub-id pub-id-type="pmid">2940312</pub-id></element-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saegusa</surname><given-names>C</given-names></name><name><surname>Watanabe</surname><given-names>K</given-names></name></person-group><article-title>Judgments of facial attractiveness as a combination of facial parts information over time: Social and aesthetic factors</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2016</year><volume>42</volume><fpage>173</fpage><lpage>179</lpage><pub-id pub-id-type="pmid">26460871</pub-id></element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yonemura</surname><given-names>K</given-names></name><name><surname>Ono</surname><given-names>F</given-names></name><name><surname>Watanabe</surname><given-names>K</given-names></name></person-group><article-title>Back View of Beauty: A Bias in Attractiveness Judgment</article-title><source>Perception</source><year>2013</year><volume>42</volume><fpage>95</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1068/p7356</pub-id><pub-id pub-id-type="pmid">23678619</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuki</surname><given-names>M</given-names></name><name><surname>Kawahara</surname><given-names>J</given-names></name></person-group><article-title>The sanitary&#x02010;mask effect on perceived facial attractiveness</article-title><source>Japanese Psychological Research</source><year>2016</year><volume>58</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1111/jpr.12116</pub-id></element-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Qin</surname><given-names>J</given-names></name></person-group><article-title>Neural responses to cartoon facial attractiveness: An event-related potential study</article-title><source>Neuroscience bulletin</source><year>2014</year><volume>30</volume><issue>3</issue><fpage>441</fpage><lpage>450</lpage><pub-id pub-id-type="doi">10.1007/s12264-013-1401-4</pub-id><pub-id pub-id-type="pmid">24526658</pub-id></element-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rhodes</surname><given-names>G</given-names></name></person-group><article-title>The evolutionary psychology of facial beauty</article-title><source>Annual Review of Psychology</source><year>2006</year><volume>57</volume><fpage>199</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.57.102904.190208</pub-id><pub-id pub-id-type="pmid">16318594</pub-id></element-citation></ref><ref id="CR41"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perrett</surname><given-names>DI</given-names></name><etal/></person-group><article-title>Symmetry and human facial attractiveness</article-title><source>Evolution and human behavior</source><year>1999</year><volume>20</volume><issue>5</issue><fpage>295</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1016/S1090-5138(99)00014-8</pub-id></element-citation></ref><ref id="CR42"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horton</surname><given-names>JJ</given-names></name><name><surname>Rand</surname><given-names>DG</given-names></name><name><surname>Zeckhauser</surname><given-names>RJ</given-names></name></person-group><article-title>The online laboratory: conducting experiments in a real labor market</article-title><source>Experimental Economics</source><year>2011</year><volume>14</volume><fpage>399</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1007/s10683-011-9273-9</pub-id></element-citation></ref><ref id="CR43"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Germine</surname><given-names>L</given-names></name><etal/></person-group><article-title>Is the Web as good as the lab? Comparable performance from Web and lab in cognitive/perceptual experiments</article-title><source>Psychonomic Bulletin and Review</source><year>2012</year><volume>19</volume><fpage>847</fpage><lpage>857</lpage><pub-id pub-id-type="doi">10.3758/s13423-012-0296-9</pub-id><pub-id pub-id-type="pmid">22829343</pub-id></element-citation></ref></ref-list></back></article>