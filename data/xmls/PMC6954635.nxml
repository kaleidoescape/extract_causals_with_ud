<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Biomed Eng Online</journal-id><journal-id journal-id-type="iso-abbrev">Biomed Eng Online</journal-id><journal-title-group><journal-title>BioMedical Engineering OnLine</journal-title></journal-title-group><issn pub-type="epub">1475-925X</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">31924202</article-id><article-id pub-id-type="pmc">6954635</article-id><article-id pub-id-type="publisher-id">746</article-id><article-id pub-id-type="doi">10.1186/s12938-019-0746-y</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title>Multifactor consciousness level assessment of participants with acquired brain injuries employing human&#x02013;computer interfaces</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Czy&#x0017c;ewski</surname><given-names>Andrzej</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5132-3016</contrib-id><name><surname>Kurowski</surname><given-names>Adam</given-names></name><address><email>adakurow@multimed.org</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Odya</surname><given-names>Piotr</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Szczuko</surname><given-names>Piotr</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2187 838X</institution-id><institution-id institution-id-type="GRID">grid.6868.0</institution-id><institution>Multimedia Systems Department, Faculty of Electronics, Telecommunications and Informatics, </institution><institution>Gda&#x00144;sk University of Technology, </institution></institution-wrap>Gda&#x00144;sk, Poland </aff></contrib-group><pub-date pub-type="epub"><day>10</day><month>1</month><year>2020</year></pub-date><pub-date pub-type="pmc-release"><day>10</day><month>1</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>19</volume><elocation-id>2</elocation-id><history><date date-type="received"><day>31</day><month>1</month><year>2019</year></date><date date-type="accepted"><day>31</day><month>12</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2020</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p></license></permissions><abstract id="Abs1"><sec><title>Background</title><p id="Par1">A lack of communication with people suffering from acquired brain injuries may lead to drawing erroneous conclusions regarding the diagnosis or therapy of patients. Information technology and neuroscience make it possible to enhance the diagnostic and rehabilitation process of patients with traumatic brain injury or post-hypoxia. In this paper, we present a new method for evaluation possibility of communication and the assessment of such patients&#x02019; state employing future generation computers extended with advanced human&#x02013;machine interfaces.</p></sec><sec><title>Methods</title><p id="Par2">First, the hearing abilities of 33 participants in the state of coma were evaluated using auditory brainstem response measurements (ABR). Next, a series of interactive computer-based exercise sessions were performed with the therapist&#x02019;s assistance. Participants&#x02019; actions were monitored with an eye-gaze tracking (EGT) device and with an electroencephalogram EEG monitoring headset. The data gathered were processed with the use of data clustering techniques.</p></sec><sec><title>Results</title><p id="Par3">Analysis showed that the data gathered and the computer-based methods developed for their processing are suitable for evaluating the participants&#x02019; responses to stimuli. Parameters obtained from EEG signals and eye-tracker data were correlated with Glasgow Coma Scale (GCS) scores and enabled separation between GCS-related classes. The results show that in the EEG and eye-tracker signals, there are specific consciousness-related states discoverable. We observe them as outliers in diagrams on the decision space generated by the autoencoder. For this reason, the numerical variable that separates particular groups of people with the same GCS is the variance of the distance of points from the cluster center that the autoencoder generates. The higher the GCS score, the greater the variance in most cases. The results proved to be statistically significant in this context.</p></sec><sec><title>Conclusions</title><p id="Par4">The results indicate that the method proposed may help to assess the consciousness state of participants in an objective manner.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Acquired brainstem response</kwd><kwd>Multimedia computers</kwd><kwd>Human&#x02013;computer interfaces</kwd><kwd>Eye tracking</kwd><kwd>Electroencephalography</kwd><kwd>Auditory brainstem injuries</kwd><kwd>Data clustering analysis</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004281</institution-id><institution>Narodowe Centrum Nauki</institution></institution-wrap></funding-source><award-id>DEC-2014/15/B/ST7/04724</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2020</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background</title><p id="Par19">The incidence rate of traumatic brain injuries (TBI) in Europe in 2006 was reported as 235 cases per 100,000 people per year [<xref ref-type="bibr" rid="CR1">1</xref>]. An efficient tool for communicating with such patients and for evaluating their communication abilities is needed. Unfortunately, both communication and assessment of the progress made in the course of care by participants with disorders such as TBI, sudden cardiac arrest (SCA), and hypoxia may pose a real challenge to a therapist. All the groups of disorders mentioned could collectively be called acquired brain injuries (ABI). Typical procedures concerning participants with TBI and other disorders include examination employing one of the various imaging techniques. The current literature indicates several common ways to assess consciousness in people suffering from acquired brain injuries ABI. Many of them involve some use of EEG signal measurement, however, techniques such as transcranial stimulation, deep brain stimulation are also possible, and they were described in the literature [<xref ref-type="bibr" rid="CR2">2</xref>]. EEG-based techniques often apply measurement and analysis of event-related potentials (ERP) [<xref ref-type="bibr" rid="CR2">2</xref>]. They also may involve using external stimuli such as auditory signals [<xref ref-type="bibr" rid="CR3">3</xref>]. Another example of an EEG-based technique is a mismatch negativity (MMN) approach [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>]. Over the last two decades, computed tomography (CT) and magnetic resonance imaging (MRI) scans have become standard medical practice, and they are widely used for many aspects of modern medicine, like for example detection of tumors or identification of sites of injury from impact [<xref ref-type="bibr" rid="CR6">6</xref>&#x02013;<xref ref-type="bibr" rid="CR8">8</xref>]. Although positron emission tomography (PET) and functional magnetic resonance imaging fMRI are also employed for medical diagnostics, they are not standard medical procedures, because of their high cost, thus they are more often used for research purposes [<xref ref-type="bibr" rid="CR9">9</xref>]. Some works also identify the need for cheap and wide-applicable solutions for the estimation of the state of patients suffering from ABI [<xref ref-type="bibr" rid="CR2">2</xref>]. Researchers and physicians have proved that the use of PET and especially fMRI is eventually inevitable in the clinical examination of patients with disorders of consciousness [<xref ref-type="bibr" rid="CR10">10</xref>&#x02013;<xref ref-type="bibr" rid="CR13">13</xref>] due to their accuracy and usability. However, high cost is still the factor that significantly limits their use for daily care, especially outside hospitals. Thus, a less expensive and more portable solution is required, which would be capable of evaluating the response of participants to stimulation performed reliably by a therapist and of improving the understanding of their current needs.</p><p id="Par20">Some studies have concluded that&#x000a0;in case of communication with patients with locked-in syndrome, it may be possible to employ EEG [<xref ref-type="bibr" rid="CR14">14</xref>] or eye-tracker data [<xref ref-type="bibr" rid="CR15">15</xref>] as a means of interaction. Some research also suggests that long-term monitoring of EEG signals gathered when participants are sleeping may help to predict the outcome of the therapeutic process [<xref ref-type="bibr" rid="CR16">16</xref>]. Recently, an approach applying an eye-tracker device was employed to assess the emotional states of healthy participants [<xref ref-type="bibr" rid="CR17">17</xref>]. There are also works that identify the need for cheap and widely applicable solutions for the estimation of the state of patients suffering from ABI [<xref ref-type="bibr" rid="CR2">2</xref>]. We would like to propose a direction of diagnosis based upon the bimodal measurement of the activity of people taking part in therapeutic sessions. Participants of our research suffered from acquired brain injuries, and they took part in therapeutic sessions which involved interaction with computer by using the eye-gaze tracking device (EGT). During those sessions, we measured EEG signals and collected data from the eye tracker. Next, we employed an autoencoder neural network to perform clustering and separability analysis of data collected in the experiment. We were especially interested in employing this method for session-to-session assessment of GCS score, as the literature indicates, that such comparison of GCS values over time may provide valuable clinical information to be used for monitoring the state of patients [<xref ref-type="bibr" rid="CR18">18</xref>]. This kind of assessment tool may improve therapists&#x02019; communication with their patients entailing more effective communication which can help, in turn, to provide reliable and a capacity to objectively evaluate the patients&#x02019; state.</p><p id="Par21">The structure of this article is as follows: in the next section results from the analysis of signals obtained from the EEG and eye-tracker devices are presented together with outcomes of the statistical analysis; next, a discussion and conclusions drawn from the results observed are presented in two consecutive sections; the last section provides a brief profile of participants who took part in the experiments together with methodology for verifying the hearing abilities of participants. It also provides a detailed description of the methodology used for making measurements and processing of the data obtained from the experiment participants.</p></sec><sec id="Sec2"><title>Results</title><p id="Par22">The signals gathered during therapeutic sessions performed with the study participants were split into a series of 12-s-long epochs. Each epoch contained signals from both the EEG headset and the eye-tracker device. It was then used to train the multimodal autoencoder neural network, which performed unsupervised data segmentation. Data were collected with two headsets consisting of 5 and 14 electrodes. A separate analysis was performed for each dataset. The only difference between neural networks employed for this purpose is number of EEG-related channels in each network. In each network this number of channels was equal to the number of electrodes in the processed dataset. The encoder part of the autoencoder was utilized to generate vectors of parameters associated with pairs of epochs extracted from EEG and EGT signals. To visualize them on a two-dimensional space, a PCA-based projection onto a plane was determined. Additionally, values of GCS and its components associated with participants from whom the epochs of signals are gathered are marked with color on those figures. Visualizations are shown in Figs.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, <xref rid="Fig2" ref-type="fig">2</xref>, <xref rid="Fig3" ref-type="fig">3</xref>, <xref rid="Fig4" ref-type="fig">4</xref>, <xref rid="Fig5" ref-type="fig">5</xref>.<fig id="Fig1"><label>Fig.&#x000a0;1</label><caption><p>Image of clusters obtained from the autoencoder with different values of GCS score marked with color</p></caption><graphic xlink:href="12938_2019_746_Fig1_HTML" id="MO1"/></fig>
<fig id="Fig2"><label>Fig.&#x000a0;2</label><caption><p>Image of clusters obtained from the autoencoder with different values of the verbal response component of GCS marked with color. Value of 1 indicates making no sounds, 2&#x02014;the ability to make sounds, and 3&#x02014;the ability to say single words</p></caption><graphic xlink:href="12938_2019_746_Fig2_HTML" id="MO2"/></fig>
<fig id="Fig3"><label>Fig.&#x000a0;3</label><caption><p>Image of clusters obtained from the autoencoder with different values of the motoric component of GCS marked with color. Value of 1 indicates that participant does not move, 2&#x02014;extends limb in response to painful stimuli, 3&#x02014;abnormally flexes a limp in response to painful stimuli, and 4&#x02014;flexion/withdrawal in response to painful stimuli</p></caption><graphic xlink:href="12938_2019_746_Fig3_HTML" id="MO3"/></fig>
<fig id="Fig4"><label>Fig.&#x000a0;4</label><caption><p>Image of clusters obtained from the autoencoder with different values of eye component of GCS marked with color. Value of 2 indicates that participants can open eyes, 3&#x02014;opens eyes in response to voice, 4&#x02014;opens eyes spontaneously</p></caption><graphic xlink:href="12938_2019_746_Fig4_HTML" id="MO4"/></fig>
<fig id="Fig5"><label>Fig.&#x000a0;5</label><caption><p>Image of clusters obtained from the autoencoder with three sessions of data acquisition marked with color</p></caption><graphic xlink:href="12938_2019_746_Fig5_HTML" id="MO5"/></fig>
</p><p id="Par23">It is discernible in the coloring of points associated with three sessions of data acquisition marked consecutively as A, B, and C. This visualization is visible in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>. During the session A, only a 5-electrode headset was used. Both headsets were employed during the session B, and during the session C, only the 14-electrode headset was employed. Such a situation permits an interesting comparison for separability of GCS classes for parameters derived from 5-channel headset and 14-channel headset. Separation of classes is more prominent for the cluster associated with a 14-electrode headset. This is especially prominent for verbal and motoric components of GCS scale. There is also a separation between classes for the cluster related to a dataset gathered with a 5-electrode headset, however it is less pronounced. It should be noted that no information about GCS was delivered to the algorithm, therefore all the separation between GCS-related classes are the result of the training process of the multimodal autoencoder.</p><p id="Par24">In the case of both headsets, the feature which is causing the separation is the variance of clusters associated with each GCS. This can support the hypothesis that for two groups with different GCS values, various amounts of outliers are present, and thus, there is a difference in the variance of data points. The variance of distance between the point and the cluster center was calculated for the 32-dimensional set of vectors produced by neural networks. The calculation was carried out separately for each of headset-related dataset and each unique value of GCS or its component. To test the statistical significance of results, the Brown&#x02013;Forsythe statistic test was employed. This test checks if variances of multiple groups of observations are different. The calculation was also performed separately for each cluster (associated with 5 or 14 electrodes), and each GCS value within that cluster. As a post hoc test after the Brown&#x02013;Forsythe test, a modified Dunn test was used. Under standard circumstances, the Dunn test is employed for repeated testing of equality of medians of multiple groups of observations. However, it can also be employed as a post hoc variant of the Brown&#x02013;Forsythe test, which is equivalent to the ANOVA test performed on vectors of observations transformed in the following way:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{x} = \left| {\varvec{x} - \bar{\varvec{x}}} \right|,$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfenced close="|" open="|" separators=""><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">&#x000af;</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="12938_2019_746_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{x}$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq1.gif"/></alternatives></inline-formula> is a vector of observations, and <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar{\varvec{x}}$$\end{document}</tex-math><mml:math id="M6"><mml:mover accent="true"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">&#x000af;</mml:mo></mml:mrow></mml:mover></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq2.gif"/></alternatives></inline-formula> is a mean value of this vector.</p><p id="Par25">As the processing was carried out separately for dataset originating from the 5-electrode headset and 14-electrode headset, statistical analyses were also performed separately for the 32-dimensional result vectors obtained from the encoding part of autoencoder neural networks. Values of variances calculated for each cluster and each value of GCS are depicted in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>. Results of calculations related to statistic test are shown in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>. It can be observed that many differences seen in values of variance are statistically significant for both the standard significance factor of 0.05 and even 0.001. From Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> it also can be observed, that in most cases, the variance of distance between points and center of the cluster increases with the value of GCS. As it can also be observed in the visualization from Figs.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, <xref rid="Fig2" ref-type="fig">2</xref>, <xref rid="Fig3" ref-type="fig">3</xref>, <xref rid="Fig4" ref-type="fig">4</xref>, this may not be true for all cases. Some of the classes are also separated by the decreasing values of the variance, and points associated with a higher GCS value are concentrated near the center of the cluster.<table-wrap id="Tab1"><label>Table&#x000a0;1</label><caption><p>Variance of distance between of center of cluster (associated with 5 or 14-electrode headset) and points associated with given GCS values, <inline-formula id="IEq9"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_{\text{ex}}$$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mi>n</mml:mi><mml:mtext>ex</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq9.gif"/></alternatives></inline-formula> denotes number of points used for variance calculation</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="3">5 electrodes, GCS (sum)</th><th align="left" colspan="3">5 electrodes, GCS (eyes)</th><th align="left" colspan="3">5 electrodes, GCS (motor)</th><th align="left" colspan="3">5 electrodes, GCS (verbal)</th></tr><tr><th align="left">GCS</th><th align="left">Variance</th><th align="left"><inline-formula id="IEq10"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_{\text{ex}}$$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mi>n</mml:mi><mml:mtext>ex</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq10.gif"/></alternatives></inline-formula></th><th align="left">GCS</th><th align="left">Variance</th><th align="left"><inline-formula id="IEq11"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_{\text{ex}}$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mi>n</mml:mi><mml:mtext>ex</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq11.gif"/></alternatives></inline-formula></th><th align="left">GCS</th><th align="left">Variance</th><th align="left"><inline-formula id="IEq12"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_{\text{ex}}$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mi>n</mml:mi><mml:mtext>ex</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq12.gif"/></alternatives></inline-formula></th><th align="left">GCS</th><th align="left">Variance</th><th align="left"><inline-formula id="IEq13"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_{\text{ex}}$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mi>n</mml:mi><mml:mtext>ex</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq13.gif"/></alternatives></inline-formula></th></tr></thead><tbody><tr><td align="left">5</td><td char="." align="char">12.860</td><td char="." align="char">410</td><td char="." align="char">2</td><td char="." align="char">29.211</td><td char="." align="char">1835</td><td char="." align="char">1</td><td char="." align="char">18.581</td><td char="." align="char">336</td><td char="." align="char">1</td><td char="." align="char">37.107</td><td char="." align="char">1915</td></tr><tr><td align="left">6</td><td char="." align="char">21.247</td><td char="." align="char">592</td><td char="." align="char">3</td><td char="." align="char">71.507</td><td char="." align="char">1295</td><td char="." align="char">2</td><td char="." align="char">67.480</td><td char="." align="char">1259</td><td char="." align="char">2</td><td char="." align="char">58.221</td><td char="." align="char">2758</td></tr><tr><td align="left">7</td><td char="." align="char">55.822</td><td char="." align="char">1953</td><td char="." align="char">4</td><td char="." align="char">55.497</td><td char="." align="char">1876</td><td char="." align="char">3</td><td char="." align="char">42.782</td><td char="." align="char">2777</td><td char="." align="char">3</td><td char="." align="char">65.401</td><td char="." align="char">333</td></tr><tr><td align="left">8</td><td char="." align="char">61.848</td><td char="." align="char">1046</td><td align="left"/><td align="left"/><td align="left"/><td char="." align="char">4</td><td char="." align="char">72.903</td><td char="." align="char">634</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">9</td><td char="." align="char">37.872</td><td char="." align="char">841</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">10</td><td char="." align="char">79.022</td><td char="." align="char">524</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr></tbody></table><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="3">14 electrodes, GCS (sum)</th><th align="left" colspan="3">14 electrodes, GCS (eyes)</th><th align="left" colspan="3">14 electrodes, GCS (motor)</th><th align="left" colspan="3">14 electrodes, GCS (verbal)</th></tr><tr><th align="left">GCS</th><th align="left">Variance</th><th align="left"><inline-formula id="IEq14"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_{\text{ex}}$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mi>n</mml:mi><mml:mtext>ex</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq14.gif"/></alternatives></inline-formula></th><th align="left">GCS</th><th align="left">Variance</th><th align="left"><inline-formula id="IEq15"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_{\text{ex}}$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mi>n</mml:mi><mml:mtext>ex</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq15.gif"/></alternatives></inline-formula></th><th align="left">GCS</th><th align="left">Variance</th><th align="left"><inline-formula id="IEq16"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_{\text{ex}}$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mi>n</mml:mi><mml:mtext>ex</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq16.gif"/></alternatives></inline-formula></th><th align="left">GCS</th><th align="left">Variance</th><th align="left"><inline-formula id="IEq17"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_{\text{ex}}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>n</mml:mi><mml:mtext>ex</mml:mtext></mml:msub></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq17.gif"/></alternatives></inline-formula></th></tr></thead><tbody><tr><td align="left">5</td><td char="." align="char">32.239</td><td char="." align="char">349</td><td char="." align="char">2</td><td char="." align="char">32.239</td><td char="." align="char">349</td><td char="." align="char">1</td><td char="." align="char">21.061</td><td char="." align="char">281</td><td char="." align="char">1</td><td char="." align="char">52.001</td><td char="." align="char">2361</td></tr><tr><td align="left">6</td><td char="." align="char">21.061</td><td char="." align="char">281</td><td char="." align="char">3</td><td char="." align="char">53.590</td><td char="." align="char">588</td><td char="." align="char">2</td><td char="." align="char">51.859</td><td char="." align="char">1088</td><td char="." align="char">2</td><td char="." align="char">39.748</td><td char="." align="char">639</td></tr><tr><td align="left">7</td><td char="." align="char">57.074</td><td char="." align="char">1006</td><td char="." align="char">4</td><td char="." align="char">52.022</td><td char="." align="char">2063</td><td char="." align="char">3</td><td char="." align="char">60.515</td><td char="." align="char">1313</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">8</td><td char="." align="char">63.194</td><td char="." align="char">1046</td><td align="left"/><td align="left"/><td align="left"/><td char="." align="char">4</td><td char="." align="char">18.385</td><td char="." align="char">318</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">10</td><td char="." align="char">18.385</td><td char="." align="char">318</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr></tbody></table><table-wrap-foot><p>Calculations were performed for points in a 32-dimensional space of autoencoder-generated embedding vectors</p></table-wrap-foot></table-wrap><table-wrap id="Tab2"><label>Table&#x000a0;2</label><caption><p>Results of Brown&#x02013;Forsythe test and <italic>p</italic> value matrices for the following post hoc test</p></caption><graphic position="anchor" xlink:href="12938_2019_746_Tab2_HTML" id="MO7"/><table-wrap-foot><p>The value of 0 in the table indicates that the <italic>p</italic> value was lesser than <inline-formula id="IEq18"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{ - 3}$$\end{document}</tex-math><mml:math id="M26"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq18.gif"/></alternatives></inline-formula>. Statistically insignificant values are marked with a italic font</p></table-wrap-foot></table-wrap></p><p id="Par26">We hypothesize that the variance increases because of the occurrence of unique states of EEG signal, which are mapped by the autoencoder onto a more distant location when compared to more common states. The occurrence of such rare states may be both an indicator of higher or lower GCS value, and their presence can be tracked and treated as a clue about current GCS value associated with each participant. In the case of 5-electrode headset and motoric component of GCS, a statistical significance was implied by Brown&#x02013;Forsythe test, but none of the pairs compared by Dunn test were found to be statistically different in the Dunn test. This is probably a consequence of unbalanced count of examples, as it can be seen in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>. We assume that all the differences, in this case, were statistically insignificant.</p></sec><sec id="Sec3"><title>Discussion</title><p id="Par27">The principal purpose of this study was to propose and to verify a set of procedures that could improve the process of assessing the consciousness level of TBI and SCA patients and of those who have experienced brain hypoxia. The first stage of our method was to administer hearing tests: objective hearing tests were preferred over subjective ones because the type of injury prevents normal communication with patients necessary to organize subjective hearing tests. Objective hearing tests were carried out to select participants who might have been able to receive verbal information. These patients participated in mental exercises allowing for an evaluation of their mental state, employing data from the system used for performing tasks formulated by the therapist.</p><p id="Par28">We found that some of the parameters calculated based on signals gathered from the eye tracker and the EEG headset are associated with the GCS scores or with their component values. We identified that in the hyperspace generated by the autoencoder employed for the unsupervised data analysis a variance of distance between points in hyperspace and center of a cluster is associated with the value of GCS and its components. This outcome was confirmed by statistical analysis. The variance of the subset may both increase or decrease with the value of GCS of its components, however, placement of points in the decision space usually allows partial separation between GCS-related classes and this may be utilized as a clue to estimate GCS-related score of patients. Such separation is especially visible in the case of GCS components related to motoric and verbal response. The above changes of variance may be caused by the occurrence of additional, &#x0201c;atypical&#x0201d; vectors of parameters caused by atypical pairs of EEG and eye tracker signal-related frames at the input of an autoencoder. They are different from the most frequent epochs which were placed in proximity of the center of each headsets cluster. Such atypical epochs may indicate rare &#x0201c;states&#x0201d; of EEG and eye-tracker signals which occur only in signals collected from participants with higher GCS values. Thus, the value of variance is increased if their signals are analyzed.</p><p id="Par29">The analysis of mental activities derived from EEG signal may open new ways of assessing engagement and the mental comfort of participants during the mental exercise making and also provide new methods to evaluate their therapy progress through the use of the association of parameters variance identified in our study with some selected components of the GCS score. There are some practical difficulties that have to be overcome before a more robust system for diagnosing, stimulating, and making mental exercises by participants with traumatic brain injuries is developed. The EEG headset has to be less sensitive to small movements from the patients and imperfect skin contact. The dry electrode-based setup of the INSIGHT EEG helmet does not satisfy the above condition, therefore for future research, the use of a saline-based setup is recommended. Despite that fact, we found that tendencies in the variance changes are also observed in signals collected from the 5-electrode headset. The drawback of such heterogeneity in data is the fact that data from the 5-electrode and 14-electrode headsets were assigned to 2 separate clusters.</p></sec><sec id="Sec4"><title>Conclusions</title><p id="Par30">We proposed a method for assessing the state of persons with ABI based on the assessment of their hearing abilities and on checking the performance of participants during mental exercises supervised by a therapist employing a multimedia computer extended with HCI devices. The proposed solution was mainly designed for use in daily therapeutic practice. Moreover, the cost-effective choice of components of the proposed system (employing ABR, EGT, and EEG devices) makes it possible to perform diagnostics of patients with brain injury even on a limited budget. As a result, more therapists may obtain the technology to examine the patients&#x02019; state objectively and reliably and to establish or maintain communication with them.</p><p id="Par31">A series of tests with a larger group of participants with varying severity of ABI is planned. These tests will permit the investigation of the structure of mental states detected by the algorithm in the context of personal differences associated with the ability to use BCI [<xref ref-type="bibr" rid="CR19">19</xref>&#x02013;<xref ref-type="bibr" rid="CR21">21</xref>]. It would also be worth investigating how the pattern and presence of rarely observed signal detected correlate with the effectiveness of using BCI and if there are participants for whom such a method of monitoring brain activity will not work. This kind of person-specific problem is particularly significant in the field of brain&#x02013;computer interfaces, where it is called &#x0201c;BCI illiteracy&#x0201d; [<xref ref-type="bibr" rid="CR22">22</xref>].</p><p id="Par32">Time-domain data associated with changes in the participants&#x02019; mental state during the therapeutic session may also deliver vital information for the therapist. We believe that the algorithm present in this study may help in the parameterization of the EEG signal for purposes of classification and evaluation of patients&#x02019; state in clinical practice. For instance, such parameterized data can be utilized to identify moments when a participant feels uncomfortable, or he or she is distracted by some external factor. Moreover, the occurrence of repetitive patterns of brain activity correlated with types of exercises performed may be a premise for concluding that a participant is actively, mentally involved in the process of communication. The ability to focus an eye on certain points during the exercise provides a similar kind of premise. The EEG signal analysis method may be used for verification of, or for searching for, some physiological reactions of participants that can be, in turn, associated with the selection of stimuli provided by a therapist. The results are encouraging because, in principle, they show that in the EEG and eye-tracker signals, there are specific consciousness-related states discoverable. We observe them as outliers in diagrams on the decision space generated by the autoencoder. For this reason, the numerical variable that separates particular groups of people with the same GCS is the variance of the distance of points from the cluster center that the autoencoder generates. The higher the GCS score, the greater the variance in most cases. We managed to prove that the results are statistically significant in this context.</p><p id="Par33">Further development of tools for data analysis is also envisaged. Namely, we would like to evaluate the performance of various classifiers for the prediction of participants&#x02019; state and for detection of events such as the feeling of discomfort and state of being distracted from the exercise by external factors. Also, more complicated pre-processing of eye-tracker data may be implemented. A recurrent neural network may be used to capture not only the probability distribution of eye gaze placement, but also track the whole trajectory of the eye-fixation point due to its time-modeling abilities. Applying the above modifications may further improve diagnostic and communication methods for people suffering from ABI employing future generation computers.</p></sec><sec id="Sec5"><title>Methods</title><sec id="Sec6"><title>Research design</title><p id="Par34">The experiment consisted of 2 stages. First, a hearing test based upon objective ABR method was conducted to find out, if participants will be able to understand spoken commands from a therapist during a therapeutic session. Next, a series of mental exercises conducted by a therapist took place. During the exercise, a set of activities such as looking at the word spoken by a therapist and simultaneously displayed on the computer monitor was performed. EEG and EGT signals were recorded during each exercise. The result of each session was an anonymized data set consisting of patient data, an EEG recording, an EGT recording, and the answers given to questions asked by the therapist.</p><p id="Par35">The second stage of the experiment helped to extract a set of parameters obtained from the EEG and EGT signals performed in order to find a correlation of these values with the Glasgow Coma Scale (GCS) scores of participants.</p></sec><sec id="Sec7"><title>Participants</title><p id="Par36">All experiments presented in this paper were carried out in the &#x0201c;EPIMIGREN&#x0201d; Neurorehabilitation Medical Centre in Osielsko, Poland. The data were collected during three periods. A total of 33 participants were involved in the study: 10 participants in the first series of experiments, 13 in the second series, and 10 participants in the third one. Experiments were conducted in periods between 12.12.2016&#x02013;31.01.2017 (A), 2017/05/29&#x02013;2017/07/13 (B), and 2018/01/22&#x02013;2018/03/15 (C). The average age of the participants was 45.5&#x000a0;years. The state of the participants&#x02019; consciousness was assessed using the GCS subjective evaluation scale. GCS is one of the most popular methods of subjective assessing the consciousness of people with brain injuries, consisting of an evaluation of eye-opening, the best verbal answer and the best motor reaction caused by external stimuli [<xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR24">24</xref>]. The participants are awarded several points, from 3 pt (related to deep unconsciousness) to 15 pt (reflecting a mild dysfunction). Both the components of the score and their sum were used for further analysis. Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> presents the demographic data of the participants and their GCS scores.<table-wrap id="Tab3"><label>Table&#x000a0;3</label><caption><p>Basic information about participants and their GCS assessment results</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Subject ID</th><th align="left">Age range</th><th align="left">Cause</th><th align="left">Interval post-ictus (months)</th><th align="left">Best eye response</th><th align="left">Best motor response</th><th align="left">Best verbal response</th><th align="left">GCS</th></tr></thead><tbody><tr><td align="left">01</td><td align="left">51&#x02013;60</td><td align="left">Sudden cardiac arrest</td><td char="." align="char">17</td><td char="." align="char">2</td><td char="." align="char">3</td><td char="." align="char">1</td><td char="." align="char">6</td></tr><tr><td align="left">02</td><td align="left">41&#x02013;50</td><td align="left">Traffic accident</td><td char="." align="char">21</td><td char="." align="char">3</td><td char="." align="char">3</td><td char="." align="char">1</td><td char="." align="char">7</td></tr><tr><td align="left">03</td><td align="left">21&#x02013;30</td><td align="left">Sudden cardiac arrest</td><td char="." align="char">17</td><td char="." align="char">3</td><td char="." align="char">2</td><td char="." align="char">2</td><td char="." align="char">7</td></tr><tr><td align="left">04</td><td align="left">21&#x02013;30</td><td align="left">Traffic accident</td><td char="." align="char">24</td><td char="." align="char">4</td><td char="." align="char">3</td><td char="." align="char">1</td><td char="." align="char">8</td></tr><tr><td align="left">05</td><td align="left">51&#x02013;60</td><td align="left">Fall down the stairs</td><td char="." align="char">26</td><td char="." align="char">2</td><td char="." align="char">3</td><td char="." align="char">3</td><td char="." align="char">8</td></tr><tr><td align="left">06</td><td align="left">41&#x02013;50</td><td align="left">Sudden cardiac arrest</td><td char="." align="char">10</td><td char="." align="char">4</td><td char="." align="char">3</td><td char="." align="char">2</td><td char="." align="char">9</td></tr><tr><td align="left">07</td><td align="left">51&#x02013;60</td><td align="left">Sudden cardiac arrest</td><td char="." align="char">12</td><td char="." align="char">4</td><td char="." align="char">4</td><td char="." align="char">1</td><td char="." align="char">9</td></tr><tr><td align="left">08</td><td align="left">21&#x02013;30</td><td align="left">Traffic accident</td><td char="." align="char">37</td><td char="." align="char">4</td><td char="." align="char">4</td><td char="." align="char">2</td><td char="." align="char">10</td></tr><tr><td align="left">09</td><td align="left">51&#x02013;60</td><td align="left">Sudden cardiac arrest</td><td char="." align="char">22</td><td char="." align="char">4</td><td char="." align="char">4</td><td char="." align="char">2</td><td char="." align="char">10</td></tr><tr><td align="left">10</td><td align="left">41&#x02013;50</td><td align="left">Fall from a ladder</td><td char="." align="char">9</td><td char="." align="char">4</td><td char="." align="char">4</td><td char="." align="char">3</td><td char="." align="char">11</td></tr><tr><td align="left">11</td><td align="left">61&#x02013;70</td><td align="left">Sudden cardiac arrest</td><td char="." align="char">10</td><td char="." align="char">2</td><td char="." align="char">2</td><td char="." align="char">1</td><td char="." align="char">5</td></tr><tr><td align="left">12</td><td align="left">41&#x02013;50</td><td align="left">Stroke</td><td char="." align="char">1</td><td char="." align="char">2</td><td char="." align="char">2</td><td char="." align="char">1</td><td char="." align="char">5</td></tr><tr><td align="left">13</td><td align="left">51&#x02013;60</td><td align="left">Fall down the stairs</td><td char="." align="char">18</td><td char="." align="char">2</td><td char="." align="char">3</td><td char="." align="char">1</td><td char="." align="char">6</td></tr><tr><td align="left">14</td><td align="left">41&#x02013;50</td><td align="left">Traffic accident</td><td char="." align="char">44</td><td char="." align="char">2</td><td char="." align="char">3</td><td char="." align="char">2</td><td char="." align="char">7</td></tr><tr><td align="left">15</td><td align="left">51&#x02013;60</td><td align="left">Sudden cardiac arrest</td><td char="." align="char">22</td><td char="." align="char">2</td><td char="." align="char">3</td><td char="." align="char">2</td><td char="." align="char">7</td></tr><tr><td align="left">16</td><td align="left">21&#x02013;30</td><td align="left">Sudden cardiac arrest</td><td char="." align="char">22</td><td char="." align="char">3</td><td char="." align="char">2</td><td char="." align="char">2</td><td char="." align="char">7</td></tr><tr><td align="left">17</td><td align="left">21&#x02013;30</td><td align="left">Traffic accident</td><td char="." align="char">31</td><td char="." align="char">4</td><td char="." align="char">2</td><td char="." align="char">1</td><td char="." align="char">7</td></tr><tr><td align="left">18</td><td align="left">31&#x02013;40</td><td align="left">Sudden cardiac arrest</td><td char="." align="char">5</td><td char="." align="char">4</td><td char="." align="char">1</td><td char="." align="char">2</td><td char="." align="char">7</td></tr><tr><td align="left">19</td><td align="left">41&#x02013;50</td><td align="left">Traffic accident</td><td char="." align="char">26</td><td char="." align="char">3</td><td char="." align="char">3</td><td char="." align="char">2</td><td char="." align="char">8</td></tr><tr><td align="left">20</td><td align="left">61&#x02013;70</td><td align="left">Stroke</td><td char="." align="char">7</td><td char="." align="char">3</td><td char="." align="char">3</td><td char="." align="char">2</td><td char="." align="char">8</td></tr><tr><td align="left">21</td><td align="left">61&#x02013;70</td><td align="left">Traffic accident</td><td char="." align="char">13</td><td char="." align="char">3</td><td char="." align="char">3</td><td char="." align="char">1</td><td char="." align="char">7</td></tr><tr><td align="left">22</td><td align="left">21&#x02013;30</td><td align="left">Traffic accident</td><td char="." align="char">43</td><td char="." align="char">4</td><td char="." align="char">4</td><td char="." align="char">2</td><td char="." align="char">10</td></tr><tr><td align="left">23</td><td align="left">51&#x02013;60</td><td align="left">Stroke</td><td char="." align="char">27</td><td char="." align="char">4</td><td char="." align="char">4</td><td char="." align="char">2</td><td char="." align="char">10</td></tr><tr><td align="left">24</td><td align="left">51&#x02013;60</td><td align="left">Sudden cardiac arrest</td><td char="." align="char">29</td><td char="." align="char">2</td><td char="." align="char">2</td><td char="." align="char">1</td><td char="." align="char">5</td></tr><tr><td align="left">25</td><td align="left">61&#x02013;70</td><td align="left">Stroke</td><td char="." align="char">14</td><td char="." align="char">4</td><td char="." align="char">3</td><td char="." align="char">1</td><td char="." align="char">8</td></tr><tr><td align="left">26</td><td align="left">31&#x02013;40</td><td align="left">Sudden cardiac arrest</td><td char="." align="char">12</td><td char="." align="char">4</td><td char="." align="char">2</td><td char="." align="char">1</td><td char="." align="char">7</td></tr><tr><td align="left">27</td><td align="left">51&#x02013;60</td><td align="left">Sudden cardiac arrest</td><td char="." align="char">24</td><td char="." align="char">4</td><td char="." align="char">1</td><td char="." align="char">1</td><td char="." align="char">6</td></tr><tr><td align="left">28</td><td align="left">51&#x02013;60</td><td align="left">Sudden cardiac arrest</td><td char="." align="char">35</td><td char="." align="char">4</td><td char="." align="char">4</td><td char="." align="char">2</td><td char="." align="char">10</td></tr><tr><td align="left">29</td><td align="left">21&#x02013;30</td><td align="left">Sudden cardiac arrest</td><td char="." align="char">29</td><td char="." align="char">3</td><td char="." align="char">3</td><td char="." align="char">2</td><td char="." align="char">8</td></tr><tr><td align="left">30</td><td align="left">41&#x02013;50</td><td align="left">Traffic accident</td><td char="." align="char">33</td><td char="." align="char">4</td><td char="." align="char">3</td><td char="." align="char">1</td><td char="." align="char">8</td></tr><tr><td align="left">31</td><td align="left">61&#x02013;70</td><td align="left">Sudden cardiac arrest</td><td char="." align="char">8</td><td char="." align="char">4</td><td char="." align="char">3</td><td char="." align="char">1</td><td char="." align="char">8</td></tr><tr><td align="left">32</td><td align="left">11&#x02013;20</td><td align="left">Sudden cardiac arrest</td><td char="." align="char">51</td><td char="." align="char">3</td><td char="." align="char">3</td><td char="." align="char">1</td><td char="." align="char">7</td></tr><tr><td align="left">33</td><td align="left">31&#x02013;40</td><td align="left">Traffic accident</td><td char="." align="char">31</td><td char="." align="char">4</td><td char="." align="char">2</td><td char="." align="char">1</td><td char="." align="char">7</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec8"><title>Measures</title><p id="Par37">Ground truth data related to the state of consciousness of participants were acquired from the expert therapist in the form of GCS scores (in terms of both the partial scores and their sums). The Pearson correlation coefficient was employed to compare GCS and parameters obtained from the unsupervised machine-learning, namely, to the analysis of EEG and EGT signals as it is presented in detail in a subsequent part of the paper.</p></sec><sec id="Sec9"><title>Procedure</title><p id="Par38">We introduced a multi-stage method in our study for multimodal monitoring of ABI participants&#x02019; performance employing human&#x02013;computer interaction. First, the hearing abilities of the participants were tested using auditory brainstem response measurements. This stage allowed for the selection of participants whose hearing abilities were good enough to enable them to understand the verbal commands of the therapist during the mental exercise session. The detailed results obtained for 23 people in the course of the experiments are presented in our paper. Also, 32 participants took part in exercise sessions with the therapist employing a multimedia computer equipped with a gaze tracker and an EEG helmet. The exercises were based upon simple tasks performed using an eye-tracker controller. The participants were encouraged to look at the image displayed on the computer monitor specified by the therapist or to fill a gap in a sentence with one of three words provided. The brain activity of participants was monitored in the course of each session. The data gathered from the eye tracker and the EEG headset were analyzed using a multimodal autoencoder neural network, which is an unsupervised machine learning algorithm use, i.e. for the task of assessing separability of classes in the datasets or learning parameterization techniques which are tailored to a particular problem. We decided to employ a so-called multimodal autoencoder which in addition to the parameterization and clustering of data in the decision space is capable of learning the method to perform a fusion of information gathered from multiple modalities which in case of our study are signals from EEG and EGT [<xref ref-type="bibr" rid="CR25">25</xref>, <xref ref-type="bibr" rid="CR26">26</xref>]. There are many recent examples of the use of autoencoders for such a purpose, i.e. in the field of robotics [<xref ref-type="bibr" rid="CR27">27</xref>&#x02013;<xref ref-type="bibr" rid="CR29">29</xref>].</p><p id="Par39">An advantage of multimodal autoencoders is that they can produce a vector of parameters based on the fusion of data originating from two or more different modalities. The way this fusion is performed is found and optimized during the training of the autoencoder.</p><p id="Par40">Unsupervised machine learning and data exploration algorithms were already used to evaluate the emotional states of participants [<xref ref-type="bibr" rid="CR30">30</xref>&#x02013;<xref ref-type="bibr" rid="CR32">32</xref>], as a diagnostic decision support mechanism in the process of epilepsy treatment [<xref ref-type="bibr" rid="CR33">33</xref>], to provide a way of controlling various devices by disabled people [<xref ref-type="bibr" rid="CR34">34</xref>], and as a basis for stroke rehabilitation [<xref ref-type="bibr" rid="CR35">35</xref>]. Estimation of brain states and activities based on data obtained from biosignal monitoring devices is a challenging concept if people with communication disorders are considered. The estimation of mental activities of the participants together with information related to their performance in the mental exercise sessions may provide a valuable source of information for the therapist deciding on further treatment. Similar mental activities may be defined for instance, in terms of a content of a particular set of values derived from EEG headset used for the monitoring of participant activity. Such an approach may address the problem of evaluating mental activities or mental abilities of people with neurological disorders or those after brain injuries. The monitoring of mental activity can even enhance communication with them by using brain&#x02013;computer interface (BCI) devices [<xref ref-type="bibr" rid="CR36">36</xref>]. Also, the interaction of such people with a computer often requires the usage of some form of BCI such as an eye tracker. Such an interface also becomes a good tool for monitoring the state of such person by analysis of the way it is used for communication with the computer. It may also provide a potential method to compare the GCS score of a patient with the outcomes of assessment made on the base of his or her performance during computer-based mental exercises. In turn, such a measure may be used as a means for tracking patients&#x02019; progress, which could be helpful for a therapist, especially if GCS is not used or was replaced by other measures [<xref ref-type="bibr" rid="CR37">37</xref>].</p></sec><sec id="Sec10"><title>Hearing tests</title><p id="Par41">The first step in the evaluation of a participant with ABI was to verify hearing ability. The ability to hear, and therefore to receive and understand commands, suggestions, and guidelines, is a prerequisite for stimulation using spoken commands. In the present study, auditory evoked potentials (ABR&#x02014;auditory brainstem response) were chosen for assessing hearing abilities of participants, since this method of assessment has numerous advantages: it is non-invasive, painless, and does not involve any complex preparation [<xref ref-type="bibr" rid="CR38">38</xref>, <xref ref-type="bibr" rid="CR39">39</xref>]. The Echodia Elios device was used for ABR measurements, which is portable (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>), its parameters can be set with a touchscreen, and the results are saved in the built-in database [<xref ref-type="bibr" rid="CR40">40</xref>].<fig id="Fig6"><label>Fig.&#x000a0;6</label><caption><p>Device for ABR measurement showing <bold>a</bold> the electrode mounted on the right mastoid and in-the-ear headset; <bold>b</bold> the left-side electrodes and device units (visible on desktop) [<xref ref-type="bibr" rid="CR40">40</xref>]. The data collected can easily be imported to a personal computer for analysis</p></caption><graphic xlink:href="12938_2019_746_Fig6_HTML" id="MO8"/></fig>
</p><p id="Par42">The measurement parameters were as follows:<list list-type="bullet"><list-item><p id="Par43">stimulus: a click;</p></list-item><list-item><p id="Par44">number of stimuli (for each level and each ear): 1000;</p></list-item><list-item><p id="Par45">number of clicks per second: 17;</p></list-item><list-item><p id="Par46">stimulus levels (first series): 60, 40, 25, 10&#x000a0;dB nHL (nHL&#x02014;normal adult hearing level);</p></list-item><list-item><p id="Par47">stimulus levels (second series): 90, 80, 70, 60, 50, 40, 30&#x000a0;dB nHL.</p></list-item></list>
</p><p id="Par48">In the second series, the stimulus level range was broadened in order to assess the participants&#x02019; hearing ability more accurately. A louder stimulus induces stronger responses, which should facilitate the process of the fifth wave detection. The 10-dB nHL level was skipped since the values of the recorded responses were a very low and subjective analysis of the plot might, therefore, have suffered due to severe inaccuracies and, as a consequence, responses obtained for this stimulus during the first series of the study were also omitted.</p><p id="Par49">The device automatically rejected answers to stimuli affected by artifacts coming from participants&#x02019; muscular activity (in that case, the stimulus was automatically repeated). Therefore, the time of any particular measurement varied between 10 and 18&#x000a0;min, depending on the number of rejected answers detected by the device.</p><p id="Par50">The analysis of recorded ABR can be used to evaluate the participants&#x02019; auditory pathway. This feature of the ABR was especially important for the current research. The ABR morphology of all the participants revealed malfunctions in the auditory pathway. The shape of the responses, amplitudes, and latencies of waves significantly differed from those observed in healthy participants. Figure&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref> illustrates the differences between responses observed in a healthy participant and a participant with ABI (participant 07).<fig id="Fig7"><label>Fig.&#x000a0;7</label><caption><p>ABR results in a comparison of <bold>a</bold> healthy person and <bold>b</bold> participant 07. Grey rectangles indicate reference latencies of the 5th wave</p></caption><graphic xlink:href="12938_2019_746_Fig7_HTML" id="MO9"/></fig>
</p><p id="Par51">It was particularly difficult to determine the exact position of the fifth wave in the brainstem response received, thus the final results may suffer from some inaccuracies caused by the subjective analysis of the plot. This issue applies particularly to the lowest stimulus intensity (10&#x000a0;dB nHL) since the values of responses recorded were very low in that case. Nevertheless, some important observations can be made based on the results obtained. The latencies of the 5th wave (Tables&#x000a0;<xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref>) were significantly longer than those observed in healthy participants.<table-wrap id="Tab4"><label>Table&#x000a0;4</label><caption><p>Latencies of the 5th wave for subjects 01&#x02013;10</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Subject ID</th><th align="left" colspan="3">Latency (ms)&#x02014;right ear</th><th align="left" colspan="3">Latency (ms)&#x02014;left ear</th></tr><tr><th align="left">c.i.</th><th align="left">60</th><th align="left">40</th><th align="left">25</th><th align="left">60</th><th align="left">40</th><th align="left">25</th></tr></thead><tbody><tr><td align="left">01</td><td char="." align="char">5.25</td><td char="." align="char">6.63</td><td char="." align="char">7.09</td><td align="left">5.34</td><td char="." align="char">6.75</td><td char="." align="char">8.28</td></tr><tr><td align="left">02</td><td char="." align="char">6.72</td><td char="." align="char">8.06</td><td char="." align="char">8.44</td><td align="left">6.41</td><td char="." align="char">6.97</td><td char="." align="char">8.03</td></tr><tr><td align="left">03</td><td char="." align="char">5.31</td><td char="." align="char">7.56</td><td char="." align="char">8.38</td><td align="left">5.50</td><td char="." align="char">7.63</td><td char="." align="char">8.00</td></tr><tr><td align="left">04</td><td char="." align="char">5.88</td><td char="." align="char">7.34</td><td char="." align="char">8.56</td><td align="left">5.81</td><td char="." align="char">7.59</td><td char="." align="char">8.91</td></tr><tr><td align="left">05</td><td char="." align="char">6.72</td><td char="." align="char">8.66</td><td char="." align="char">8.88</td><td align="left">&#x02013;</td><td char="." align="char">8.88</td><td char="." align="char">9.72</td></tr><tr><td align="left">06</td><td char="." align="char">6.16</td><td char="." align="char">6.34</td><td char="." align="char">7.72</td><td align="left">5.63</td><td char="." align="char">6.63</td><td char="." align="char">7.19</td></tr><tr><td align="left">07</td><td char="." align="char">7.34</td><td char="." align="char">7.28</td><td char="." align="char">8.59</td><td align="left">6.41</td><td char="." align="char">7.31</td><td char="." align="char">9.19</td></tr><tr><td align="left">08</td><td char="." align="char">6.22</td><td char="." align="char">6.59</td><td char="." align="char">8.69</td><td align="left">7.25</td><td char="." align="char">7.91</td><td char="." align="char">9.66</td></tr><tr><td align="left">09</td><td char="." align="char">6.03</td><td char="." align="char">7.31</td><td char="." align="char">8.19</td><td align="left">6.09</td><td char="." align="char">7.19</td><td char="." align="char">8.09</td></tr><tr><td align="left">10</td><td char="." align="char">6.63</td><td char="." align="char">8.00</td><td char="." align="char">9.78</td><td align="left">6.66</td><td char="." align="char">8.03</td><td char="." align="char">9.78</td></tr></tbody></table><table-wrap-foot><p>c.i. denotes click intensity (dB HL)</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab5"><label>Table&#x000a0;5</label><caption><p>Latencies of the 5th wave for subjects 11&#x02013;23, c.i. denotes click intensity [dB HL]</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="5">Subject ID</th><th align="left" colspan="5">Latency (ms)&#x02014;right ear</th><th align="left" colspan="5">Latency (ms)&#x02014;left ear</th></tr><tr><th align="left">c.i.</th><th align="left">90</th><th align="left">80</th><th align="left">70</th><th align="left">60</th><th align="left">50</th><th align="left">40</th><th align="left">30</th><th align="left">90</th><th align="left">80</th><th align="left">70</th><th align="left">60</th><th align="left">50</th><th align="left">40</th><th align="left">30</th></tr></thead><tbody><tr><td align="left">11</td><td char="." align="char">5.34</td><td char="." align="char">5.56</td><td char="." align="char">5.78</td><td align="left">6.44</td><td char="." align="char">6.44</td><td align="left">6.56</td><td align="left">7.28</td><td align="left">4.91</td><td char="." align="char">5.5</td><td char="." align="char">5.88</td><td align="left">6.31</td><td align="left">6.81</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td></tr><tr><td align="left">12</td><td char="." align="char">6,00</td><td char="." align="char">6.25</td><td char="." align="char">6.63</td><td align="left">7.38</td><td char="." align="char">7.59</td><td align="left">8.28</td><td align="left">8.19</td><td align="left">&#x02013;</td><td char="." align="char">7.09</td><td char="." align="char">6.34</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td></tr><tr><td align="left">13</td><td char="." align="char">5.06</td><td char="." align="char">4.88</td><td char="." align="char">5.03</td><td align="left">5.16</td><td char="." align="char">6.34</td><td align="left">7.22</td><td align="left">8.09</td><td align="left">5.41</td><td char="." align="char">5.28</td><td char="." align="char">6.38</td><td align="left">6.19</td><td align="left">7.28</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td></tr><tr><td align="left">14</td><td char="." align="char">6.63</td><td char="." align="char">6.72</td><td char="." align="char">6.81</td><td align="left">6.81</td><td char="." align="char">7.06</td><td align="left">7.06</td><td align="left">7.22</td><td align="left">5.16</td><td char="." align="char">5.47</td><td char="." align="char">5.66</td><td align="left">5.91</td><td align="left">6.16</td><td align="left">7.19</td><td align="left">7.44</td></tr><tr><td align="left">15</td><td char="." align="char">4.81</td><td char="." align="char">5.66</td><td char="." align="char">5.47</td><td align="left">5.75</td><td char="." align="char">6.25</td><td align="left">6.94</td><td align="left">7.16</td><td align="left">5.41</td><td char="." align="char">6.06</td><td char="." align="char">5.97</td><td align="left">6.44</td><td align="left">7.19</td><td align="left">7.75</td><td align="left">8.22</td></tr><tr><td align="left">16</td><td char="." align="char">5.69</td><td char="." align="char">5.59</td><td char="." align="char">6.28</td><td align="left">6.41</td><td char="." align="char">6.91</td><td align="left">8.5</td><td align="left">&#x02013;</td><td align="left">6.16</td><td char="." align="char">6.22</td><td char="." align="char">6.66</td><td align="left">6.72</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td></tr><tr><td align="left">17</td><td char="." align="char">5.22</td><td char="." align="char">5.59</td><td char="." align="char">5.69</td><td align="left">5.47</td><td char="." align="char">6.22</td><td align="left">7.41</td><td align="left">&#x02013;</td><td align="left">5.06</td><td char="." align="char">5.47</td><td char="." align="char">6.53</td><td align="left">6.97</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td></tr><tr><td align="left">18</td><td char="." align="char">5.00</td><td char="." align="char">5.72</td><td char="." align="char">6.13</td><td align="left">&#x02013;</td><td char="." align="char">6.88</td><td align="left">&#x02013;</td><td align="left">7.53</td><td align="left">4.91</td><td char="." align="char">5.28</td><td char="." align="char">5.5</td><td align="left">6.13</td><td align="left">6.81</td><td align="left">7.09</td><td align="left">&#x02013;</td></tr><tr><td align="left">19</td><td char="." align="char">5.63</td><td char="." align="char">5.81</td><td char="." align="char">6.16</td><td align="left">6.31</td><td char="." align="char">7.56</td><td align="left">7.75</td><td align="left">&#x02013;</td><td align="left">5.69</td><td char="." align="char">5.56</td><td char="." align="char">5.81</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">6.91</td><td align="left">&#x02013;</td></tr><tr><td align="left">20</td><td char="." align="char">5.84</td><td char="." align="char">5.47</td><td char="." align="char">6.19</td><td align="left">6.66</td><td char="." align="char">7.28</td><td align="left">7.66</td><td align="left">9.09</td><td align="left">5.34</td><td char="." align="char">6,00</td><td char="." align="char">6.19</td><td align="left">6.59</td><td align="left">6.75</td><td align="left">7.94</td><td align="left">&#x02013;</td></tr><tr><td align="left">21</td><td char="." align="char">5.81</td><td char="." align="char">6.13</td><td char="." align="char">6.22</td><td align="left">6.94</td><td char="." align="char">7.91</td><td align="left">8.16</td><td align="left">&#x02013;</td><td align="left">6.28</td><td char="." align="char">6.84</td><td char="." align="char">7.34</td><td align="left">8.28</td><td align="left">8.72</td><td align="left">9.69</td><td align="left">&#x02013;</td></tr><tr><td align="left">22</td><td char="." align="char">5.09</td><td char="." align="char">5.53</td><td char="." align="char">6.00</td><td align="left">6.03</td><td char="." align="char">6.28</td><td align="left">6.72</td><td align="left">7.25</td><td align="left">5.59</td><td char="." align="char">5.53</td><td char="." align="char">5.97</td><td align="left">6.38</td><td align="left">6.41</td><td align="left">6.56</td><td align="left">6.75</td></tr><tr><td align="left">23</td><td char="." align="char">5.88</td><td char="." align="char">5.84</td><td char="." align="char">6.03</td><td align="left">6.59</td><td char="." align="char">7.09</td><td align="left">7.16</td><td align="left">7.59</td><td align="left">5.56</td><td char="." align="char">5.69</td><td char="." align="char">6.09</td><td align="left">6.41</td><td align="left">6.72</td><td align="left">7.22</td><td align="left">8.03</td></tr></tbody></table></table-wrap>
</p><p id="Par52">However, with decreasing stimulus intensity, latencies elongated systematically (Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>). The above effect was also observed in healthy participants. To conclude, the results of the participants with ABI showed significant abnormalities in the auditory pathway, but they did not seem to exclude the possibility of verbal communication with the participants.<fig id="Fig8"><label>Fig.&#x000a0;8</label><caption><p>Latencies of the 5th wave from all participants plotted against stimulus intensity, <bold>a</bold> participants 1&#x02013;10, <bold>b</bold> participants 11&#x02013;23</p></caption><graphic xlink:href="12938_2019_746_Fig8_HTML" id="MO10"/></fig>
</p></sec><sec id="Sec11"><title>Assessment of participants&#x02019; performance through classification of mental activities</title><p id="Par53">To discover if participants were able to be mentally involved in activities initiated by the therapist, a set of activities was conceived. They are associated either to solving simple puzzles like filling a gap in a sentence with one of three possible words shown on the computer screen or to answering questions asked by the therapist by gazing at the correct answer on the screen, i.e., &#x0201c;yes&#x0201d; or &#x0201c;no&#x0201d; or through selecting displayed objects. The fixation of eye-gaze lasting for longer than 2&#x000a0;s was interpreted as an act of choosing the indicated option. All the activities were performed using the eye-tracker device by Tobii EyeX [<xref ref-type="bibr" rid="CR41">41</xref>]. According to the eye-tracker device technical specification, fixation point coordinates are recorded with more than 60&#x000a0;samples/s. The device was mounted at the bottom of the computer monitor and then used for collecting information related to the areas of the screen that drew the attention of the participant. The participant was located at about 60&#x000a0;cm distance (head to screen surface). The multimodal system for experimenting is depicted in Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref>.<fig id="Fig9"><label>Fig.&#x000a0;9</label><caption><p>The multimodal stand used for performing mental exercises and experiments with participants suffering from traumatic brain injuries. The system programmer tests its operational readiness</p></caption><graphic xlink:href="12938_2019_746_Fig9_HTML" id="MO11"/></fig>
</p><p id="Par54">The setup consists of an EEG headset (placed on the head of the participant), eye tracker, monitor, and video cameras for capturing the overall course of the experiment. The system was controlled by a computer equipped with the developed software for providing stimuli and performing data acquisition (the words displayed are in Polish).</p><p id="Par55">Emotiv INSIGHT and Emotiv EPOC EEG headsets [<xref ref-type="bibr" rid="CR42">42</xref>&#x02013;<xref ref-type="bibr" rid="CR44">44</xref>] were used for acquiring data related to the brain activity of participants while they performed specified tasks. The INSIGHT headset [<xref ref-type="bibr" rid="CR43">43</xref>] consisted of five signal electrodes, AF3, AF4, P7, P8, Pz, and a single reference electrode; the sensors were dry-type. The EPOC EEG headset allowed us to gather data from 14 electrodes, AF3, AF4, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8 (following standardized electrode marks). The above device was equipped with saline-based electrodes. Data were transmitted via a wireless connection to a PC equipped with a USB receiver dongle. The sampling frequency of output signals was equal to 128&#x000a0;Hz. The software provided by the headset manufacturer was employed to get samples of each electrode signal, whereas a self-developed software stored them to .csv file for the usage at further stages of the analysis.</p><p id="Par56">An automatic system for providing various types of tasks and for collecting feedback from the therapist was prepared. A detailed description of each activity, together with its identification number, is listed in Table&#x000a0;<xref rid="Tab6" ref-type="table">6</xref>. The ID numbers of activities were employed for the analysis in subsequent parts of this paper to indicate a set of activities performed by a participant in each session described.<table-wrap id="Tab6"><label>Table&#x000a0;6</label><caption><p>ID numbers and types of activities performed by subjects with the use of eye tracker</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Activity ID</th><th align="left">Description of the activity</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">Typing on the virtual keyboard</td></tr><tr><td align="left">2</td><td align="left">Putting down letters in words associated with displayed images</td></tr><tr><td align="left">3</td><td align="left">Rewriting a word presented on the screen</td></tr><tr><td align="left">4</td><td align="left">Selecting a word spoken by a therapist</td></tr><tr><td align="left">5</td><td align="left">Selecting of a sentence spoken by a therapist</td></tr><tr><td align="left">6</td><td align="left">Matching missing words to gaps in the sentences</td></tr><tr><td align="left">7</td><td align="left">Selecting images specified by a therapist</td></tr><tr><td align="left">8</td><td align="left">Answering yes/no through displayed panel</td></tr></tbody></table></table-wrap>
</p><p id="Par57">The activity of the participant was monitored using an eye-tracker device and cameras. The therapist who had control throughout the session could also report various events that occurred during each performed activity by clicking on appropriate buttons displayed on the control screen of the application. Events were automatically logged in the data file produced by the eye tracker and linked to the record of the current position of the fixation point. Such events could be tracked later on in the post-processing stage and visualized as annotations on the processed EEG signals. The eye tracker has to be calibrated before the experiment. For majority participants with ABI, it is very hard or even impossible to perform calibration of the device through the process consists of fixating the eye gaze on selected points in the corners of the screen lasting about 4&#x000a0;s. In turn, the necessity to follow calibration points by gaze often makes it impossible for participants to perform the calibration. Therefore, the therapist had to plan where the participant will be positioned during the experiment in order to calibrate the device while occupying the same place as the person engaged in the experiment. Events that could be reported in this way were: beginning and ending of an identified exercise, somebody entering the room during the session, the pain felt by the participant, or movement of the participant, among others.</p><p id="Par58">A set of results gathered from 85 therapeutic sessions was collected. Data from electrodes of the headset were stored in the .csv and .edf file formats. The data consisted of EEG records associated with the brain activity of participants, data from the eye-tracker device, and logs of events reported by the therapist. Both 5-channel and 14-channel data were taken into consideration during the post-processing and machine learning-based analysis stage.</p><p id="Par59">Such an approach to signal acquisition permitted to analyze data obtained from both headsets with a single algorithm. One of the problems which were spotted in the initial stage of research is differences in the quality of signals obtained from individual participants. The quality varied depending on the shape of the head of the participant taking part in the experiment and on skin conductivity. Problems with skin contact were especially prominent in the 5-electrode headset. The main problem was noise occurrence caused by the lack of perfect contact between electrode and participants&#x02019; skin. Example of a signal containing only spike-like artifacts occurring due to a sudden loss of contact between the skin an the electrode is depicted in Fig.&#x000a0;<xref rid="Fig10" ref-type="fig">10</xref>. An example of a signal degraded by noise is shown in Fig.&#x000a0;<xref rid="Fig11" ref-type="fig">11</xref>.<fig id="Fig10"><label>Fig.&#x000a0;10</label><caption><p>Example of the signal acquired from a single electrode (AF3) of the 5-electrode EEG headset with electrodes having good contact with the subject&#x02019;s skin</p></caption><graphic xlink:href="12938_2019_746_Fig10_HTML" id="MO12"/></fig>
<fig id="Fig11"><label>Fig.&#x000a0;11</label><caption><p>Example of signal gathered from a single electrode (AF3) of the 5-electrode EEG headset with electrodes having bad contact with the subject&#x02019;s skin</p></caption><graphic xlink:href="12938_2019_746_Fig11_HTML" id="MO13"/></fig>
</p><p id="Par60">We decided to leave also the signals containing visible artifacts in the dataset, as the next step of the processing would involve the use of unsupervised machine learning algorithms&#x02014;a multimodal autoencoder. One of the features of such an algorithm is its ability to find a general model for even a noisy data output. Our dataset also consisted of a large number of epochs which do not contain any artifacts (especially ones utilizing 14 saline-based electrodes). Also, presence of noise is one more reason to process the data with the autoencoder algorithm, as its bottleneck-type structure forces it to seek the best, simple model of data that is received during the training. Noise reduction is one of common applications of autoencoders [<xref ref-type="bibr" rid="CR45">45</xref>, <xref ref-type="bibr" rid="CR46">46</xref>].</p></sec><sec id="Sec12"><title>Post-processing of EEG signals</title><p id="Par61">Computations were performed using software written in Python programming language extended by several scientific calculation libraries, including SciPy [<xref ref-type="bibr" rid="CR47">47</xref>], NumPy [<xref ref-type="bibr" rid="CR48">48</xref>], Keras [<xref ref-type="bibr" rid="CR49">49</xref>], and TensorFlow [<xref ref-type="bibr" rid="CR50">50</xref>]. The input signal obtained from the EEG headset consisted of 5 or 14 electrodes, depending on the type of headset used for the data acquisition. The common electrodes between both headsets are AF3, AF4, P7, and P8. Therefore, to unify the format of all signal sets&#x02014;in place of all signals which were not gathered by a headset, a placeholder signal consisting of a sequence of zeros was introduced. After this step, each set of EEG signals consisted of 15 channels, and it contained either 10 or 1 placeholder signals.</p><p id="Par62">The next step was to split EEG signals into epochs. No overlap was employed for this part of processing. Information about the time of beginning and ending of each frame was also kept to allow synchronization of EEG signal epochs with analogous epochs of signals gathered from the eye tracker. The whole dataset after pre-processing stage consisted of 9436 epochs of EEG and eye-tracker signals. Each of them lasted 12&#x000a0;s, therefore all the epochs are equivalent to 31.5&#x000a0;h of continuous data acquisition. After splitting the signals into epochs, for each channel of EEG in each epoch, a spectrogram was calculated. The size of FFT employed for this task was 256, the overlapping factor of 0.9 was employed. As a windowing function, a Tukey window with a shape parameter of 0.25 was used. It is a default window used in the SciPy library procedure employed for the task of this calculation. Each spectrogram was standardized according to the formula (<xref rid="Equ2" ref-type="">2</xref>):<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S\left( {f,t} \right) = \frac{{S\left( {f,t} \right) - \overline{{S\left( {f,t} \right)}} }}{{{\text{std}}\left( {S\left( {f,t} \right)} \right)}},$$\end{document}</tex-math><mml:math id="M28" display="block"><mml:mrow><mml:mi>S</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mover><mml:mrow><mml:mi>S</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>std</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>S</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="12938_2019_746_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq3"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S\left( {f,t} \right)$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:mi>S</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq3.gif"/></alternatives></inline-formula> denotes a spectrogram which is a function frequency <inline-formula id="IEq4"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f$$\end{document}</tex-math><mml:math id="M32"><mml:mi>f</mml:mi></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq4.gif"/></alternatives></inline-formula> and time <inline-formula id="IEq5"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t$$\end{document}</tex-math><mml:math id="M34"><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq5.gif"/></alternatives></inline-formula>. The mean value of the spectrogram averaged over time and frequency is denoted by <inline-formula id="IEq6"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S\left( {f,t} \right)$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:mi>S</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq6.gif"/></alternatives></inline-formula>, and standard deviation of the spectrogram is denoted by <inline-formula id="IEq7"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{std}}\left( {S\left( {f,t} \right)} \right)$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mtext>std</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>S</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq7.gif"/></alternatives></inline-formula>. An example of spectrogram calculated for one of EEG epochs is depicted in Fig.&#x000a0;<xref rid="Fig12" ref-type="fig">12</xref>.<fig id="Fig12"><label>Fig.&#x000a0;12</label><caption><p>Example of spectrogram calculated for the epoch obtained from the T8 electrode of the 5-electrode headset</p></caption><graphic xlink:href="12938_2019_746_Fig12_HTML" id="MO15"/></fig>
</p></sec><sec id="Sec13"><title>Post-processing of eye-tracker data</title><p id="Par63">Another tool used for monitoring the behavior of the participants was the eye tracker. Results obtained from this device are a series of data points containing 3 coordinates, namely: x and y coordinates of eye-fixation point at the given moment, plus the time index. Eye-tracker signal was initially split into epochs in such a manner, that it always corresponded to one of the frames of EEG signals. Each of these epochs was then further processed by the post-processing algorithm. Data contained by each of eye-tracker signal epochs may be visualized by plotting the trajectory of eye-fixation point movement. However, the form of visualization is illegible if long trajectories are plotted. Therefore, the data were processed in such a way that for each pixel of the computer screen, a frequency of eyepoint fixation occurrence at this point was calculated. The matrix of such values may be treated as an estimate of the probability that the participant was looking at a certain pixel. The matrix may be displayed in a graphical form. This type of visualization is called heatmap. To derive parameters from the 2D graphical representation of the eye-gaze position, projections of the heatmap on the <italic>x</italic>-axis and <italic>y</italic>-axis of the coordinate system were calculated. Illustration of the process and the procedure of displaying the heatmap is shown in Fig.&#x000a0;<xref rid="Fig13" ref-type="fig">13</xref>.<fig id="Fig13"><label>Fig.&#x000a0;13</label><caption><p>Example of calculation of a heatmap for a given matrix of values</p></caption><graphic xlink:href="12938_2019_746_Fig13_HTML" id="MO16"/></fig>
</p><p id="Par64">As eye-tracker signal in such a form is represented simply by a two-dimensional matrix, it is a convenient form of data to be used as an input for the convolutional neural network, which consisted of an autoencoder network employed in our study. Example of heatmap calculated from the data obtained during the experiment is depicted in Fig.&#x000a0;<xref rid="Fig14" ref-type="fig">14</xref>.<fig id="Fig14"><label>Fig.&#x000a0;14</label><caption><p>Example of spectrogram calculated for the epoch obtained from the T8 electrode of the 5-electrode headset</p></caption><graphic xlink:href="12938_2019_746_Fig14_HTML" id="MO17"/></fig>
</p></sec><sec id="Sec14"><title>Analysis of EEG and eye-tracker signals with the multimodal autoencoder neural network</title><p id="Par65">The algorithm of the multimodal autoencoder used to analyze the data from the experiment took as an input the pairs of signal epochs obtained from the EEG and EGT acquisition devices. Each pair of post-processed epochs is processed by the neural network, and the effect of this processing is a vector of 32 floating-point numbers which can be treated as a vector of parameters assigned to each pair of the epoch by the neural network. The particular way of assignment of those vectors is found up by the autoencoder algorithm itself during its training. Schematically, the process of the postprocessing of those signals may be depicted as in Fig.&#x000a0;<xref rid="Fig15" ref-type="fig">15</xref>.<fig id="Fig15"><label>Fig.&#x000a0;15</label><caption><p>Schematic depiction of EGT and EGT signals post-processing and formation of data frames analyzed by the autoencoder neural network</p></caption><graphic xlink:href="12938_2019_746_Fig15_HTML" id="MO18"/></fig>
</p><p id="Par66">Methods of data analysis with embeddings calculated with use of neural networks or other techniques of were proven to be useful in many applications such as text or image processing [<xref ref-type="bibr" rid="CR51">51</xref>, <xref ref-type="bibr" rid="CR52">52</xref>]. There are also examples of such techniques applied to medical data [<xref ref-type="bibr" rid="CR53">53</xref>, <xref ref-type="bibr" rid="CR54">54</xref>]. Auto-extracted features of medical signals were processed with use of methods such as PCA, which may be applied as a tool for visualization of high-dimensional feature vectors calculated by neural networks. The more detailed analysis may be performed with use of standard statistical methods such statistic tests. The approach based upon statistics is more precise in this case because it can be performed without dimensionality reduction, which is caused by use of PCA. In our work we use PCA only for the purpose of visualization. Calculation associated with statistical analysis is performed on data derived from high-dimensional feature vectors (embeddings) generated by employed neural networks.</p><p id="Par67">In the case of our experiment, data from neural network inputs are processed by consecutive layers of the network. After certain amounts of layers, intermediate results of calculations made for EEG and EGT signals are merged and from that point processed together to produce a single vector of parameters consisting of 32 floating-point numbers.</p><p id="Par68">The decoder part of the autoencoder usually has an inversed structure when compared to the encoder, as it performs the inverse operation on the vector of parameters generated by the encoder. The goal during the training of the autoencoder is to minimize the error measured between the input of the encoder and the reconstructed versions of the input obtained from the decoder. In such a way, the autoencoder learns to encode information about the input on the limited number of parameters of which the result vector of parameters consists. The architecture of the encoder and decoder parts of the neural network employed in our study is depicted in Figs.&#x000a0;<xref rid="Fig16" ref-type="fig">16</xref> and <xref rid="Fig17" ref-type="fig">17</xref>. They also provide details of layers used, their activation functions, and numbers of feature maps in each layer of the convolutional subnetworks.<fig id="Fig16"><label>Fig.&#x000a0;16</label><caption><p>Example of spectrogram calculated for the epoch obtained from the T8 electrode of the 5-electrode headset</p></caption><graphic xlink:href="12938_2019_746_Fig16_HTML" id="MO19"/></fig>
<fig id="Fig17"><label>Fig.&#x000a0;17</label><caption><p>Example of spectrogram calculated for the epoch obtained from the T8 electrode of the 5-electrode headset</p></caption><graphic xlink:href="12938_2019_746_Fig17_HTML" id="MO20"/></fig>
</p><p id="Par69">The EEG signal representation is provided to the network as a 5-channel or 14-channel input layer of a convolutional neural network. The exact number of channels depends on whether the input dataset is one obtained with the use of 5-channel or 14-channel EEG headset. The EGT input is a single-channel input layer of the convolutional network. The whole dataset used for training and further analysis of vectors of parameters generated by the autoencoder consisted of 5006 pairs of EEG and EGT signal epochs in case of dataset related to the 5-electrode headset and 3000 pairs in case of one related to the 14-electrode headset.</p><p id="Par70">In each case, the training set consisted of 90% of examples and validation set consisted of 10% of all examples. The advantage of employing the autoencoder algorithm is the fact that it can process heterogeneous data as an input. This is a kind of data we have in our study because they were acquired with the use of two different headsets. The autoencoder is also capable of learning the way of fusion of information obtained from the EEG and EGT modalities and encode it in a single vector of parameters. Stochastic gradient descent was used as a training algorithm, the ADAM learning rate optimization method was employed. The base learning rate for the ADAM optimizer was set to <inline-formula id="IEq8"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{ - 4}$$\end{document}</tex-math><mml:math id="M40"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12938_2019_746_Article_IEq8.gif"/></alternatives></inline-formula>, the rest of the parameters were set to their defaults in the Keras library implementation. The mean squared error was used as a loss function for the optimization algorithm. The total value of the loss was the weighted sum of losses calculated for the two separate outputs of the autoencoder. The weight of the loss of a loss component related to EEG output was equal to 1, the weight of loss component related to EGT was set to 100. The batch size was equal to 128. The algorithm was trained for 200 iterations, each of them lasted for 9&#x000a0;s. Therefore, the total duration of the training was approximately equal to 1&#x000a0;h. The training was performed on Titan RTX graphics card.</p></sec></sec></body><back><glossary><title>Abbreviations</title><def-list><def-item><term>EEG</term><def><p id="Par5">electroencephalography</p></def></def-item><def-item><term>ABR</term><def><p id="Par6">auditory brainstem response</p></def></def-item><def-item><term>EGT</term><def><p id="Par7">eye-gaze tracking</p></def></def-item><def-item><term>GCS</term><def><p id="Par8">Glasgow Coma Scale</p></def></def-item><def-item><term>TBI</term><def><p id="Par9">traumatic brain injury</p></def></def-item><def-item><term>SCA</term><def><p id="Par10">sudden cardiac arrest</p></def></def-item><def-item><term>ABI</term><def><p id="Par11">acquired brain injury</p></def></def-item><def-item><term>CT</term><def><p id="Par12">computed tomography</p></def></def-item><def-item><term>MRI</term><def><p id="Par13">magnetic resonance imaging</p></def></def-item><def-item><term>PET</term><def><p id="Par14">positron emission tomography</p></def></def-item><def-item><term>BCI</term><def><p id="Par15">brain&#x02013;computer interface</p></def></def-item><def-item><term>nHL</term><def><p id="Par16">normal adult hearing level</p></def></def-item><def-item><term>ICA</term><def><p id="Par17">independent component analysis</p></def></def-item><def-item><term>DWT</term><def><p id="Par18">discrete wavelet transform</p></def></def-item></def-list></glossary><fn-group><fn><p><bold>Publisher's Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>The authors wish to thank Dr. med. Krzysztof Nicpo&#x00144;, Director of the Neurorehabilitation Centre, &#x0201c;EPIMIGREN&#x0201d; Department of Rehabilitation Medical Centre in Osielsko, Poland, for allowing experiments to be conducted with the patients.</p></ack><notes notes-type="author-contribution"><title>Authors&#x02019; contributions</title><p>AC designed the experimental procedures and submitted the plan of the study to the ethical committee, then supervised the research carried out. AK designed and conducted analyses based upon data clustering methods. PO conducted and analyzed hearing tests. PS prepared the hardware for experiments and contributed to &#x0201c;<xref rid="Sec1" ref-type="sec">Background</xref>&#x0201d; and &#x0201c;<xref rid="Sec3" ref-type="sec">Discussion</xref>&#x0201d; sections. All authors read and approved the final manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>The project was supported by the National Science Centre on the basis of decision number DEC-2014/15/B/ST7/04724.</p></notes><notes notes-type="data-availability"><title>Availability of data and materials</title><p>The datasets used and/or analyzed during the current study are available from the corresponding author on reasonable request.</p></notes><notes><title>Ethics approval and consent to participate</title><p id="Par71">The research plan was submitted for approval to the ethics committee of the &#x0201c;Collegium Medicum Ludwika Rydygiera&#x0201d; (being a part of Nicolaus Copernicus University), in the city of Bydgoszcz, Poland. The positive decision of this committee (No. KB 115/2017) was issued on February 2nd, 2017. The ethics committee of the Collegium Medicum Ludwika Rydygiera being a part of a large medical university located in the city of Bydgoszcz, Poland, was the body authorized to review the research plan submitted by the &#x0201c;EPIMIGREN&#x0201d; medical care center located in the village of Osielsko, near Bydgoszcz. Written informed consent for participation was obtained from the carers and relatives of all involved participants.</p></notes><notes><title>Consent for publication</title><p id="Par72">Not applicable.</p></notes><notes notes-type="COI-statement"><title>Competing interests</title><p id="Par73">The authors declare that they have no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peeters</surname><given-names>W</given-names></name><name><surname>van den Brande</surname><given-names>R</given-names></name><name><surname>Polinder</surname><given-names>S</given-names></name><name><surname>Brazinova</surname><given-names>A</given-names></name><name><surname>Steyerberg</surname><given-names>EW</given-names></name><name><surname>Lingsma</surname><given-names>HF</given-names></name><name><surname>Maas</surname><given-names>AIR</given-names></name></person-group><article-title>Epidemiology of traumatic brain injury in Europe</article-title><source>Acta Neurochir</source><year>2015</year><volume>157</volume><issue>10</issue><fpage>1683</fpage><lpage>1696</lpage><pub-id pub-id-type="doi">10.1007/s00701-015-2512-7</pub-id><pub-id pub-id-type="pmid">26269030</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lancioni</surname><given-names>GE</given-names></name><name><surname>Bosco</surname><given-names>A</given-names></name><name><surname>O&#x02019;Reilly</surname><given-names>MF</given-names></name><etal/></person-group><article-title>Assessment and intervention with patients with severe disorders of consciousness</article-title><source>Adv Neurodev Disord</source><year>2017</year><volume>1</volume><issue>3</issue><fpage>196</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1007/s41252-017-0025-5</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juan</surname><given-names>E</given-names></name><name><surname>Nguissi</surname><given-names>NAN</given-names></name><name><surname>Tzovara</surname><given-names>A</given-names></name><name><surname>Viceic</surname><given-names>D</given-names></name><name><surname>Rusca</surname><given-names>M</given-names></name><name><surname>Oddo</surname><given-names>M</given-names></name><name><surname>Rossetti</surname><given-names>AO</given-names></name><name><surname>De Lucia</surname><given-names>M</given-names></name></person-group><article-title>Evidence of trace conditioning in comatose patients revealed by the reactivation of EEG responses to alerting sounds</article-title><source>NeuroImage</source><year>2016</year><volume>141</volume><fpage>530</fpage><lpage>541</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.07.039</pub-id><pub-id pub-id-type="pmid">27444570</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Fu</surname><given-names>R</given-names></name><name><surname>Xia</surname><given-names>X</given-names></name><etal/></person-group><article-title>Spatial properties of mismatch negativity in patients with disorders of consciousness</article-title><source>Neurosci Bull</source><year>2018</year><volume>34</volume><issue>4</issue><fpage>700</fpage><lpage>708</lpage><pub-id pub-id-type="doi">10.1007/s12264-018-0260-4</pub-id><pub-id pub-id-type="pmid">30030749</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lugo</surname><given-names>ZR</given-names></name><name><surname>Quitadamo</surname><given-names>LR</given-names></name><name><surname>Bianchi</surname><given-names>L</given-names></name><etal/></person-group><article-title>Cognitive processing in non-communicative patients: what can event-related potentials tell us?</article-title><source>Front Hum Neurosci</source><year>2016</year><pub-id pub-id-type="doi">10.3389/fnhum.2016.00569</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hakozaki</surname><given-names>M</given-names></name><name><surname>Tajino</surname><given-names>T</given-names></name><name><surname>Yamada</surname><given-names>H</given-names></name><name><surname>Hasegawa</surname><given-names>O</given-names></name><name><surname>Tasaki</surname><given-names>K</given-names></name><name><surname>Watanabe</surname><given-names>K</given-names></name><name><surname>Konno</surname><given-names>S</given-names></name></person-group><article-title>Radiological and pathological characteristics of giant cell tumor of bone treated with denosumab</article-title><source>Diagn Pathol</source><year>2014</year><volume>9</volume><issue>11</issue><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1186/1746-1596-9-111</pub-id><pub-id pub-id-type="pmid">24398161</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mari&#x000eb;n</surname><given-names>P</given-names></name><name><surname>Beaton</surname><given-names>A</given-names></name></person-group><article-title>The enigmatic linguistic cerebellum: clinical relevance and unanswered questions on nonmotor speech and language deficits in cerebellar disorders</article-title><source>Cerebellum Ataxias</source><year>2014</year><volume>1</volume><issue>12</issue><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1186/2053-8871-1-12</pub-id><pub-id pub-id-type="pmid">26331025</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>J</given-names></name><name><surname>Cheng</surname><given-names>JL</given-names></name><name><surname>Li</surname><given-names>CF</given-names></name><name><surname>Lian</surname><given-names>YB</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>XP</given-names></name><name><surname>Wang</surname><given-names>CY</given-names></name></person-group><article-title>The findings of CT and MRI in patients with metanephric adenoma</article-title><source>Diagn Pathol</source><year>2016</year><volume>11</volume><issue>104</issue><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1186/s13000-016-0535-x</pub-id><pub-id pub-id-type="pmid">26746436</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>AH</given-names></name><name><surname>Connolly</surname><given-names>JF</given-names></name></person-group><article-title>Finding a way in: a review and practical evaluation of fMRI and EEG for detection and assessment in disorders of consciousness</article-title><source>Neurosci Biobehav Rev</source><year>2013</year><volume>37</volume><issue>8</issue><fpage>1403</fpage><lpage>1419</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2013.05.004</pub-id><pub-id pub-id-type="pmid">23680699</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruno</surname><given-names>M-A</given-names></name><name><surname>Vanhaudenhuyse</surname><given-names>A</given-names></name><name><surname>Schnakers</surname><given-names>C</given-names></name><name><surname>Boly</surname><given-names>M</given-names></name><name><surname>Gosseries</surname><given-names>O</given-names></name><name><surname>Demertzi</surname><given-names>A</given-names></name><name><surname>Laureys</surname><given-names>S</given-names></name></person-group><article-title>Visual fixation in the vegetative state: an observational case series PET study</article-title><source>BMC Neurol</source><year>2010</year><volume>10</volume><issue>35</issue><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1186/1471-2377-10-35</pub-id><pub-id pub-id-type="pmid">20051133</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lundervold</surname><given-names>A</given-names></name></person-group><article-title>On consciousness, resting state fMRI, and neurodynamics</article-title><source>Nonlinear Biomed Phys</source><year>2010</year><volume>4</volume><issue>1</issue><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1186/1753-4631-4-S1-S9</pub-id><pub-id pub-id-type="pmid">20236553</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>N</given-names></name><name><surname>Galanaud</surname><given-names>D</given-names></name><name><surname>Carpentier</surname><given-names>A</given-names></name><name><surname>Naccache</surname><given-names>L</given-names></name><name><surname>Puybasset</surname><given-names>L</given-names></name></person-group><article-title>Clinical review: prognostic value of magnetic resonance imaging in acute brain injury and coma</article-title><source>Crit Care</source><year>2007</year><volume>11</volume><issue>5</issue><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1186/cc6107</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Perri</surname><given-names>C</given-names></name><name><surname>Thibaut</surname><given-names>A</given-names></name><name><surname>Heine</surname><given-names>L</given-names></name><name><surname>Soddu</surname><given-names>A</given-names></name><name><surname>Demertzi</surname><given-names>A</given-names></name><name><surname>Laureys</surname><given-names>S</given-names></name></person-group><article-title>Measuring consciousness in coma and related states</article-title><source>World J Radiol</source><year>2014</year><volume>6</volume><issue>8</issue><fpage>589</fpage><lpage>597</lpage><pub-id pub-id-type="doi">10.4329/wjr.v6.i8.589</pub-id><pub-id pub-id-type="pmid">25170396</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iversen</surname><given-names>I</given-names></name><name><surname>Ghanayim</surname><given-names>N</given-names></name><name><surname>K&#x000fc;bler</surname><given-names>A</given-names></name><name><surname>Neumann</surname><given-names>N</given-names></name><name><surname>Birbaumer</surname><given-names>N</given-names></name><name><surname>Kaiser</surname><given-names>J</given-names></name></person-group><article-title>Conditional associative learning examined in a paralyzed patient with amyotrophic lateral sclerosis using brain-computer interface technology</article-title><source>Behav Brain Funct</source><year>2008</year><volume>4</volume><issue>53</issue><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1186/1744-9081-4-53</pub-id><pub-id pub-id-type="pmid">18173840</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vessoyan</surname><given-names>K</given-names></name><name><surname>Steckle</surname><given-names>G</given-names></name><name><surname>Easton</surname><given-names>B</given-names></name><name><surname>Nichols</surname><given-names>M</given-names></name><name><surname>Mok Siu</surname><given-names>V</given-names></name><name><surname>McDougall</surname><given-names>J</given-names></name></person-group><article-title>Using eye-tracking technology for communication in Rett syndrome: perceptions of impact</article-title><source>Augment Altern Commun</source><year>2018</year><volume>34</volume><issue>3</issue><fpage>230</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1080/07434618.2018.1462848</pub-id><pub-id pub-id-type="pmid">29703090</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malinowska</surname><given-names>U</given-names></name><name><surname>Chatelle</surname><given-names>C</given-names></name><name><surname>Marie-Aur&#x000e9;lie</surname><given-names>B</given-names></name><name><surname>Noirhomme</surname><given-names>Q</given-names></name><name><surname>Laureys</surname><given-names>S</given-names></name><name><surname>Durka</surname><given-names>P</given-names></name></person-group><article-title>Electroencephalographic profiles for differentiation of disorders of consciousness</article-title><source>Biomed Eng Online</source><year>2013</year><volume>12</volume><issue>109</issue><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1186/1475-925X-12-109</pub-id><pub-id pub-id-type="pmid">23289769</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Przyby&#x00142;o</surname><given-names>J</given-names></name><name><surname>Ka&#x00144;toch</surname><given-names>E</given-names></name><name><surname>Augustyniak</surname><given-names>P</given-names></name></person-group><article-title>Eyetracking-based assessment of affect-related decay of human performance in visual tasks</article-title><source>Future Gener Comput Syst</source><year>2019</year><volume>92</volume><fpage>504</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.1016/j.future.2018.02.012</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mena</surname><given-names>JH</given-names></name><name><surname>Sanchez</surname><given-names>AI</given-names></name><name><surname>Rubiano</surname><given-names>AM</given-names></name><name><surname>Peitzman</surname><given-names>AB</given-names></name><name><surname>Sperry</surname><given-names>JL</given-names></name><name><surname>Gutierrez</surname><given-names>MI</given-names></name><name><surname>Puyana</surname><given-names>JC</given-names></name></person-group><article-title>Effect of the modified Glasgow Coma Scale score criteria for mild traumatic brain injury on mortality prediction: comparing classic and modified Glasgow Coma Scale score model scores of 13</article-title><source>J Trauma</source><year>2011</year><volume>71</volume><issue>5</issue><fpage>1185</fpage><lpage>1193</lpage><pub-id pub-id-type="doi">10.1097/TA.0b013e31823321f8</pub-id><pub-id pub-id-type="pmid">22071923</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Szczuko P. Rough set-based classification of EEG signals related to real and imagery motion. In: Proceedings of 2016 signal processing: algorithms, architectures, arrangements, and applications (SPA), Pozna&#x00144;, Poland; 2016. 10.1109/SPA.2016.7763583.</mixed-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szczuko</surname><given-names>P</given-names></name></person-group><article-title>Real and imaginary motion classification based on rough set analysis of EEG signals for multimedia applications</article-title><source>Multimed Tools Appl Multimedia Tools Appl</source><year>2017</year><volume>76</volume><issue>24</issue><fpage>25697</fpage><lpage>25711</lpage><pub-id pub-id-type="doi">10.1007/s11042-017-4458-7</pub-id></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Szczuko</surname><given-names>P</given-names></name><name><surname>Lech</surname><given-names>M</given-names></name><name><surname>Czy&#x0017c;ewski</surname><given-names>A</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Sta&#x00144;czyk</surname><given-names>U</given-names></name><name><surname>Zielosko</surname><given-names>B</given-names></name><name><surname>Jain</surname><given-names>L</given-names></name></person-group><article-title>Comparison of classification methods for EEG signals of real and imaginary motion</article-title><source>Advances in feature selection for data and pattern recognition</source><year>2018</year><publisher-loc>Cham</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>C</given-names></name><name><surname>Blankertz</surname><given-names>B</given-names></name></person-group><article-title>Towards a cure for BCI illiteracy</article-title><source>Brain Topogr</source><year>2010</year><volume>23</volume><issue>2</issue><fpage>194</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1007/s10548-009-0121-6</pub-id><pub-id pub-id-type="pmid">19946737</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teasdale</surname><given-names>G</given-names></name><name><surname>Jennett</surname><given-names>B</given-names></name></person-group><article-title>Assessment of coma and impaired consciousness. A practical scale</article-title><source>Lancet</source><year>1974</year><volume>304</volume><issue>7872</issue><fpage>81</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(74)91639-0</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNett</surname><given-names>M</given-names></name></person-group><article-title>A review of the predictive ability of Glasgow Coma Scale scores in head-injured patients</article-title><source>J Neurosci Nurs</source><year>2007</year><volume>39</volume><issue>2</issue><fpage>68</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1097/01376517-200704000-00002</pub-id><pub-id pub-id-type="pmid">17477220</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baltru&#x00161;aitis</surname><given-names>T</given-names></name><name><surname>Ahuja</surname><given-names>C</given-names></name><name><surname>Morency</surname><given-names>L</given-names></name></person-group><article-title>Multimodal machine learning: a survey and taxonomy</article-title><source>IEEE Trans Pattern Anal Mach Intell</source><year>2019</year><volume>41</volume><issue>2</issue><fpage>423</fpage><lpage>443</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2018.2798607</pub-id><pub-id pub-id-type="pmid">29994351</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Jiquan N, Khosla A, Kim M, Nam J, Lee H, Ng AY. Multimodal deep learning. In: ICML; 2011.</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Cadena C, Dick AR, Reid ID. Multi-modal auto-encoders as joint estimators for robotics scene understanding. In: Proceedings of robotics: science and systems conference, Arbor, Michigan, USA; 2016. 10.15607/RSS.2016.XII.041.</mixed-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Droniou</surname><given-names>A</given-names></name><name><surname>Ivaldi</surname><given-names>S</given-names></name><name><surname>Sigaud</surname><given-names>O</given-names></name></person-group><article-title>Deep unsupervised network for multimodal perception, representation and classification</article-title><source>Robot Auton Syst</source><year>2015</year><volume>71</volume><fpage>83</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.robot.2014.11.005</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choi</surname><given-names>J-H</given-names></name><name><surname>Lee</surname><given-names>J-S</given-names></name></person-group><article-title>EmbraceNet: a robust deep learning architecture for multimodal classification</article-title><source>Inf Fusion</source><year>2019</year><volume>51</volume><fpage>259</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.inffus.2019.02.010</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Maca&#x00161; M, Vavrecka M, Gerla V, Lhotsk&#x000e1; L. Classification of the emotional states based on the EEG signal processing. In: Proceedings of the 9th international conference in information technology and applications in biomedicine, Larnaca, Cyprus. 2009;1&#x02013;4. <pub-id pub-id-type="doi">10.1109/ITAB.2009.5394429</pub-id>.</mixed-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenke</surname><given-names>R</given-names></name><name><surname>Peer</surname><given-names>A</given-names></name><name><surname>Buss</surname><given-names>M</given-names></name></person-group><article-title>Feature extraction and selection for emotion recognition from EEG</article-title><source>IEEE Trans Affect Comput</source><year>2014</year><volume>5</volume><issue>3</issue><fpage>327</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1109/TAFFC.2014.2339834</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelstra</surname><given-names>S</given-names></name><name><surname>Muhl</surname><given-names>C</given-names></name><name><surname>Soleymani</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>J-S</given-names></name><name><surname>Yazdani</surname><given-names>A</given-names></name><etal/></person-group><article-title>DEAP: a database for emotion analysis using physiological signals</article-title><source>IEEE Trans Affect Comput</source><year>2011</year><volume>3</volume><issue>1</issue><fpage>18</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1109/T-AFFC.2011.15</pub-id></element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orhan</surname><given-names>U</given-names></name><name><surname>Hekim</surname><given-names>M</given-names></name><name><surname>Ozer</surname><given-names>M</given-names></name></person-group><article-title>EEG signals classification using the K-means clustering and multilayer perceptron neural network model</article-title><source>Expert Syst Appl</source><year>2011</year><volume>38</volume><issue>10</issue><fpage>13475</fpage><lpage>13481</lpage><pub-id pub-id-type="doi">10.1016/j.eswa.2011.04.149</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>G&#x000fc;rk&#x000f6;k</surname><given-names>H</given-names></name><name><surname>Nijholt</surname><given-names>A</given-names></name></person-group><article-title>Brain-computer interfaces for multimodal interaction: a survey and principles</article-title><source>Int J Hum Comput Interact</source><year>2011</year><volume>28</volume><issue>5</issue><fpage>292</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1080/10447318.2011.582022</pub-id></element-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leamy</surname><given-names>DJ</given-names></name><name><surname>Kocijan</surname><given-names>J</given-names></name><name><surname>Domijan</surname><given-names>K</given-names></name><name><surname>Duffin</surname><given-names>J</given-names></name><name><surname>Roche</surname><given-names>RAP</given-names></name><name><surname>Commins</surname><given-names>S</given-names></name><name><surname>Collins</surname><given-names>R</given-names></name><name><surname>Ward</surname><given-names>TE</given-names></name></person-group><article-title>An exploration of EEG features during recovery following stroke&#x02014;implications for BCI-mediated neurorehabilitation therapy</article-title><source>J Neuroeng Rehabilit</source><year>2014</year><volume>11</volume><issue>9</issue><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1186/1743-0003-11-9</pub-id></element-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Pan</surname><given-names>J</given-names></name><name><surname>He</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name><name><surname>Laureys</surname><given-names>S</given-names></name><name><surname>Xie</surname><given-names>Q</given-names></name><name><surname>Yu</surname><given-names>R</given-names></name></person-group><article-title>Detecting number processing and mental calculation in patients with disorders of consciousness using a hybrid brain-computer interface system</article-title><source>BMC Neurol</source><year>2015</year><volume>15</volume><issue>259</issue><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1186/s12883-015-0521-z</pub-id><pub-id pub-id-type="pmid">25595849</pub-id></element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giacino</surname><given-names>JT</given-names></name><name><surname>Kalmar</surname><given-names>K</given-names></name><name><surname>Whyte</surname><given-names>J</given-names></name></person-group><article-title>The JFK coma recovery scale-revised: measurement characteristics and diagnostic utility</article-title><source>Arch Phys Med Rehabil</source><year>2004</year><volume>85</volume><issue>12</issue><fpage>2020</fpage><lpage>2029</lpage><pub-id pub-id-type="doi">10.1016/j.apmr.2004.02.033</pub-id><pub-id pub-id-type="pmid">15605342</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodrigues</surname><given-names>RA</given-names></name><name><surname>Busssiere</surname><given-names>M</given-names></name><name><surname>Froeschl</surname><given-names>M</given-names></name><name><surname>Nathan</surname><given-names>HJ</given-names></name></person-group><article-title>Auditory-evoked potentials during coma: do they improve our prediction of awakening in comatose patients?</article-title><source>J Crit Care</source><year>2014</year><volume>29</volume><issue>1</issue><fpage>93</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1016/j.jcrc.2013.08.020</pub-id><pub-id pub-id-type="pmid">24125771</pub-id></element-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skoe</surname><given-names>E</given-names></name><name><surname>Kraus</surname><given-names>N</given-names></name></person-group><article-title>Auditory brainstem response to complex sounds: a tutorial</article-title><source>Ear Hear</source><year>2010</year><volume>31</volume><issue>3</issue><fpage>302</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1097/AUD.0b013e3181cdb272</pub-id><pub-id pub-id-type="pmid">20084007</pub-id></element-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Echodia. Echodia Elios user guide. 2015. ver. 2.1.1. Saint Beauzire, France; 2015.</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Tobii. Tobii&#x02014;EyeX controller technical specification; 2017. <ext-link ext-link-type="uri" xlink:href="http://www.tobii.com/xperience/products/#Specification">http://www.tobii.com/xperience/products/#Specification</ext-link>.</mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">Emotiv. Emotiv insight user manual, Revision 1.0, user manual provided by the manufacturer with the hardware equipment; 2015.</mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Emotiv. Emotiv EPOC user manual. <ext-link ext-link-type="uri" xlink:href="https://emotiv.zendesk.com/hc/en-us/articles/201222455-Where-can-I-find-a-user-manual">https://emotiv.zendesk.com/hc/en-us/articles/201222455-Where-can-I-find-a-user-manual</ext-link>. Accessed 14 Nov 2018.</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Emotiv. Manufacturers web site available at <ext-link ext-link-type="uri" xlink:href="https://emotiv.zendesk.com">https://emotiv.zendesk.com</ext-link>.</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Vincent P, Larochelle H, Bengio Y, Manzagol PA. Extracting and composing robust features with denoising autoencoders. In: Proceedings of the 25th international conference on Machine learning ICML &#x02018;08, New York, NY, USA. 2008;1096&#x02013;103. 10.1145/1390156.1390294.</mixed-citation></ref><ref id="CR46"><label>46.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name></person-group><source>Deep learning</source><year>2016</year><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Jones E, et al. SciPy. Open source scientific tools for python; 2001. <ext-link ext-link-type="uri" xlink:href="http://www.scipy.org">http://www.scipy.org</ext-link>. Accessed 21 Dec 2018.</mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">NumPy. Package for scientific computing with Python; 2017. <ext-link ext-link-type="uri" xlink:href="http://www.numpy.org">http://www.numpy.org</ext-link>. Accessed 21 Dec 2018.</mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Chollet F. Keras machine learning library; 2015. Software available at <ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link>.</mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Abadi M, et al. TensorFlow: large-scale machine learning on heterogeneous systems; 2015. Software available from <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org">https://www.tensorflow.org</ext-link>.</mixed-citation></ref><ref id="CR51"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Salakhutdinov</surname><given-names>RR</given-names></name></person-group><article-title>Reducing the dimensionality of data with neural networks</article-title><source>Science</source><year>2006</year><pub-id pub-id-type="doi">10.1126/science.1127647</pub-id></element-citation></ref><ref id="CR52"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Afzal</surname><given-names>N</given-names></name><name><surname>Rastegar-Mojarad</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Shen</surname><given-names>F</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name></person-group><article-title>A comparison of word embeddings for biomedical natural language processing</article-title><source>J Biomed Inform</source><year>2018</year><volume>87</volume><fpage>12</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1016/j.jbi.2018.09.008</pub-id><pub-id pub-id-type="pmid">30217670</pub-id></element-citation></ref><ref id="CR53"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tran</surname><given-names>T</given-names></name><name><surname>Luo</surname><given-names>W</given-names></name><name><surname>Phung</surname><given-names>D</given-names></name><name><surname>Gupta</surname><given-names>S</given-names></name><name><surname>Rana</surname><given-names>S</given-names></name><name><surname>Kennedy</surname><given-names>R</given-names></name><name><surname>Larkins</surname><given-names>A</given-names></name><name><surname>Venkatesh</surname><given-names>S</given-names></name></person-group><article-title>A framework for feature extraction from hospital medical data with applications in risk prediction</article-title><source>BMC Bioinform</source><year>2014</year><volume>15</volume><fpage>6596</fpage><pub-id pub-id-type="doi">10.1186/s12859-014-0425-8</pub-id></element-citation></ref><ref id="CR54"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buci&#x00144;ski</surname><given-names>A</given-names></name><name><surname>B&#x00105;czek</surname><given-names>T</given-names></name><name><surname>Krysi&#x00144;ski</surname><given-names>J</given-names></name><name><surname>Szoszkiewicz</surname><given-names>R</given-names></name><name><surname>Za&#x00142;uski</surname><given-names>J</given-names></name></person-group><article-title>Clinical data analysis using artificial neural networks (ANN) and principal component analysis (PCA) of patients with breast cancer after mastectomy</article-title><source>Rep Pract Oncol Radiother</source><year>2007</year><volume>12</volume><issue>1</issue><fpage>9</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1016/S1507-1367(10)60036-3</pub-id></element-citation></ref></ref-list></back></article>