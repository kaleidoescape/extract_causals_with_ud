<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">31923214</article-id><article-id pub-id-type="pmc">6953780</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0226345</article-id><article-id pub-id-type="publisher-id">PONE-D-19-06175</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Earth Sciences</subject><subj-group><subject>Geomorphology</subject><subj-group><subject>Topography</subject><subj-group><subject>Landforms</subject><subj-group><subject>Valleys</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Reptiles</subject><subj-group><subject>Squamates</subject><subj-group><subject>Snakes</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Birds</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Mathematical and Statistical Techniques</subject><subj-group><subject>Mathematical Functions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Earth Sciences</subject><subj-group><subject>Atmospheric Science</subject><subj-group><subject>Meteorology</subject><subj-group><subject>Clouds</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject><subj-group><subject>Image Analysis</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Color image segmentation using adaptive hierarchical-histogram thresholding</article-title><alt-title alt-title-type="running-head">Color image segmentation using adaptive hierarchical-histogram thresholding</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5428-6276</contrib-id><name><surname>Li</surname><given-names>Min</given-names></name><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Funding acquisition</role><role content-type="http://credit.casrai.org/">Software</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="aff" rid="aff002"><sup>2</sup></xref><xref ref-type="corresp" rid="cor001">*</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Lei</given-names></name><role content-type="http://credit.casrai.org/">Validation</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="aff" rid="aff002"><sup>2</sup></xref><xref ref-type="corresp" rid="cor001">*</xref></contrib><contrib contrib-type="author"><name><surname>Deng</surname><given-names>Shaobo</given-names></name><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Software</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="aff" rid="aff002"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Chunhua</given-names></name><role content-type="http://credit.casrai.org/">Funding acquisition</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Nanchang Institute of Technology, Nanchang, Jiangxi, PR China</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Jiangxi Province Key Laboratory of Water Information Cooperative Sensing and Intelligent Processing, Nanchang, Jiangxi, PR China</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>School of Life Sciences, Nanchang University, Nanchang, Jiangxi, PR China</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Wang</surname><given-names>Yuanquan</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1"><addr-line>Beijing University of Technology, CHINA</addr-line></aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>liminghuadi@hotmail.com</email> (ML); <email>ezhoulei@163.com</email> (LW)</corresp></author-notes><pub-date pub-type="epub"><day>10</day><month>1</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>15</volume><issue>1</issue><elocation-id>e0226345</elocation-id><history><date date-type="received"><day>3</day><month>3</month><year>2019</year></date><date date-type="accepted"><day>25</day><month>11</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; 2020 Li et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Li et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0226345.pdf"/><abstract><p>Histogram-based thresholding is one of the widely applied techniques for conducting color image segmentation. The key to such techniques is the selection of a set of thresholds that can discriminate objects and background pixels. Many thresholding techniques have been proposed that use the shape information of histograms and identify the optimum thresholds at valleys. In this work, we introduce the novel concept of a hierarchical-histogram, which corresponds to a multigranularity abstraction of the color image. Based on this, we present a new histogram thresholding&#x02014;Adaptive Hierarchical-Histogram Thresholding (AHHT) algorithm, which can adaptively identify the thresholds from valleys. The experimental results have demonstrated that the AHHT algorithm can obtain better segmentation results compared with the histon-based and the roughness-index-based techniques with drastically reduced time complexity.</p></abstract><funding-group><award-group id="award001"><funding-source><institution>National Science Foundation of China</institution></funding-source><award-id>61562061</award-id><principal-award-recipient><name><surname>Wang</surname><given-names>Lei</given-names></name></principal-award-recipient></award-group><funding-statement>Research on this work was partially supported by the grants from Jiangxi Education Department (No. GJJ151126), the funds from Science and Technology Support Foundation of Jiangxi Province (No.20161BBE50050, 20161BBE50051), by the funds from the National Science Foundation of China (No. 61363047, 61562061).</funding-statement></funding-group><counts><fig-count count="9"/><table-count count="6"/><page-count count="24"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All relevant data are within the manuscript and its Supporting Information files.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All relevant data are within the manuscript and its Supporting Information files.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Image segmentation plays a crucial role in the areas of image analysis, pattern recognition and computer vision-related applications. In segmentation, an image is partitioned into different nonoverlapping regions that are homogenous with respect to certain properties, such as color information, edges, and texture [<xref rid="pone.0226345.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0226345.ref002" ref-type="bibr">2</xref>]. Although many techniques for image segmentation have been proposed, it is still a very challenging research topic due to the variety and complexity of images. Moreover, color images can provide richer information than grayscale images, and natural color image segmentation is increasingly paid more attention by scholars.</p><p>Generally, image segmentation routines are divided into histogram-based approaches [<xref rid="pone.0226345.ref003" ref-type="bibr">3</xref>&#x02013;<xref rid="pone.0226345.ref005" ref-type="bibr">5</xref>], edge detection approaches [<xref rid="pone.0226345.ref006" ref-type="bibr">6</xref>, <xref rid="pone.0226345.ref007" ref-type="bibr">7</xref>], region-based approaches [<xref rid="pone.0226345.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0226345.ref009" ref-type="bibr">9</xref>], clustering approaches [<xref rid="pone.0226345.ref010" ref-type="bibr">10</xref>&#x02013;<xref rid="pone.0226345.ref014" ref-type="bibr">14</xref>] and combinations of several approaches [<xref rid="pone.0226345.ref015" ref-type="bibr">15</xref>&#x02013;<xref rid="pone.0226345.ref017" ref-type="bibr">17</xref>]. Although a larger number of segmentation algorithms have been developed, each has its own applicability and limitations. The properties of these techniques have been discussed in Ref [<xref rid="pone.0226345.ref018" ref-type="bibr">18</xref>].</p><p>One of the most widely applied techniques for image segmentation is histogram-based thresholding, which assumes that homogeneous objects in the image manifest themselves as clusters. The key to the histogram-based technique is the selection of a set of thresholds that can discriminate objects and background pixels. Numerous histogram-based thresholding methods have been proposed over the years. These methods can be broadly classified into two categories. The first category contains thresholding techniques that determine the optimal thresholds by optimizing a certain objective function [<xref rid="pone.0226345.ref019" ref-type="bibr">19</xref>&#x02013;<xref rid="pone.0226345.ref025" ref-type="bibr">25</xref>]. Among these thresholding techniques, entropy-based approaches are the most popular, and many algorithms have been proposed in this direction. Examples of these include Shannon Entropy, Renyi&#x02032;s entropy [<xref rid="pone.0226345.ref024" ref-type="bibr">24</xref>,<xref rid="pone.0226345.ref026" ref-type="bibr">26</xref>], entropic correlation [<xref rid="pone.0226345.ref005" ref-type="bibr">5</xref>], and cross entropy [<xref rid="pone.0226345.ref020" ref-type="bibr">20</xref>]. However, the main problem associated with these algorithms is their large time complexity. For the multilevel thresholding problem in Minimum Cross Entropy Thresholding [<xref rid="pone.0226345.ref025" ref-type="bibr">25</xref>,<xref rid="pone.0226345.ref027" ref-type="bibr">27</xref>], the time complexity is <italic>O</italic>(<italic>mL</italic><sup><italic>m</italic>+1</sup>), where <italic>m</italic> represents the number of threshold values and <italic>L</italic> indicates the number of gray levels. The second category contains approaches that determine the optimal thresholds by utilizing shape information of the histogram of a given image. The rationale for threshold determination implicitly relies on the assumption that the intensities of pixels, or data in a more general setting, should be similar within the same objects and different between different objects [<xref rid="pone.0226345.ref016" ref-type="bibr">16</xref>]. In this manner, the intensity-level histogram values of each object could appear as a bell-shaped mode [<xref rid="pone.0226345.ref019" ref-type="bibr">19</xref>]. The peak of the bell-shaped region and its adjacent position intensity correspond to the main-body pixels of the object, while the boundary of the bell-shaped region corresponds to the edge pixels of the object. Therefore, the peaks and valleys in the histogram are used to locate the clusters in the image, and the optimum thresholds must be located in the valley regions. For example, Rosenfeld <italic>et al</italic>. investigated histogram concavity analysis as an approach for threshold selection [<xref rid="pone.0226345.ref028" ref-type="bibr">28</xref>]. Lim and Lee presented a valley-seeking approach that smoothes the histogram and detects the valleys as thresholds by calculating the derivatives of the smoothed histogram [<xref rid="pone.0226345.ref029" ref-type="bibr">29</xref>]. Because the histogram only includes the information of intensity levels, these methods do not consider the spatial correlation of the same or similarly valued elements. To overcome this drawback, some variations of the histogram are presented. Mohabey and Ray [<xref rid="pone.0226345.ref030" ref-type="bibr">30</xref>, <xref rid="pone.0226345.ref031" ref-type="bibr">31</xref>] utilized rough set theory [<xref rid="pone.0226345.ref032" ref-type="bibr">32</xref>] to construct the concept of a histon. Different from a histogram, each bin of a histon is the pixel scale belonging to the corresponding intensity with uncertainty [<xref rid="pone.0226345.ref002" ref-type="bibr">2</xref>]. With the aid of rough set theory, the histogram and histon can be respectively considered as the lower and upper approximations. Mushrif and Ray then proposed using the roughness measure at every intensity level to extract the homogeneous regions of a color image [<xref rid="pone.0226345.ref033" ref-type="bibr">33</xref>]. For some images, however, it is difficult to obtain the significant peaks and valleys of the roughness measure; Xie <italic>et al</italic>. used local polynomial regression to smooth the histogram and histon and then calculated the roughness measure, which enabled their approach to find the real peaks and valleys more easily [<xref rid="pone.0226345.ref034" ref-type="bibr">34</xref>].</p><p>Similar to the histogram, both the histon and roughness indexes provide the global information of homogeneous regions in the image, and every peak and its adjacent position represent a homogeneous region. Theory analysis shows that the histon pays little attention to the small homogenous regions, and the roughness index can effectively indicate the region homogeneity degree and avoid the disturbance of imbalanced color distribution. As two variants of the histogram, the histon and roughness index were demonstrated to achieve better segmentation results. Both histon-based and roughness-index-based algorithms, however, need to calculate the color difference between every pixel and its neighborhood, which means both require significant time. The steps of the above techniques involve some smoothing of the histogram (histon or roughness-index) data, searching for significant modes, and placing thresholds at the minima between them.</p><p>In this paper, we propose an original segmentation scheme named <italic>AHHT</italic> (Adaptive Hierarchical-Histogram Thresholding), which uses a structure called the <italic>hierarchical-histogram</italic> to adaptively identify the thresholds at valleys for thresholding. A <italic>hierarchical-histogram</italic> includes a group of histograms that corresponds to a multigranularity abstraction of the image. The lower the histogram is in the <italic>hierarchical-histogram</italic>, the more elaborate the details of the image it pertains to are. The role of the prior-level histogram in the <italic>hierarchical-histogram</italic> is generating for the next-level histogram, and the top-level histogram is applied to segment the image. To verify the effectiveness of <italic>AHHT</italic>, experiments are performed on the Berkeley Segmentation Data Set and Benchmark, and a comparison with the histon-based technique and roughness-index-based technique is made in terms of both visual and quantitative evaluations.</p><p>This paper is organized as follows. Section 2 reviews the related work. Section 3.1 describes the main idea of the proposed <italic>AHHT</italic> algorithm. Section 3.2 presents the <italic>AHHT</italic> algorithm in detail. Section 3.3 analyzes the complexity of the <italic>AHHT</italic> algorithm. Section 4 analyzes the experimental results. Section 5 concludes the paper.</p></sec><sec id="sec002"><title>Related work</title><p><italic>RGB</italic> is the most commonly used model for the television systems and pictures acquired by digital cameras. As discussed in other related works, this paper also focuses on color image segmentation in the <italic>RGB</italic> color space. Consider <italic>I</italic> to be an <italic>RGB</italic> image of size <italic>M</italic> &#x000d7; <italic>N</italic>, consisting of three primary color components: red <italic>R</italic>, green <italic>G</italic>, and blue <italic>B</italic>. The classic histogram of the image for each color component is defined as
<disp-formula id="pone.0226345.e001"><alternatives><graphic xlink:href="pone.0226345.e001.jpg" id="pone.0226345.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>&#x003b4;</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="10pt"/><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="4pt"/><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>l</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mspace width="4pt"/><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></alternatives><label>(1)</label></disp-formula>
where <inline-formula id="pone.0226345.e002"><alternatives><graphic xlink:href="pone.0226345.e002.jpg" id="pone.0226345.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mi>&#x003b4;</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives></inline-formula> is the indicator function and <italic>L</italic> is the intensity scale in each of the color components. The value <italic>h</italic><sub><italic>i</italic></sub>(<italic>l</italic>) is the number of pixels having intensity <italic>l</italic> in color component <italic>i</italic>.</p><p>Let <italic>c</italic><sub>1</sub> and <italic>c</italic><sub>2</sub> be color vectors in the <italic>RGB</italic> color space. The Euclidean distance between the two vectors is given by
<disp-formula id="pone.0226345.e003"><alternatives><graphic xlink:href="pone.0226345.e003.jpg" id="pone.0226345.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msqrt><mml:mo>.</mml:mo></mml:math></alternatives><label>(2)</label></disp-formula></p><p>For a <italic>P</italic> &#x000d7; <italic>Q</italic> neighborhood around a pixel <italic>I</italic>(<italic>m</italic>, <italic>n</italic>), the color difference between <italic>I</italic>(<italic>m</italic>, <italic>n</italic>) and its surrounding pixels in neighborhood is defined as [<xref rid="pone.0226345.ref033" ref-type="bibr">33</xref>]:
<disp-formula id="pone.0226345.e004"><alternatives><graphic xlink:href="pone.0226345.e004.jpg" id="pone.0226345.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:msub><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:math></alternatives><label>(3)</label></disp-formula></p><p>If the color difference <italic>d</italic><sub><italic>T</italic></sub>(<italic>m</italic>, <italic>n</italic>) is less than a threshold <italic>T</italic><sub>0</sub>, the surrounding pixels in neighborhood fall in the sphere of a similar color. For an <italic>RGB</italic> image <italic>I</italic> of size <italic>M</italic> &#x000d7; <italic>N</italic>, a matrix <italic>I</italic>&#x02032; of size <italic>M</italic> &#x000d7; <italic>N</italic> is defined such that an element <italic>I</italic>&#x02032;(<italic>m</italic>, <italic>n</italic>) is given by
<disp-formula id="pone.0226345.e005"><alternatives><graphic xlink:href="pone.0226345.e005.jpg" id="pone.0226345.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:maligngroup/><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>.</mml:mo></mml:math></alternatives><label>(4)</label></disp-formula></p><p>Then, the histon is defined as follows [<xref rid="pone.0226345.ref031" ref-type="bibr">31</xref>]:
<disp-formula id="pone.0226345.e006"><alternatives><graphic xlink:href="pone.0226345.e006.jpg" id="pone.0226345.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mi>&#x003b4;</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="10pt"/><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="4pt"/><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>l</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mspace width="4pt"/><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></alternatives><label>(5)</label></disp-formula></p><p>The histogram and the histon can be associated with the concept of approximation space in rough set theory [<xref rid="pone.0226345.ref032" ref-type="bibr">32</xref>,<xref rid="pone.0226345.ref035" ref-type="bibr">35</xref>]. For intensity class <italic>l</italic>, the value of <italic>h</italic><sub><italic>i</italic></sub>(<italic>l</italic>) is the number of pixels that have intensity value <italic>l</italic> and therefore can be viewed as the lower approximation, and the value of <inline-formula id="pone.0226345.e007"><alternatives><graphic xlink:href="pone.0226345.e007.jpg" id="pone.0226345.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> can be considered as the upper approximation. Mushrif and Ray then proposed the roughness measure as follows [<xref rid="pone.0226345.ref033" ref-type="bibr">33</xref>].</p><disp-formula id="pone.0226345.e008"><alternatives><graphic xlink:href="pone.0226345.e008.jpg" id="pone.0226345.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:msub><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="10pt"/><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="4pt"/><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>l</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mspace width="4pt"/><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></alternatives><label>(6)</label></disp-formula><p>Like the histogram, the histon and the roughness index for all intensity values also give the global information of homogeneous regions in the image, and every peak and its adjacent position represent a homogeneous region. Therefore, the histon and the roughness index are two variations of the histogram. The histogram, the histon and the roughness index are collectively called histogram-based techniques in this paper. The segmentation process of such histogram-based techniques is divided into three stages [<xref rid="pone.0226345.ref033" ref-type="bibr">33</xref>], as shown in <xref ref-type="fig" rid="pone.0226345.g001">Fig 1</xref>.</p><fig id="pone.0226345.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0226345.g001</object-id><label>Fig 1</label><caption><title>Flowchart of the histogram-based technique.</title></caption><graphic xlink:href="pone.0226345.g001"/></fig><p>We take the roughness-index-based [<xref rid="pone.0226345.ref033" ref-type="bibr">33</xref>] technique for example to illustrate the flowchart of <xref ref-type="fig" rid="pone.0226345.g001">Fig 1</xref>. First, the roughness index for each plane of <italic>R</italic>, <italic>G</italic> and <italic>B</italic> of the image is calculated. Then, two criteria are used to obtain the significant peaks of the roughness indexes: (1) the height of the peak is greater than 20% of the average value of all peaks; and (2) the distance between two peaks is greater than 10. After the significant peaks are selected, the thresholds are identified at the minima between every two adjacent significant peaks. Second, all selected thresholds are applied to split the image into multiple clusters. The color representing each cluster is obtained by averaging all the pixels within the cluster. At this point, the initial segmentation is completed. Generally, this process usually results in over-segmentation. Lastly, the <italic>Region-Merging</italic> process uses the algorithm proposed by Cheng et al. [<xref rid="pone.0226345.ref036" ref-type="bibr">36</xref>] to deal with small regions and similar regions. Concretely, the following two steps are carried out. (1) The clusters with pixels less than a predefined threshold <italic>T</italic><sub><italic>n</italic></sub> are merged with the nearest clusters. (2) Two closest clusters are combined to form a single cluster if the distance between the two clusters is less than a predefined threshold <italic>T</italic><sub><italic>d</italic></sub>.</p><p>The basic thresholding procedure consists of analysis of an image histogram and subsequent threshold selection from the values located in the valleys between peaks. However, the determination of peaks and valleys in a multimodal histogram is a nontrivial problem. In general, there are many local peaks and local valleys in the histogram of each color space of an <italic>RGB</italic> color image. The steps of the above techniques involve some smoothing of the histogram data, searching for significant peaks, and then identification of thresholds at the minima between two adjacent significant peaks. This means that the selection of significant peaks will be used to determine the thresholds, which consequently determine the final segmentation result of the image. As such, the above histogram-based thresholding techniques mainly focus on how to identify the significant peaks in the histogram, and then identify the valleys for thresholding. As two variants of the classic histogram, the histon and roughness index were demonstrated to achieve better segmentation results. Both histon-based and roughness-index-based algorithms, however, need to calculate the distance between every pixel and its neighborhood, which means that significant time is required to calculate the histograms. In addition, as mentioned above, both algorithms also need to determine the significant peaks to identify the thresholds. Moreover, it is difficult for these techniques to find the exact threshold point if the valley is flat.</p></sec><sec id="sec003"><title>Color image segmentation based on adaptive hierarchical-histogram thresholding</title><p>In this section, we propose a segmentation technique that uses a hierarchy structure of histograms to adaptively obtain the thresholds for color image segmentation. Our method does not need to find the significant peaks, it can adaptively identify the thresholds from valleys, and it has high efficiency.</p><sec id="sec004"><title>Main idea of adaptive hierarchical-histogram thresholding</title><p>Based on experiments performed on hundreds of <italic>RGB</italic> color images, we found that each image yields dozens of local valleys and local peaks in each histogram of the <italic>R</italic>, <italic>G</italic>, and <italic>B</italic> planes. As presented above, in the histogram, the peak of the bell-shaped region and its adjacent position intensity correspond to the main-body pixels of the object, while the boundary of the bell-shaped region corresponds to the edge pixels of the object. Therefore, in the histogram, the intensities between every pair of adjacent local valleys correspond to a small breadth bell-shaped region. All pixels ranged in a small bell-shaped region can also be regarded as a small homogeneous region. If we use all local valleys in the histogram of each color plane to segment an image, the image will be divided into a mass of small homogeneous regions. The colors of these small homogeneous regions will very close to the corresponding colors in the original image because such segmentations are overelaborate. Although such segmentation is exquisite, the segmented image can be viewed as an abstract version of the original image. For the original image, we noticed that a more abstract version with a relatively small number of homogeneous regions can be generated based on the segmented image. This finding is what inspired us to propose the <italic>AHHT</italic> algorithm for color image segmentation. The main idea of <italic>AHHT</italic> is to build a group of hierarchical histograms that corresponds to a multigranularity abstraction of the original image.</p><p>For each different color space, <italic>AHHT</italic> adopts a bottom-up approach to generate a group of histograms that form a hierarchy graph, and the obtained top-level histograms will be applied to segment the image. The rough process of <italic>AHHT</italic> is as follows. In each plane of <italic>R</italic>, <italic>G</italic>, and <italic>B</italic>, according to <xref ref-type="disp-formula" rid="pone.0226345.e001">Eq 1</xref>, the histogram is calculated as the first (bottom)-level histogram. From the first-level histogram, each small bell-shaped region is merged into a bin expressed by the count (the number of pixels within the intensity range of the small bell-shaped region) and the weighted average intensity (the average intensity of all pixels within the small bell-shaped region), and then the second-level histogram is obtained. Next, similar action is applied to generate the third-level histogram, that is, from the second-level histogram, each bell-shaped region is merged into a bin expressed by the count and the weighted average intensity of all pixels within the bell-shaped region. Such process continues until the last-generated histogram has no valleys or the difference of every adjacent pair of bins is larger than a threshold <italic>w</italic>. Obviously, each bin of the top-level histogram corresponds to a group of pixels in the image. All bins&#x02032; information in the top-level histogram in each plane of <italic>R</italic>, <italic>G</italic>, and <italic>B</italic> is applied to split the image into multiple clusters. The color representing each cluster is obtained by averaging all the pixels within the cluster. At this point, the initial segmentation is completed. In the process of <italic>Region-Merging</italic>, the <italic>AHHT</italic> algorithm adopts an approach identical to that used in the roughness-index-based algorithm.</p><p>In each color plane of <italic>R</italic>, <italic>G</italic>, and <italic>B</italic>, <italic>AHHT</italic> generates a group of histograms in a hierarchical fashion. Hereafter, such a group of histograms is called a <italic>hierarchical-histogram</italic>. The lower a histogram is in the <italic>hierarchical-histogram</italic>, the more elaborate the details of the image it encodes are. The role of the prior-level histogram is generating for the next-level histogram. The experiments performed on hundreds of color images show that <italic>AHHT</italic> commonly generates four to five histograms for each color plane of the image. The <italic>AHHT</italic> algorithm segmentation of the image is based on the top-level histograms.</p><p>The <italic>hierarchical-histogram</italic> for each plane of <italic>R</italic>, <italic>G</italic>, and <italic>B</italic> of the image <italic>Moon</italic>, generated by the <italic>AHHT</italic> algorithm, are shown in <xref ref-type="fig" rid="pone.0226345.g002">Fig 2(a)&#x02013;2(c)</xref>, respectively. In the experiment, the parameter <italic>w</italic> = 20, which means that the difference of every adjacent pair of bins in the histogram is larger than 20, and the top-level histogram is generated. As shown in <xref ref-type="fig" rid="pone.0226345.g002">Fig 2</xref>, <italic>AHHT</italic> generates four histograms for each color plane. The first-level histogram is generated from the original image <italic>Moon</italic>, and the next-level histogram is generated from the prior-level histogram. In each histogram of <xref ref-type="fig" rid="pone.0226345.g002">Fig 2</xref>, each dashed line marks a valley&#x02032;s position. From <xref ref-type="fig" rid="pone.0226345.g002">Fig 2</xref>, we can see that the first-level histogram (of each plane of <italic>R</italic>, <italic>G</italic>, and <italic>B</italic>) has many local valleys, which means that there are many small bell-shaped regions in the first-level histogram. Each small bell-shaped region in the first-level histogram is expressed by a bin in the second-level histogram, and so on. The fourth-level (top-level) histogram of each color plane is applied to segment the image. In the <italic>Region-Merging</italic> process of this experiment, the regions with fewer than 0.1% of the pixels are merged with the nearest region, and two regions with a distance of less than 70 are combined to form a single region. <xref ref-type="fig" rid="pone.0226345.g003">Fig 3(b) and 3(c)</xref> show the initial segmented result and the final segmented result of the image <italic>Moon</italic>, respectively.</p><fig id="pone.0226345.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0226345.g002</object-id><label>Fig 2</label><caption><title>Hierarchical-histogram of each plane of R, G, and B of the image Moon obtained by AHHT.</title></caption><graphic xlink:href="pone.0226345.g002"/></fig><fig id="pone.0226345.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0226345.g003</object-id><label>Fig 3</label><caption><title>The image Moon: (a) original image, (b) initial segmented result (225 colors), (c) final segmented result (4 colors).</title></caption><graphic xlink:href="pone.0226345.g003"/></fig></sec><sec id="sec005"><title>Algorithm</title><p>In a histogram, a valley corresponds to a local minimum, which is present near a local lowest point or a local lowest horizontal line. All valleys in the histogram of a color plane can be identified by the following rule.</p><disp-formula id="pone.0226345.e009"><alternatives><graphic xlink:href="pone.0226345.e009.jpg" id="pone.0226345.e009g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M9"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="4pt"/><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="4pt"/><mml:mo>&#x00026;</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="40pt"/><mml:mtext>then</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mspace width="4pt"/><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="4pt"/><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="4pt"/><mml:mo>&#x00026;</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="4pt"/><mml:mo>&#x00026;</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="40pt"/><mml:mtext>then</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>&#x0230a;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>&#x0230b;</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mspace width="4pt"/><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mspace width="10pt"/><mml:mi>f</mml:mi><mml:mtext>or</mml:mtext><mml:mspace width="4pt"/><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>l</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>L</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mspace width="4pt"/><mml:mtext>and</mml:mtext><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives><label>(7)</label></disp-formula><p>In our program, a bin in the histogram is expressed as a triple of [<italic>h</italic>(<italic>l</italic>), <italic>l</italic>, <italic>lR</italic>], where <italic>l</italic>(0 &#x02264; <italic>l</italic> &#x02264; <italic>L</italic> &#x02212; 1) is the intensity, <italic>h</italic>(<italic>l</italic>) is the number of image pixels having intensity <italic>l</italic>, and <italic>lR</italic> is the right endpoint&#x02032;s intensity of the bin. <italic>H</italic> is an array of bins in ascending order according to intensity, which means that <italic>H</italic> corresponds to a histogram. The <italic>m</italic>th and (<italic>m</italic> + 1)th elements of <italic>H</italic> are <italic>H</italic>[<italic>m</italic>] = [<italic>h</italic>(<italic>l</italic><sub><italic>m</italic></sub>), <italic>l</italic><sub><italic>m</italic></sub>, <italic>l</italic><sub><italic>m</italic></sub><italic>R</italic>] and <italic>H</italic>[<italic>m</italic> + 1] = [<italic>h</italic>(<italic>l</italic><sub><italic>m</italic>+1</sub>), <italic>l</italic><sub><italic>m</italic>+1,</sub>
<italic>l</italic><sub><italic>m</italic>+1</sub><italic>R</italic>], respectively, where <italic>l</italic><sub><italic>m</italic></sub> &#x0003c; <italic>l</italic><sub><italic>m</italic>+1</sub> holds. The first-level histogram <italic>H</italic> is generated from the original image. For every bin [<italic>h</italic>(<italic>l</italic>), <italic>l</italic>, <italic>LR</italic>] of the first-level histogram, <italic>lR</italic> = <italic>l</italic> holds. According to <xref ref-type="disp-formula" rid="pone.0226345.e009">Eq 7</xref>, a function named <italic>GetValleys</italic>(<italic>H</italic>) is used to find all valleys (sorted in ascending order) from a histogram <italic>H</italic>. The details of the function are left out in order to keep the paper reasonably concise. If <italic>H</italic> is generated from the original image, which means that <italic>H</italic> is the first-level histogram, then <italic>GetValleys</italic>(<italic>H</italic>) can find all local valleys that will be used to generate the second-level histogram. If <italic>H</italic> is the <italic>k</italic>th (<italic>k &#x0003e;</italic> 1)-level histogram, then the result of <italic>GetValleys</italic>(<italic>H</italic>) will be used to generate the (<italic>k</italic> + 1)th-level histogram. <bold>Function 1 &#x02013;</bold><italic>GetNextHist</italic> is used to generate the next-level histogram of <italic>H</italic>, and the pseudocode is as follows.</p><p><bold>Function 1.</bold>
<italic>GetNextHist</italic>(<italic>H</italic>, <italic>valleys</italic>, <italic>w</italic>).</p><p specific-use="line">Input: a histogram <italic>H</italic> and its v<italic>alleys</italic>; bin merge threshold <italic>w</italic>;</p><p specific-use="line">Output: <italic>H</italic>&#x02032;, which is the next-level histogram of <italic>H</italic>;</p><p specific-use="line">(1) <italic>H</italic>&#x02032; = &#x02205;; //initialize the result</p><p specific-use="line">(2) <italic>left</italic> = 1; <italic>right</italic> = 1; // left-end and right-end intensities of a bell-shaped region</p><p specific-use="line">(3) <italic>bins</italic> = []; //store bins of a bell-shaped region</p><p specific-use="line">(4) <italic>for r</italic> = 1 <italic>to length</italic>(<italic>valleys</italic>)</p><p specific-use="line">(5) &#x02003;<italic>for m</italic> = <italic>left to length</italic>(<italic>H</italic>)//find the bin for which the intensity equals <italic>valleys</italic>[<italic>r</italic>]</p><p specific-use="line">(6) &#x02003;&#x02003;[<italic>h</italic>(<italic>l</italic><sub><italic>m</italic></sub>), <italic>l</italic><sub><italic>m</italic>,</sub>
<italic>l</italic><sub><italic>m</italic></sub><italic>R</italic>] = <italic>H</italic>[<italic>m</italic>]; //fetch the <italic>mth</italic> bin of <italic>H</italic></p><p specific-use="line">(7) &#x02003;&#x02003;<italic>if</italic>
<italic>l<sub>m</sub></italic> &#x02a75; <italic>valleys</italic>[<italic>r</italic>] <italic>then</italic></p><p specific-use="line">(8) &#x02003;&#x02003;&#x02003;{<italic>right</italic> = <italic>m</italic>; <italic>break</italic>;}</p><p specific-use="line">(9) &#x02003;<italic>bins</italic> = <italic>GetMergeBin</italic>(<italic>H</italic>, <italic>left</italic>, <italic>right</italic>, <italic>w</italic>); //generate bins for the bell-shaped region</p><p specific-use="line">(10) &#x02003;<italic>H&#x02032;</italic> = <italic>H&#x02032;</italic> &#x0222a; <italic>bins</italic>;// Append every <italic>bin</italic> of <italic>bins</italic> to <italic>H</italic>&#x02032;</p><p specific-use="line">(11) &#x02003;<italic>left</italic> = <italic>right</italic> + 1</p><p specific-use="line">(12) <italic>bins</italic> = <italic>GetMergeBin</italic>(<italic>H</italic>, <italic>left</italic>, <italic>length</italic>(<italic>H</italic>), <italic>w</italic>); //generate bins for last bell-shaped region</p><p specific-use="line">(13) <italic>H&#x02032;</italic> = <italic>H&#x02032;</italic> &#x0222a; <italic>bins</italic>;</p><p specific-use="line">(14) <italic>return H&#x02032;</italic>;</p><p>In lines 9 and 12, a function named <italic>GetMergeBin</italic> returns bin or a group of bins for a bell-shaped region. In line 9, the parameters <italic>left</italic> and <italic>right</italic> of <italic>GetMergeBin</italic> are the left-end index and right-end index of a bell-shaped region in the histogram <italic>H</italic>. If the difference between the right-end intensity and left-end intensity is less than <italic>w</italic>, then <italic>GetMergeBin</italic> returns a bin corresponding to the bell-shaped region; otherwise, at most &#x0230a;(<italic>l</italic><sub><italic>right</italic></sub> &#x02212; <italic>l</italic><sub><italic>left</italic></sub>)/<italic>w</italic>&#x0230b; +1 bins will be returned. For simplicity, the pseudocode of <italic>GetMergeBin</italic> is omitted. Note that the function <italic>GetMergeBin</italic> only merges adjacent bins within a bell-shaped region. This mechanism makes the next-level histogram match the original intensity distribution of the image well.</p><p>On the basis of the above functions, Function 2 &#x02013;<italic>GetAHH</italic> (Get Adaptive Hierarchical-Histograms) is used to generate a <italic>hierarchical-histogram</italic> for each plane of <italic>R</italic>, <italic>G</italic>, and <italic>B</italic> of the image, and the pseudocode is as follows.</p><p><bold>Function 2.</bold>
<italic>GetAHH</italic>(<italic>I</italic>, <italic>w</italic>).</p><p specific-use="line">Input: an <italic>RGB</italic> color image <italic>I</italic>; bin merge threshold <italic>w</italic>;</p><p specific-use="line">Output: <italic>hierarchical-histogram</italic> for each plane of <italic>R</italic>, <italic>G</italic>, and <italic>B</italic> of the image;</p><p specific-use="line">(1) for each color plane <italic>i</italic> &#x02208; {<italic>R</italic>, <italic>G</italic>, <italic>B</italic>} of image <italic>I</italic></p><p specific-use="line">(2) &#x02003;<italic>Hists</italic><sub><italic>i</italic></sub> = &#x02205;; //store a hierarchical-histogram;</p><p specific-use="line">(3) &#x02003;Calculate <italic>w</italic><sub><italic>i</italic></sub> according to <styled-content><xref ref-type="disp-formula" rid="pone.0226345.e015">Eq 8</xref></styled-content>;</p><p specific-use="line">(4) &#x02003;Generated the first-level histogram <italic>H</italic><sub><italic>i</italic></sub>;</p><p specific-use="line">(5) &#x02003;Append <italic>H</italic><sub><italic>i</italic></sub> to <italic>Hists</italic><sub><italic>i</italic></sub>;</p><p specific-use="line">(6) &#x02003;<italic>valleys</italic> = <italic>Getvalleys</italic>(<italic>H</italic><sub><italic>i</italic></sub>);</p><p specific-use="line">(7) &#x02003;<inline-formula id="pone.0226345.e010"><alternatives><graphic xlink:href="pone.0226345.e010.jpg" id="pone.0226345.e010g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M10"><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>y</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula></p><p specific-use="line">(8) &#x02003;while <inline-formula id="pone.0226345.e011"><alternatives><graphic xlink:href="pone.0226345.e011.jpg" id="pone.0226345.e011g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M11"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02032;</mml:mo><mml:mo>&#x02260;</mml:mo><mml:mi>H</mml:mi></mml:math></alternatives></inline-formula> do</p><p specific-use="line">(9) &#x02003;&#x02003;&#x02003;Append <inline-formula id="pone.0226345.e012"><alternatives><graphic xlink:href="pone.0226345.e012.jpg" id="pone.0226345.e012g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M12"><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> to <italic>Hists</italic><sub><italic>i</italic></sub>;</p><p specific-use="line">(10) &#x02003;&#x02003;<inline-formula id="pone.0226345.e013"><alternatives><graphic xlink:href="pone.0226345.e013.jpg" id="pone.0226345.e013g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M13"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>;</p><p specific-use="line">(11) &#x02003;&#x02003;<italic>valleys</italic> = <italic>Getvalleys</italic>(<italic>H</italic><sub><italic>i</italic></sub>);</p><p specific-use="line">(12) &#x02003;&#x02003;<inline-formula id="pone.0226345.e014"><alternatives><graphic xlink:href="pone.0226345.e014.jpg" id="pone.0226345.e014g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M14"><mml:mrow><mml:msubsup><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>y</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula></p><p specific-use="line">(13) return <italic>Hists</italic><sub><italic>i</italic></sub> for each <italic>i</italic> &#x02208; {<italic>R</italic>, <italic>G</italic>, <italic>B</italic>}</p><p>For each color plane of the image, a <italic>hierarchical-histogram</italic> is generated starting with the first-level histogram, and the next-level histograms are iteratively generated until the new-level histogram has no change.</p><p>For an <italic>RGB</italic> color image, there are different widths of valid intensity between each plane of <italic>R</italic>, <italic>G</italic>, and <italic>B</italic>. Taking <xref ref-type="fig" rid="pone.0226345.g002">Fig 2</xref>&#x02018;s image of <italic>Moon</italic> as an example, the (first-level) histogram of the Blue plane has a wide distribution of valid intensity, and the (first-level) histogram of the Green plane has a relatively narrow distribution of valid intensity. Therefore, different threshold values <italic>w</italic> should be set for different color planes. A reasonable threshold <italic>w</italic> should be given a relatively large value for a color plane with a wide width of valid intensity and a relatively smaller value for a color plane with a narrow width of valid intensity. For different color planes, the threshold <italic>w</italic><sub><italic>i</italic></sub> can be calculated as follows.</p><disp-formula id="pone.0226345.e015"><alternatives><graphic xlink:href="pone.0226345.e015.jpg" id="pone.0226345.e015g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M15"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>L</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mtext>for</mml:mtext><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="center"><mml:mrow><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>l</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>L</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02260;</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>l</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>L</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02260;</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives><label>(8)</label></disp-formula><p>According to <xref ref-type="disp-formula" rid="pone.0226345.e015">Eq 8</xref>, <italic>w</italic><sub><italic>i</italic></sub> is calculated as the value of <italic>w</italic> multiplied by the scale factor <inline-formula id="pone.0226345.e016"><alternatives><graphic xlink:href="pone.0226345.e016.jpg" id="pone.0226345.e016g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M16"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>, where <italic>SPAN</italic><sub><italic>i</italic></sub> is the difference between the max valid intensity and min valid intensity of the color plane <italic>i</italic>. In this manner, a relatively larger threshold <italic>w</italic><sub><italic>i</italic></sub> is applied for a color plane with a wide width of intensity. However, for some images, there are only a very small number of pixels distributed at lower intensities or higher intensities of a color plane, which makes a relatively larger threshold <italic>w</italic><sub><italic>i</italic></sub> be applied to the color plane. To avoid noise trouble, we use the <italic>SPAN</italic><sub><italic>i</italic></sub> of Formula (<xref ref-type="disp-formula" rid="pone.0226345.e017">9</xref>) to replace the <italic>SPAN</italic><sub><italic>i</italic></sub> of Formula (<xref ref-type="disp-formula" rid="pone.0226345.e015">8</xref>).</p><disp-formula id="pone.0226345.e017"><alternatives><graphic xlink:href="pone.0226345.e017.jpg" id="pone.0226345.e017g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M17"><mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>argmax</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>l</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mo>&#x0003e;</mml:mo><mml:mn>0.01</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>l</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mo>&#x0003e;</mml:mo><mml:mn>0.01</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(9)</label></disp-formula><p>In expression 9, the threshold value of 0.01 means the <italic>SPAN</italic><sub><italic>i</italic></sub> is the difference between the max valid intensity and min valid intensity, excluding the top 1 percent and bottom 1 percent of pixels, which improves the robustness of the calculation of <italic>SPAN</italic><sub><italic>i</italic></sub>.</p><p>Once the <italic>hierarchical-histograms</italic> for each <italic>R</italic>, <italic>G</italic>, and <italic>B</italic> plane are generated, the top-level histograms in the <italic>hierarchical-histograms</italic> are used to segment the image. Concretely, for a color plane <italic>i</italic>, every pixel with an original intensity value range of [<italic>l</italic><sub><italic>m</italic>&#x02212;1</sub><italic>R</italic>, <italic>l</italic><sub><italic>m</italic></sub><italic>R</italic>]) is set to the intensity value of <italic>l</italic><sub><italic>m</italic></sub>, where <italic>l</italic><sub><italic>m</italic>&#x02212;1</sub><italic>R</italic>, <italic>l</italic><sub><italic>m</italic></sub><italic>R</italic> and <italic>l</italic><sub><italic>m</italic></sub> come from the (<italic>m</italic> &#x02212; 1)th and <italic>m</italic>th elements (bins) in the top-level histogram <italic>H</italic><sub><italic>i</italic></sub>, that is, <italic>H</italic>[<italic>m</italic>] = [<italic>h</italic>(<italic>l</italic><sub><italic>m</italic></sub>), <italic>l</italic><sub><italic>m</italic></sub>, <italic>l</italic><sub><italic>m</italic></sub><italic>R</italic>] and <italic>H</italic>[<italic>m</italic> + 1] = [<italic>h</italic>(<italic>l</italic><sub><italic>m</italic>&#x02212;1</sub>), <italic>l</italic><sub><italic>m</italic>&#x02212;1</sub>, <italic>l</italic><sub><italic>m</italic>&#x02212;1</sub><italic>R</italic>]. After such process, the initially segmentation is completed. It is pretty remarkable that the obtained top-level histograms correspond to the histogram of each <italic>R</italic>, <italic>G</italic>, and <italic>B</italic> plane of the segmented image. On the basis of the above, <italic>AHHT</italic> (Adaptive Hierarchical-Histogram Thresholding) algorithm for color image segmentation is described as follows.</p><p specific-use="line"><bold>Algorithm 1.</bold>
<italic>AHHT</italic>(Adaptive Hierarchical-Histogram Thresholding)</p><p specific-use="line">Input: an <italic>RGB</italic> color image; bin merge threshold <italic>w</italic>; small region threshold <italic>T</italic><sub><italic>n</italic></sub>; distance <italic>T</italic><sub><italic>d</italic></sub> for merging close regions;</p><p specific-use="line">Output: the segmented image;</p><p specific-use="line">Step 1: Calculate the <italic>hierarchical-histogram</italic> for each <italic>R</italic>, <italic>G</italic>, and <italic>B</italic> plane of the image by calling Function 2 &#x02013;<italic>GetAhh</italic>;</p><p specific-use="line">Step 2: Segment the image by using the top-level histograms obtained by Step 1;</p><p specific-use="line">Step 3: Merge small regions and close regions.</p><p>The <italic>AHHT</italic> algorithm has three main advantages: (1) <italic>AHHT</italic> adopts a bottom-up strategy to build the structure of the <italic>hierarchical-histogram</italic>, which can adaptively identify the thresholds from valleys; (2) In the process of identifying the thresholds, <italic>AHHT</italic> does not need to determine peaks, and only one parameter, <italic>w</italic>, is involved; and (3) <italic>AHHT</italic> finds the thresholds with high efficiency.</p></sec><sec id="sec006"><title>Complexity analysis</title><p>The computational complexity of the <italic>AHHT</italic> algorithm is analyzed as follows. An <italic>RGB</italic> color image <italic>I</italic> with n pixels and an intensity scale <italic>L</italic> for each color space is given. The total computation time includes that consumed in each of three major steps.</p><p>In the first step, the <italic>hierarchical-histogram</italic> for each color plane is computed. The complexity of generating all three first-level histograms is <italic>O</italic>(3<italic>n</italic>). The complexity of generating all three second-level histograms is <italic>O</italic>(3<italic>L</italic>). The complexity of generating all three <italic>m</italic>th-level histograms is <italic>O</italic>(3<italic>L</italic><sub><italic>m</italic></sub>), where <italic>L</italic><sub><italic>m</italic></sub> is the average number of bins in the three (<italic>m</italic> &#x02212; 1)th-level histograms. Because a <italic>hierarchical-histogram</italic> only includes a limited number of histograms, the time required to generate the first-level histograms is far greater than the rest of the time required to generate the others. The complexity of step 1 can thus be considered as <italic>O</italic>(3<italic>n</italic>). In the second step, every pixel is distributed into the corresponding bin and assigned the intensity value of the bin by using the three top-level histograms. It is given that the number of bins in every top-level histogram is <italic>k</italic>. This process requires approximately 3<italic>kn</italic> operations, and the complexity of step 2 can be considered as <italic>O</italic>(3<italic>kn</italic>). The third step is the Region-Merging process. Suppose that <italic>r</italic><sub>1</sub> is the number of regions before merging and that <italic>r</italic><sub>2</sub> is the number of regions merged. The complexity of calculating the difference between regions is <inline-formula id="pone.0226345.e018"><alternatives><graphic xlink:href="pone.0226345.e018.jpg" id="pone.0226345.e018g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M18"><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mn>3</mml:mn><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>, and the complexity of merging the regions is <italic>O</italic>(3<italic>r</italic><sub>2</sub><italic>n</italic>). Therefore, the complexity of step 3 is <inline-formula id="pone.0226345.e019"><alternatives><graphic xlink:href="pone.0226345.e019.jpg" id="pone.0226345.e019g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M19"><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>n</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. To summarize, the expected time complexity of the <italic>AHHT</italic> algorithm is <inline-formula id="pone.0226345.e020"><alternatives><graphic xlink:href="pone.0226345.e020.jpg" id="pone.0226345.e020g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M20"><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>n</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. It is worth mentioning that the histon-based and roughness-index-based algorithms need to calculate the Euclidean distance 24<italic>n</italic> times to find the thresholds. By contrast, the <italic>AHHT</italic> algorithm has substantially reduced the time consumption.</p></sec></sec><sec id="sec007"><title>Experimental results</title><p>As two variations of histogram-based techniques, the histon-based and roughness-index-based techniques have been demonstrated to achieve better segmentation results. In this study, the performance of the proposed <italic>AHHT</italic> technique is compared with them. The experiments are performed on Berkeley Segmentation Data Set 300 (BSDS300) as well as Berkeley Segmentation Data Set 500 (BSDS500). Each image is 481 &#x000d7; 321 pixels. For each image, a set of ground truths compiled by the human observers is provided. All the images are normalized to have the longest side equivalent to 320 pixels.</p><p>All of these techniques include three major steps, and each one of the steps offers similar functionality. For the histon-based and roughness-index-based techniques, all parameters involved are set the same as those used in the original papers [<xref rid="pone.0226345.ref031" ref-type="bibr">31</xref>,<xref rid="pone.0226345.ref033" ref-type="bibr">33</xref>]. Concretely, in step 1, two parameters are involved for finding the significant peaks: (1) the peak is greater than 20% of the average value of all peaks; and (2) the distance between two peaks is greater than 10. In the post-processing step (step 3), two parameters <italic>T</italic><sub><italic>n</italic></sub> and <italic>T</italic><sub><italic>d</italic></sub> for region merging are involved. Unless otherwise stated the results, <italic>T</italic><sub><italic>n</italic></sub> is set as 0.1%, and <italic>T</italic><sub><italic>d</italic></sub> is set as 20, respectively. For the proposed <italic>AHHT</italic> algorithm, only one parameter, the bin merge threshold <italic>w</italic>, is involved in step 1. In our experiments, <italic>w</italic> is set as 15, which means that any adjacent pair of bins cannot be merged if the difference between the two bins is larger than 15. In the same post-processing step, the two involved parameters are identical to those of the histon-based and roughness-index-based algorithms to make a fair comparison.</p><sec id="sec008"><title>Visual evaluation of segmentation results</title><p>In this section, the segmentation results for compared algorithms are visually evaluated by using 6 of all the segmented images. The segmentation results for the images <italic>Birds(</italic>#135069), <italic>Church(</italic>#126007), <italic>Mountain</italic>(#14037), <italic>Marsh</italic>(#92059), <italic>Boating</italic>(#147021) and <italic>Snake</italic>(#196073) are shown in Figs <xref ref-type="fig" rid="pone.0226345.g004">4</xref>&#x02013;<xref ref-type="fig" rid="pone.0226345.g009">9</xref>, respectively. Considering that all of the compared techniques adopt the same <italic>Region-Merging</italic> processing, in Figs <xref ref-type="fig" rid="pone.0226345.g004">4</xref>&#x02013;<xref ref-type="fig" rid="pone.0226345.g009">9</xref>, we present the initial segmented result and the result after region merging for each technique. In <xref rid="pone.0226345.t001" ref-type="table">Table 1</xref>, columns 3&#x02013;5 present the number of bands in each plane of <italic>R</italic>, <italic>G</italic>, and <italic>B</italic> of the initial segmented result, and columns 6&#x02013;7 present the color number in the initial segmented result and the color number in the postmerging result. Generally, based on visual evaluation, the <italic>AHHT</italic> technique produces better segmentation results.</p><fig id="pone.0226345.g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0226345.g004</object-id><label>Fig 4</label><caption><title>The image <italic>Birds</italic>: (a) original image, (b, c) initial segmented result and result after region merging based on the histon, (d, e) initial segmented result and result after region merging based on the roughness index, (f, g) initial segmented result and result after region merging based on <italic>AHHT</italic>.</title></caption><graphic xlink:href="pone.0226345.g004"/></fig><fig id="pone.0226345.g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0226345.g005</object-id><label>Fig 5</label><caption><title>The image <italic>Church</italic>: (a) original image, (b, c) initial segmented result and result after region merging based on the histon, (d, e) initial segmented result and result after region merging based on the roughness index, (f, g) initial segmented result and result after region merging based on <italic>AHHT</italic>.</title></caption><graphic xlink:href="pone.0226345.g005"/></fig><fig id="pone.0226345.g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0226345.g006</object-id><label>Fig 6</label><caption><title>The image <italic>Mountain</italic>: (a) original image, (b, c) initial segmented result and result after region merging based on the histon, (d, e) initial segmented result and result after region merging based on the roughness index, (f, g) initial segmented result and result after region merging based on <italic>AHHT</italic>.</title></caption><graphic xlink:href="pone.0226345.g006"/></fig><fig id="pone.0226345.g007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0226345.g007</object-id><label>Fig 7</label><caption><title>The image <italic>Marsh</italic>: (a) original image, (b, c) initial segmented result and result after region merging based on the histon, (d, e) initial segmented result and result after region merging based on the roughness index, (f, g) initial segmented result and result after region merging based on <italic>AHHT</italic>.</title></caption><graphic xlink:href="pone.0226345.g007"/></fig><fig id="pone.0226345.g008" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0226345.g008</object-id><label>Fig 8</label><caption><title>The image <italic>Boating</italic>: (a) original image, (b, c) initial segmented result and result after region merging based on the histon, (d, e) initial segmented result and result after region merging based on the roughness index, (f, g) initial segmented result and result after region merging based on <italic>AHHT</italic>.</title></caption><graphic xlink:href="pone.0226345.g008"/></fig><fig id="pone.0226345.g009" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0226345.g009</object-id><label>Fig 9</label><caption><title>The image <italic>Snake</italic>: (a) original image, (b, c) initial segmented result and result after region merging based on the histon, (d, e) initial segmented result and result after region merging based on the roughness index, (f, g) initial segmented result and result after region merging based on <italic>AHHT</italic>.</title></caption><graphic xlink:href="pone.0226345.g009"/></fig><table-wrap id="pone.0226345.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0226345.t001</object-id><label>Table 1</label><caption><title>Comparison of the number of thresholds and the color number in the initial segmented result and the result after merging.</title></caption><alternatives><graphic id="pone.0226345.t001g" xlink:href="pone.0226345.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="2" colspan="1"><italic>Image</italic></th><th align="left" rowspan="2" colspan="1">Methods</th><th align="left" rowspan="2" colspan="1">Red band</th><th align="left" rowspan="2" colspan="1">Green band</th><th align="left" rowspan="2" colspan="1">Blue band</th><th align="left" colspan="2" rowspan="1">Color number</th></tr><tr><th align="left" rowspan="1" colspan="1">Initial segmentation</th><th align="left" rowspan="1" colspan="1">After merging</th></tr></thead><tbody><tr><td align="left" rowspan="3" colspan="1">Birds #135069</td><td align="left" rowspan="1" colspan="1">Histon</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">6</td><td align="left" rowspan="1" colspan="1">18</td><td align="left" rowspan="1" colspan="1">6</td></tr><tr><td align="left" rowspan="1" colspan="1">Roughness-index</td><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">11</td><td align="left" rowspan="1" colspan="1">13</td><td align="left" rowspan="1" colspan="1">73</td><td align="left" rowspan="1" colspan="1">10</td></tr><tr><td align="left" rowspan="1" colspan="1">AHHT</td><td align="left" rowspan="1" colspan="1">41</td><td align="left" rowspan="1" colspan="1">23</td><td align="left" rowspan="1" colspan="1">17</td><td align="left" rowspan="1" colspan="1">308</td><td align="left" rowspan="1" colspan="1">13</td></tr><tr><td align="left" rowspan="3" colspan="1">Church #126007</td><td align="left" rowspan="1" colspan="1">Histon</td><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">210</td><td align="left" rowspan="1" colspan="1">36</td></tr><tr><td align="left" rowspan="1" colspan="1">Roughness-index</td><td align="left" rowspan="1" colspan="1">12</td><td align="left" rowspan="1" colspan="1">9</td><td align="left" rowspan="1" colspan="1">14</td><td align="left" rowspan="1" colspan="1">245</td><td align="left" rowspan="1" colspan="1">35</td></tr><tr><td align="left" rowspan="1" colspan="1">AHHT</td><td align="left" rowspan="1" colspan="1">13</td><td align="left" rowspan="1" colspan="1">11</td><td align="left" rowspan="1" colspan="1">15</td><td align="left" rowspan="1" colspan="1">274</td><td align="left" rowspan="1" colspan="1">40</td></tr><tr><td align="left" rowspan="3" colspan="1">Mountain #14037</td><td align="left" rowspan="1" colspan="1">Histon</td><td align="left" rowspan="1" colspan="1">12</td><td align="left" rowspan="1" colspan="1">13</td><td align="left" rowspan="1" colspan="1">9</td><td align="left" rowspan="1" colspan="1">128</td><td align="left" rowspan="1" colspan="1">22</td></tr><tr><td align="left" rowspan="1" colspan="1">Roughness-index</td><td align="left" rowspan="1" colspan="1">15</td><td align="left" rowspan="1" colspan="1">15</td><td align="left" rowspan="1" colspan="1">15</td><td align="left" rowspan="1" colspan="1">201</td><td align="left" rowspan="1" colspan="1">27</td></tr><tr><td align="left" rowspan="1" colspan="1">AHHT</td><td align="left" rowspan="1" colspan="1">21</td><td align="left" rowspan="1" colspan="1">19</td><td align="left" rowspan="1" colspan="1">15</td><td align="left" rowspan="1" colspan="1">181</td><td align="left" rowspan="1" colspan="1">22</td></tr><tr><td align="left" rowspan="3" colspan="1">Marsh #92059</td><td align="left" rowspan="1" colspan="1">Histon</td><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">6</td><td align="left" rowspan="1" colspan="1">125</td><td align="left" rowspan="1" colspan="1">29</td></tr><tr><td align="left" rowspan="1" colspan="1">Roughness-index</td><td align="left" rowspan="1" colspan="1">17</td><td align="left" rowspan="1" colspan="1">13</td><td align="left" rowspan="1" colspan="1">17</td><td align="left" rowspan="1" colspan="1">323</td><td align="left" rowspan="1" colspan="1">31</td></tr><tr><td align="left" rowspan="1" colspan="1">AHHT</td><td align="left" rowspan="1" colspan="1">13</td><td align="left" rowspan="1" colspan="1">15</td><td align="left" rowspan="1" colspan="1">14</td><td align="left" rowspan="1" colspan="1">245</td><td align="left" rowspan="1" colspan="1">33</td></tr><tr><td align="left" rowspan="3" colspan="1">Boating #147021</td><td align="left" rowspan="1" colspan="1">Histon</td><td align="left" rowspan="1" colspan="1">11</td><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">290</td><td align="left" rowspan="1" colspan="1">27</td></tr><tr><td align="left" rowspan="1" colspan="1">Roughness-index</td><td align="left" rowspan="1" colspan="1">14</td><td align="left" rowspan="1" colspan="1">17</td><td align="left" rowspan="1" colspan="1">17</td><td align="left" rowspan="1" colspan="1">373</td><td align="left" rowspan="1" colspan="1">35</td></tr><tr><td align="left" rowspan="1" colspan="1">AHHT</td><td align="left" rowspan="1" colspan="1">13</td><td align="left" rowspan="1" colspan="1">15</td><td align="left" rowspan="1" colspan="1">16</td><td align="left" rowspan="1" colspan="1">322</td><td align="left" rowspan="1" colspan="1">46</td></tr><tr><td align="left" rowspan="3" colspan="1">Snake #196073</td><td align="left" rowspan="1" colspan="1">Histon</td><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">4</td><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">10</td></tr><tr><td align="left" rowspan="1" colspan="1">Roughness-index</td><td align="left" rowspan="1" colspan="1">9</td><td align="left" rowspan="1" colspan="1">9</td><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">35</td><td align="left" rowspan="1" colspan="1">17</td></tr><tr><td align="left" rowspan="1" colspan="1">AHHT</td><td align="left" rowspan="1" colspan="1">25</td><td align="left" rowspan="1" colspan="1">25</td><td align="left" rowspan="1" colspan="1">23</td><td align="left" rowspan="1" colspan="1">197</td><td align="left" rowspan="1" colspan="1">11</td></tr></tbody></table></alternatives></table-wrap><p>For the image <italic>Birds</italic>, <xref ref-type="fig" rid="pone.0226345.g004">Fig 4</xref> shows the initial segmented result and the result after region merging for the histon-based, roughness-index-based and <italic>AHHT</italic> techniques. For the histon, roughness-index- and <italic>AHHT</italic> techniques, the numbers of colors in the initial segmented results are 18 (<xref ref-type="fig" rid="pone.0226345.g004">Fig 4b</xref>), 73 (<xref ref-type="fig" rid="pone.0226345.g004">Fig 4d</xref>) and 308 (<xref ref-type="fig" rid="pone.0226345.g004">Fig 4f</xref>), respectively; the numbers of colors in the final segmented results are 6 (<xref ref-type="fig" rid="pone.0226345.g004">Fig 4c</xref>), 10 (<xref ref-type="fig" rid="pone.0226345.g004">Fig 4e</xref>) and 13 (<xref ref-type="fig" rid="pone.0226345.g004">Fig 4g</xref>), respectively. For the histon-based technique (<xref ref-type="fig" rid="pone.0226345.g004">Fig 4b and 4c</xref>), we can see that there are fewer colors in the segmented results, which leads to larger homogenous regions in the results. However, the white feathers of the birds have been mistakenly assigned to the sky by the histon technique. For the roughness-index-based technique (<xref ref-type="fig" rid="pone.0226345.g004">Fig 4d and 4e</xref>), the white feathers of the birds have been assigned to a color close to that of the sky. By contrast, the <italic>AHHT</italic> technique has successfully avoided this classification error. Therefore, although the initial segmented results based on the histon and roughness index produced a lower number of colors, they lose many details of small distinct regions. It is worth noting that, although all three techniques adopt the same region merging process, the number of colors in the final segmented result by the histon technique is obviously less than that of the other two techniques. The reason for this is that, for the histon technique, there are small color differences between the different regions in the initial segmented result, which in turn cause these different regions to be further merged.</p><p>For the image <italic>Church</italic>, <xref ref-type="fig" rid="pone.0226345.g005">Fig 5</xref> shows the initial segmented result and the result after region merging for the histon-based, roughness-index-based and <italic>AHHT</italic> techniques, respectively. For the histon-based, roughness-index-based and <italic>AHHT</italic> techniques, the numbers of colors in the initial segmented result are 210 (<xref ref-type="fig" rid="pone.0226345.g005">Fig 5b</xref>), 245 (<xref ref-type="fig" rid="pone.0226345.g005">Fig 5d</xref>) and 274 (<xref ref-type="fig" rid="pone.0226345.g005">Fig 5f</xref>), respectively; the numbers of colors in the final segmented results are 36 (<xref ref-type="fig" rid="pone.0226345.g005">Fig 5c</xref>), 35 (<xref ref-type="fig" rid="pone.0226345.g005">Fig 5e</xref>) and 40 (<xref ref-type="fig" rid="pone.0226345.g005">Fig 5g</xref>), respectively. In the histon-based segmentation of <xref ref-type="fig" rid="pone.0226345.g005">Fig 5b and 5c</xref>, the red mountain ridge in the distance and the light green of the exterior wall of the building do not match with those in the original image. In addition, in the histon-based and roughness-index-based segmentations of <xref ref-type="fig" rid="pone.0226345.g005">Fig 5b&#x02013;5e</xref>, we observed that the very dark color of the gentle mountain slope at the left bottom corner does not match those in the original image. One can see from the original image (<xref ref-type="fig" rid="pone.0226345.g005">Fig 5a</xref>) that there is a clearly green boundary between the gentle mountain slope at the left bottom corner and the middle mountain. However, this green boundary is almost gone in the segmented results (<xref ref-type="fig" rid="pone.0226345.g005">Fig 5b&#x02013;5e</xref>). Whereas in the <italic>AHHT</italic> technique of segmentation, as shown in <xref ref-type="fig" rid="pone.0226345.g005">Fig 5f and 5g</xref>, we observe that the colors of buildings, mountains, the sky and clouds match exactly with colors of the corresponding regions in the original image.</p><p>For the image <italic>Mountain</italic>, <xref ref-type="fig" rid="pone.0226345.g006">Fig 6</xref> shows the initial segmented result and the result after region merging for the histon-based, roughness-index-based and <italic>AHHT</italic> techniques. For the histon-based, roughness-index-based and <italic>AHHT</italic> techniques, the numbers of colors in the initial segmented result are 128 (<xref ref-type="fig" rid="pone.0226345.g006">Fig 6b</xref>), 201 (<xref ref-type="fig" rid="pone.0226345.g006">Fig 6d</xref>) and 181 (<xref ref-type="fig" rid="pone.0226345.g006">Fig 6f</xref>), respectively; the numbers of colors in the final segmented results are 22 (<xref ref-type="fig" rid="pone.0226345.g006">Fig 6c</xref>), 27(<xref ref-type="fig" rid="pone.0226345.g006">Fig 6e</xref>) and 22 (<xref ref-type="fig" rid="pone.0226345.g006">Fig 6g</xref>), respectively. In the histon-based segmentation of <xref ref-type="fig" rid="pone.0226345.g006">Fig 6b and 6c</xref>, the color of the distant mountain in the middle of the image is a slight violet, and the color of the cloud on the top of the statue is assigned to the color of pink, which do not match those in the original image. For the roughness-index-based segmentations of <xref ref-type="fig" rid="pone.0226345.g006">Fig 6d and 6e</xref>, the color of the middle sky is violet, and the color of the distant mountain is assigned to the color of the top sky, which also do not match those in the original image. However, the <italic>AHHT</italic> technique prevents the above classification errors. We can see from <xref ref-type="fig" rid="pone.0226345.g006">Fig 6f and 6g</xref> that the <italic>AHHT</italic> technique yields much better segmentation results, results in which the colors of clouds, the sky and the distant mountains match those in the original image.</p><p>For the image <italic>Marsh</italic>, <xref ref-type="fig" rid="pone.0226345.g007">Fig 7</xref> shows the initial segmented result and the result after region merging for the histon-based, roughness-index-based and <italic>AHHT</italic> techniques, respectively. For the histon-based, roughness-index-based and <italic>AHHT</italic> techniques, the numbers of colors in initial segmented result are 125 (<xref ref-type="fig" rid="pone.0226345.g007">Fig 7b</xref>), 323 (<xref ref-type="fig" rid="pone.0226345.g007">Fig 7d</xref>) and 245 (<xref ref-type="fig" rid="pone.0226345.g007">Fig 7f</xref>), respectively; and the numbers of colors in final segmented results are 29 (<xref ref-type="fig" rid="pone.0226345.g007">Fig 7c</xref>), 31(<xref ref-type="fig" rid="pone.0226345.g007">Fig 7e</xref>) and 33 (<xref ref-type="fig" rid="pone.0226345.g007">Fig 7g</xref>), respectively. In histon-based segmentation of <xref ref-type="fig" rid="pone.0226345.g007">Fig 7b and 7</xref>, we can see that the color of the inner surface is pink, which does not match the white color of the corresponding regions in the original image. In addition, although both the histon-based and roughness-index-based techniques produce more homogenous water surface, there are considerable pixels of the nearshore that are assigned as part of the water surface. By contrast, in the segmented results (<xref ref-type="fig" rid="pone.0226345.g007">Fig 7f and 7g</xref>) of the <italic>AHHT</italic> technique, the border between the water surface and the nearshore is nicely retained. As shown in <xref ref-type="fig" rid="pone.0226345.g007">Fig 7f and 7g</xref>, we observe that the colors of the boat, water surface, and nearshore match the colors of the corresponding regions in the original image.</p><p>For the image <italic>Boating</italic>, <xref ref-type="fig" rid="pone.0226345.g008">Fig 8</xref> shows the initial segmented result and the result after region merging for the histon-based, roughness-index-based and <italic>AHHT</italic> techniques. For the histon-based, roughness-index-based and <italic>AHHT</italic> techniques, the numbers of colors in the initial segmented result are 290 (<xref ref-type="fig" rid="pone.0226345.g008">Fig 8b</xref>), 373 (<xref ref-type="fig" rid="pone.0226345.g008">Fig 8d</xref>) and 322 (<xref ref-type="fig" rid="pone.0226345.g008">Fig 8f</xref>), respectively; the numbers of colors in the final segmented results are 27 (<xref ref-type="fig" rid="pone.0226345.g008">Fig 8c</xref>), 35(<xref ref-type="fig" rid="pone.0226345.g008">Fig 8e</xref>) and 46 (<xref ref-type="fig" rid="pone.0226345.g008">Fig 8g</xref>), respectively. In the histon-based segmentation of <xref ref-type="fig" rid="pone.0226345.g008">Fig 8b and 8c</xref>, we can see the red and yellow on the top of the umbrella and the white on the side of the bow, and in the original image, these are changed to pink. In the roughness-index-based segmentation of <xref ref-type="fig" rid="pone.0226345.g008">Fig 8d and 8e</xref>, the yellow and green stripe on the surface of the bow is causing a slight confusion. By contrast, the <italic>AHHT</italic> technique obtains better segmentation results; see <xref ref-type="fig" rid="pone.0226345.g008">Fig 8f and 8g</xref>.</p><p>For the image <italic>Snake</italic>, <xref ref-type="fig" rid="pone.0226345.g009">Fig 9</xref> shows the initial segmented result and the result after region merging for all comparison techniques. For the histon-based, roughness-index-based and <italic>AHHT</italic> techniques, the numbers of colors in the initial segmented result are 10 (<xref ref-type="fig" rid="pone.0226345.g009">Fig 9b</xref>), 35 (<xref ref-type="fig" rid="pone.0226345.g009">Fig 9d</xref>) and 197 (<xref ref-type="fig" rid="pone.0226345.g009">Fig 9f</xref>), respectively; and the numbers of colors in the final segmented results are 5 (<xref ref-type="fig" rid="pone.0226345.g009">Fig 9c</xref>), 17 (<xref ref-type="fig" rid="pone.0226345.g009">Fig 9e</xref>) and 11 (<xref ref-type="fig" rid="pone.0226345.g009">Fig 9g</xref>), respectively. In histon-based segmentation of <xref ref-type="fig" rid="pone.0226345.g009">Fig 9b and 9c</xref>, we can see that there are fewer colors in the segmented results, which leads to larger homogenous regions in the results. However, the colors of the snake and desert are the same color, and they do not match their colors in the original image. In addition, the color of the snake&#x02032;s shadow is seen as light green instead of black. In case of roughness-index-based segmentation of <xref ref-type="fig" rid="pone.0226345.g009">Fig 9d and 9e</xref>, the texture of sand surface is not clearly visible, although the snake is clearly visible. In contrast, we can see from the <italic>AHHT</italic>-based segmentation of <xref ref-type="fig" rid="pone.0226345.g009">Fig 9f and 9g</xref> that the colors of the snake, the snake&#x02032;s shadow and the desert, and the texture of the sand surface match those in the original image.</p><p>The analysis of the above experiments illustrates that the initial segmentation plays a decisive role in the full process of segmentation. <italic>AHHT</italic> obtains the best visual features in the initial segmentation, which in turn allows it to produce the best visual features in the end.</p><p>In addition, to better analyze the characteristics of the compared techniques, <xref rid="pone.0226345.t002" ref-type="table">Table 2</xref> presents the mean number of colors in the initial segmented results and the results after region merging over all images of the BSDS300 and the BSDS500. Take BSDS500 for example, for the histon-based, roughness-index-based and <italic>AHHT</italic> techniques, the mean numbers of colors in the initial segmented results are 338, 377 and 333, respectively; the mean number of colors in the final segmented results are 44, 47 and 49, respectively. This illustrates that, although all compared techniques adopt the same <italic>Region-Merging</italic> processing, the <italic>AHHT</italic> technique obtains a slightly larger number of colors in the final segmented results. The reason for this is that, for the <italic>AHHT</italic> technique, there are relatively larger differences between the colors in the initial segmented result, which in turn restricts those colors from being further merged, thus ensuring good segmentation quality.</p><table-wrap id="pone.0226345.t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0226345.t002</object-id><label>Table 2</label><caption><title>Comparison between the mean number of colors in the initial segmented result and the result after merging.</title></caption><alternatives><graphic id="pone.0226345.t002g" xlink:href="pone.0226345.t002"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="2" colspan="1">Image</th><th align="left" rowspan="2" colspan="1">Methods</th><th align="left" colspan="2" rowspan="1">Mean number of colors</th><th align="left" rowspan="2" colspan="1">Image</th><th align="left" rowspan="2" colspan="1">Methods</th><th align="left" colspan="2" rowspan="1">Mean number of colors</th></tr><tr><th align="left" rowspan="1" colspan="1">Initial segmentation</th><th align="left" rowspan="1" colspan="1">After merging</th><th align="left" rowspan="1" colspan="1">Initial segmentation</th><th align="left" rowspan="1" colspan="1">After merging</th></tr></thead><tbody><tr><td align="left" rowspan="3" colspan="1">BSDS300</td><td align="left" rowspan="1" colspan="1">Histon</td><td align="left" rowspan="1" colspan="1">316</td><td align="left" rowspan="1" colspan="1">41</td><td align="left" rowspan="3" colspan="1">BSDS500</td><td align="left" rowspan="1" colspan="1">Histon</td><td align="left" rowspan="1" colspan="1">338</td><td align="left" rowspan="1" colspan="1">44</td></tr><tr><td align="left" rowspan="1" colspan="1">Roughness-index</td><td align="left" rowspan="1" colspan="1">355</td><td align="left" rowspan="1" colspan="1">42</td><td align="left" rowspan="1" colspan="1">Roughness-index</td><td align="left" rowspan="1" colspan="1">377</td><td align="left" rowspan="1" colspan="1">47</td></tr><tr><td align="left" rowspan="1" colspan="1">AHHT</td><td align="left" rowspan="1" colspan="1">327</td><td align="left" rowspan="1" colspan="1">46</td><td align="left" rowspan="1" colspan="1">AHHT</td><td align="left" rowspan="1" colspan="1">333</td><td align="left" rowspan="1" colspan="1">49</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec009"><title>Quantitative evaluation of segmentation results</title><p>In this section, the results of each image segmentation technique are compared using quantitative evaluations, such as the mean square error (<italic>MSE</italic>), <italic>F</italic>(<italic>I</italic>) [<xref rid="pone.0226345.ref037" ref-type="bibr">37</xref>], and <italic>Q</italic>(<italic>I</italic>) [<xref rid="pone.0226345.ref038" ref-type="bibr">38</xref>]. The <italic>MSE</italic> evaluation function can be described as
<disp-formula id="pone.0226345.e021"><alternatives><graphic xlink:href="pone.0226345.e021.jpg" id="pone.0226345.e021g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M21"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msub><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math></alternatives><label>(10)</label></disp-formula>
where <italic>I</italic> is the original <italic>RGB</italic> color image, <italic>M</italic> &#x000d7; <italic>N</italic> is the image size, and <italic>I</italic>&#x02032; is the segmented image of <italic>I</italic>. In general, a lower <italic>MSE</italic> indicates good segmentation quality of the output in the case that the numbers of regions are close for different segmented results. The evaluation function of <italic>F</italic>(<italic>I</italic>) is defined as follows [<xref rid="pone.0226345.ref037" ref-type="bibr">37</xref>]:
<disp-formula id="pone.0226345.e022"><alternatives><graphic xlink:href="pone.0226345.e022.jpg" id="pone.0226345.e022g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M22"><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1000</mml:mn><mml:mo>(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:msqrt><mml:mi>R</mml:mi></mml:msqrt><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msqrt><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math></alternatives><label>(11)</label></disp-formula>
and <italic>Q</italic>(<italic>I</italic>) is further refined from <italic>F</italic>(<italic>I</italic>) by Borsotti <italic>et al</italic>. as follows [<xref rid="pone.0226345.ref038" ref-type="bibr">38</xref>]:
<disp-formula id="pone.0226345.e023"><alternatives><graphic xlink:href="pone.0226345.e023.jpg" id="pone.0226345.e023g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M23"><mml:mi>Q</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1000</mml:mn><mml:mo>(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:msqrt><mml:mi>R</mml:mi></mml:msqrt><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math></alternatives><label>(12)</label></disp-formula>
where <italic>I</italic> is the segmented color image of size <italic>M</italic> &#x000d7; <italic>N</italic>, <italic>R</italic> is the number of regions of the segmented image, <italic>A</italic><sub><italic>j</italic></sub> denotes the number of pixels in the <italic>j</italic>th region. <italic>e</italic><sub><italic>j</italic></sub> is defined as the sum of the color differences between the <italic>RGB</italic> color vectors of the pixels of the <italic>j</italic>th region and the color vector attributed to the <italic>j</italic>th region, and <italic>S</italic>(<italic>A</italic><sub><italic>j</italic></sub>) represents the number of regions having an area equal to <italic>A</italic><sub><italic>j</italic></sub>. Although <italic>F</italic>(<italic>I</italic>) and <italic>Q</italic>(<italic>I</italic>) are different, both measures are used to penalize segmentations that form too many regions and have nonhomogeneous regions by assigning them larger values.</p><p>The <italic>MSE</italic>, <italic>F</italic>(<italic>I</italic>) and <italic>Q</italic>(<italic>I</italic>) values of segmentation results are tabulated in <xref rid="pone.0226345.t003" ref-type="table">Table 3</xref> for the images shown in Figs <xref ref-type="fig" rid="pone.0226345.g004">4</xref>&#x02013;<xref ref-type="fig" rid="pone.0226345.g009">9</xref>. The smaller the values of these indexes, the better the segmentation result should be. The bolded values indicate the best results. The comparison results show that the <italic>AHHT</italic> technique obtains the best <italic>MSE</italic>, <italic>F</italic>(<italic>I</italic>) and <italic>Q</italic>(<italic>I</italic>) values on the same three images; the roughness-index-based technique obtains the best <italic>F</italic>(<italic>I</italic>) and <italic>Q</italic>(<italic>I</italic>) values on the one image. The direct comparison of these results can be obtained by checking the mean value in the last row in <xref rid="pone.0226345.t003" ref-type="table">Table 3</xref>. Obviously, the <italic>AHHT</italic> technique outperforms the roughness-index-based and histon-based techniques by obtaining the relatively small mean values of indexes in segmenting all of these images.</p><table-wrap id="pone.0226345.t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0226345.t003</object-id><label>Table 3</label><caption><title>Comparison <italic>MSE</italic>, <italic>F</italic>(<italic>I</italic>) and <italic>Q</italic>(<italic>I</italic>) evaluation function of segmentation results.</title></caption><alternatives><graphic id="pone.0226345.t003g" xlink:href="pone.0226345.t003"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="2" colspan="1">Image</th><th align="left" colspan="3" rowspan="1">MSE</th><th align="left" colspan="3" rowspan="1">F(I)</th><th align="left" colspan="3" rowspan="1"><italic>Q(I)</italic></th></tr><tr><th align="left" rowspan="1" colspan="1">Histon</th><th align="left" rowspan="1" colspan="1">Roughness-index</th><th align="left" rowspan="1" colspan="1">AHHT</th><th align="left" rowspan="1" colspan="1">Histon</th><th align="left" rowspan="1" colspan="1">Roughness-index</th><th align="left" rowspan="1" colspan="1">AHHT</th><th align="left" rowspan="1" colspan="1">Histon</th><th align="left" rowspan="1" colspan="1">Roughness-index</th><th align="left" rowspan="1" colspan="1">AHHT</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Birds</td><td align="char" char="." rowspan="1" colspan="1">11.59010</td><td align="char" char="." rowspan="1" colspan="1">11.53233</td><td align="char" char="." rowspan="1" colspan="1"><bold>8.61797</bold></td><td align="char" char="." rowspan="1" colspan="1">52.67094</td><td align="char" char="." rowspan="1" colspan="1">70.64469</td><td align="char" char="." rowspan="1" colspan="1"><bold>42.93231</bold></td><td align="char" char="." rowspan="1" colspan="1">78.72281</td><td align="char" char="." rowspan="1" colspan="1">109.90817</td><td align="char" char="." rowspan="1" colspan="1"><bold>65.06391</bold></td></tr><tr><td align="left" rowspan="1" colspan="1">Church</td><td align="char" char="." rowspan="1" colspan="1">14.80610</td><td align="char" char="." rowspan="1" colspan="1">13.21823</td><td align="char" char="." rowspan="1" colspan="1"><bold>12.02938</bold></td><td align="char" char="." rowspan="1" colspan="1">105.59323</td><td align="char" char="." rowspan="1" colspan="1">73.15529</td><td align="char" char="." rowspan="1" colspan="1"><bold>72.32908</bold></td><td align="char" char="." rowspan="1" colspan="1">74.04804</td><td align="char" char="." rowspan="1" colspan="1"><bold>62.60939</bold></td><td align="char" char="." rowspan="1" colspan="1">69.32062</td></tr><tr><td align="left" rowspan="1" colspan="1">Mountain</td><td align="char" char="." rowspan="1" colspan="1">10.39769</td><td align="char" char="." rowspan="1" colspan="1">9.93918</td><td align="char" char="." rowspan="1" colspan="1"><bold>8.95015</bold></td><td align="char" char="." rowspan="1" colspan="1">44.98419</td><td align="char" char="." rowspan="1" colspan="1">43.65162</td><td align="char" char="." rowspan="1" colspan="1"><bold>31.06930</bold></td><td align="char" char="." rowspan="1" colspan="1">39.11403</td><td align="char" char="." rowspan="1" colspan="1">38.92950</td><td align="char" char="." rowspan="1" colspan="1"><bold>27.31039</bold></td></tr><tr><td align="left" rowspan="1" colspan="1">Marsh</td><td align="char" char="." rowspan="1" colspan="1">14.51536</td><td align="char" char="." rowspan="1" colspan="1">12.17336</td><td align="char" char="." rowspan="1" colspan="1"><bold>11.83566</bold></td><td align="char" char="." rowspan="1" colspan="1">105.72585</td><td align="char" char="." rowspan="1" colspan="1">77.40083</td><td align="char" char="." rowspan="1" colspan="1"><bold>74.04451</bold></td><td align="char" char="." rowspan="1" colspan="1">84.94691</td><td align="char" char="." rowspan="1" colspan="1"><bold>74.75858</bold></td><td align="char" char="." rowspan="1" colspan="1">75.07219</td></tr><tr><td align="left" rowspan="1" colspan="1">Boating</td><td align="char" char="." rowspan="1" colspan="1">11.00687</td><td align="char" char="." rowspan="1" colspan="1">9.32096</td><td align="char" char="." rowspan="1" colspan="1"><bold>9.06405</bold></td><td align="char" char="." rowspan="1" colspan="1">76.88258</td><td align="char" char="." rowspan="1" colspan="1"><bold>42.72762</bold></td><td align="char" char="." rowspan="1" colspan="1">50.86452</td><td align="char" char="." rowspan="1" colspan="1">56.00279</td><td align="char" char="." rowspan="1" colspan="1"><bold>37.20927</bold></td><td align="char" char="." rowspan="1" colspan="1">49.51961</td></tr><tr><td align="left" rowspan="1" colspan="1">Snake</td><td align="char" char="." rowspan="1" colspan="1">17.67190</td><td align="char" char="." rowspan="1" colspan="1">13.49228</td><td align="char" char="." rowspan="1" colspan="1"><bold>9.59335</bold></td><td align="char" char="." rowspan="1" colspan="1">184.02131</td><td align="char" char="." rowspan="1" colspan="1">127.79234</td><td align="char" char="." rowspan="1" colspan="1"><bold>52.51785</bold></td><td align="char" char="." rowspan="1" colspan="1">338.82650</td><td align="char" char="." rowspan="1" colspan="1">233.31171</td><td align="char" char="." rowspan="1" colspan="1"><bold>82.90309</bold></td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>Mean value</bold></td><td align="char" char="." rowspan="1" colspan="1">13.33134</td><td align="char" char="." rowspan="1" colspan="1">11.61272</td><td align="char" char="." rowspan="1" colspan="1"><bold>10.01509</bold></td><td align="char" char="." rowspan="1" colspan="1">94.97968</td><td align="char" char="." rowspan="1" colspan="1">72.56207</td><td align="char" char="." rowspan="1" colspan="1"><bold>53.95959</bold></td><td align="char" char="." rowspan="1" colspan="1">111.94351</td><td align="char" char="." rowspan="1" colspan="1">92.78777</td><td align="char" char="." rowspan="1" colspan="1"><bold>61.53164</bold></td></tr></tbody></table></alternatives></table-wrap><p>To better support the abovementioned findings, the mean values of <italic>MSE</italic>, <italic>F</italic>(<italic>I</italic>) and <italic>Q</italic>(<italic>I</italic>) are tabulated in <xref rid="pone.0226345.t004" ref-type="table">Table 4</xref> for all images of the BSDS300 and images of the BSDS500. From the results in <xref rid="pone.0226345.t004" ref-type="table">Table 4</xref>, it is clear that the proposed <italic>AHHT</italic> technique outperforms the other techniques based on the <italic>MSE</italic>, <italic>F</italic>(<italic>I</italic>) and <italic>Q</italic>(<italic>I</italic>) measures.</p><table-wrap id="pone.0226345.t004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0226345.t004</object-id><label>Table 4</label><caption><title>Comparison average of <italic>MSE</italic>, <italic>F</italic>(<italic>I</italic>) and <italic>Q</italic>(<italic>I</italic>) on BSDS300 and on BSDS500.</title></caption><alternatives><graphic id="pone.0226345.t004g" xlink:href="pone.0226345.t004"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Image</th><th align="left" rowspan="1" colspan="1">Methods</th><th align="left" rowspan="1" colspan="1">MSE (Mean)</th><th align="left" rowspan="1" colspan="1">F(I) (Mean)</th><th align="left" rowspan="1" colspan="1">Q(I) (Mean)</th><th align="left" rowspan="1" colspan="1">Image</th><th align="left" rowspan="1" colspan="1">Methods</th><th align="left" rowspan="1" colspan="1">MSE (Mean)</th><th align="left" rowspan="1" colspan="1">F(I) (Mean)</th><th align="left" rowspan="1" colspan="1"><italic>Q(I) (Mean)</italic></th></tr></thead><tbody><tr><td align="left" rowspan="3" colspan="1">BSDS300</td><td align="left" rowspan="1" colspan="1">Histon</td><td align="char" char="." rowspan="1" colspan="1">12.13341</td><td align="char" char="." rowspan="1" colspan="1">69.86678</td><td align="char" char="." rowspan="1" colspan="1">62.92299</td><td align="left" rowspan="3" colspan="1">BSDS500</td><td align="left" rowspan="1" colspan="1">Histon</td><td align="char" char="." rowspan="1" colspan="1">12.09972</td><td align="char" char="." rowspan="1" colspan="1">68.85657</td><td align="char" char="." rowspan="1" colspan="1">61.05272</td></tr><tr><td align="left" rowspan="1" colspan="1">Roughness-index</td><td align="char" char="." rowspan="1" colspan="1">11.69978</td><td align="char" char="." rowspan="1" colspan="1">70.19091</td><td align="char" char="." rowspan="1" colspan="1">65.72476</td><td align="left" rowspan="1" colspan="1">Roughness-index</td><td align="char" char="." rowspan="1" colspan="1">11.70673</td><td align="char" char="." rowspan="1" colspan="1">68.25779</td><td align="char" char="." rowspan="1" colspan="1">61.58926</td></tr><tr><td align="left" rowspan="1" colspan="1">AHHT</td><td align="char" char="." rowspan="1" colspan="1"><bold>10.97841</bold></td><td align="char" char="." rowspan="1" colspan="1"><bold>57.7446</bold></td><td align="char" char="." rowspan="1" colspan="1"><bold>53.18803</bold></td><td align="left" rowspan="1" colspan="1">AHHT</td><td align="char" char="." rowspan="1" colspan="1"><bold>11.04783</bold></td><td align="char" char="." rowspan="1" colspan="1"><bold>57.97239</bold></td><td align="char" char="." rowspan="1" colspan="1"><bold>51.83581</bold></td></tr></tbody></table></alternatives></table-wrap><p>The above benchmark indices are used to estimate the empirical accuracy of the segmentation results. They include some human characterizations on the properties of ideal segmentation requiring no prior knowledge of correct segmentation.</p><p>For each image in the BSDS, a set of ground truths compiled by human observers is provided. Therefore, we intend to compare segmentation results against external criteria. The following image segmentation indices were used. The Probability Rand Index (PRI) counts the fraction of pairs of pixels whose labels are consistent between the computed segmentation and the ground truth, averaging across multiple ground truth segmentations to account for scale variation in human perception [<xref rid="pone.0226345.ref039" ref-type="bibr">39</xref>]. The Variation of Information (VOI) is used for quantification of the loss of information and the gain between two clusters belonging to the lattice of possible partitions [<xref rid="pone.0226345.ref040" ref-type="bibr">40</xref>]. The Boundary Displacement Error (BDE) is used for evaluation of the average displacement error of boundary pixels between two segmented images by computing the distance between the pixel and the closest pixel in the other segmentation [<xref rid="pone.0226345.ref041" ref-type="bibr">41</xref>]. The Global Consistency Error (GCE) is used for quantification of the extent to which a segmentation can be viewed as the refinement of the others [<xref rid="pone.0226345.ref042" ref-type="bibr">42</xref>]. These four measures must be considered all together to evaluate the performance of a given segmentation algorithm. Higher values of PRI indicate a large similarity between the segmented images and the ground truth; whereas for rest of the indices, lower values indicate closer similarity of the segmentation obtained and the ground truth.</p><p><xref rid="pone.0226345.t005" ref-type="table">Table 5</xref> presents the average performance indices (PRI, BDE, GCE and VOI) obtained by the proposed AHHT algorithm compared with Histon and Roughness-index algorithms. As mentioned above, these three algorithms are histogram-based algorithms. In addition, <xref rid="pone.0226345.t005" ref-type="table">Table 5</xref> also lists results of some other popular algorithms. The results of Mean-Shift [<xref rid="pone.0226345.ref043" ref-type="bibr">43</xref>], NCuts [<xref rid="pone.0226345.ref044" ref-type="bibr">44</xref>], FH [<xref rid="pone.0226345.ref045" ref-type="bibr">45</xref>], CTM [<xref rid="pone.0226345.ref012" ref-type="bibr">12</xref>], and MCET_DE [<xref rid="pone.0226345.ref027" ref-type="bibr">27</xref>], were obtained from literature sources [<xref rid="pone.0226345.ref012" ref-type="bibr">12</xref>,<xref rid="pone.0226345.ref027" ref-type="bibr">27</xref>]. For the three histogram-based techniques of Histon, Roughness-index, and AHHT, we found an improvement in the results in terms of GCE and VOI with a larger value of <italic>T</italic><sub><italic>d</italic></sub>, in contrast to that of the PRI measurement, which decreased. Compared with Histon and Roughness-index algorithms, the AHHT algorithm obtains better values of BDE, GCE and VOI when the same value of <italic>T</italic><sub><italic>d</italic></sub> is used. From <xref rid="pone.0226345.t005" ref-type="table">Table 5</xref>, it can be seen that the three histogram-based techniques of Histon, Roughness-index, and AHHT can obtain superior BDE values compared with other algorithms.</p><table-wrap id="pone.0226345.t005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0226345.t005</object-id><label>Table 5</label><caption><title>Comparison averages of PRI, BDE, GCE and VOI on BSDS300.</title></caption><alternatives><graphic id="pone.0226345.t005g" xlink:href="pone.0226345.t005"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Algorithm</th><th align="left" rowspan="1" colspan="1">PRI</th><th align="left" rowspan="1" colspan="1">BDE</th><th align="left" rowspan="1" colspan="1">GCE</th><th align="left" rowspan="1" colspan="1">VOI</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Mean-Shift</td><td align="char" char="." rowspan="1" colspan="1">0.7550</td><td align="char" char="." rowspan="1" colspan="1">9.7001</td><td align="char" char="." rowspan="1" colspan="1">0.2598</td><td align="char" char="." rowspan="1" colspan="1">2.4770</td></tr><tr><td align="left" rowspan="1" colspan="1">Ncuts</td><td align="char" char="." rowspan="1" colspan="1">0.7229</td><td align="char" char="." rowspan="1" colspan="1">9.6038</td><td align="char" char="." rowspan="1" colspan="1">0.2182</td><td align="char" char="." rowspan="1" colspan="1">2.9329</td></tr><tr><td align="left" rowspan="1" colspan="1">FH</td><td align="char" char="." rowspan="1" colspan="1">0.7841</td><td align="char" char="." rowspan="1" colspan="1">9.9497</td><td align="char" char="." rowspan="1" colspan="1">0.1895</td><td align="char" char="." rowspan="1" colspan="1">2.6447</td></tr><tr><td align="left" rowspan="1" colspan="1">CTM(n = 0.2)</td><td align="char" char="." rowspan="1" colspan="1">0.7617</td><td align="char" char="." rowspan="1" colspan="1">9.8962</td><td align="char" char="." rowspan="1" colspan="1">0.1877</td><td align="char" char="." rowspan="1" colspan="1">2.0236</td></tr><tr><td align="left" rowspan="1" colspan="1">MCET_DE(Q = 15,LV = 7)</td><td align="char" char="." rowspan="1" colspan="1">0.7493</td><td align="char" char="." rowspan="1" colspan="1">9.6597</td><td align="char" char="." rowspan="1" colspan="1">0.2542</td><td align="char" char="." rowspan="1" colspan="1">2.1864</td></tr><tr><td align="left" rowspan="1" colspan="1">Histon(<italic>T</italic><sub><italic>d</italic></sub> = 20)</td><td align="char" char="." rowspan="1" colspan="1">0.72361</td><td align="char" char="." rowspan="1" colspan="1">9.476</td><td align="char" char="." rowspan="1" colspan="1">0.41158</td><td align="char" char="." rowspan="1" colspan="1">4.246</td></tr><tr><td align="left" rowspan="1" colspan="1">Histon(<italic>T</italic><sub><italic>d</italic></sub> = 70)</td><td align="char" char="." rowspan="1" colspan="1">0.63756</td><td align="char" char="." rowspan="1" colspan="1">9.1357</td><td align="char" char="." rowspan="1" colspan="1">0.30279</td><td align="char" char="." rowspan="1" colspan="1">2.6513</td></tr><tr><td align="left" rowspan="1" colspan="1">Roughness-index(<italic>T</italic><sub><italic>d</italic></sub> = 20)</td><td align="char" char="." rowspan="1" colspan="1">0.72333</td><td align="char" char="." rowspan="1" colspan="1">9.4655</td><td align="char" char="." rowspan="1" colspan="1">0.41292</td><td align="char" char="." rowspan="1" colspan="1">4.226</td></tr><tr><td align="left" rowspan="1" colspan="1">Roughness-index(<italic>T</italic><sub><italic>d</italic></sub> = 70)</td><td align="char" char="." rowspan="1" colspan="1">0.6396</td><td align="char" char="." rowspan="1" colspan="1">8.7628</td><td align="char" char="." rowspan="1" colspan="1">0.30521</td><td align="char" char="." rowspan="1" colspan="1">2.6578</td></tr><tr><td align="left" rowspan="1" colspan="1">AHHT(<italic>T</italic><sub><italic>d</italic></sub> = 30)</td><td align="char" char="." rowspan="1" colspan="1">0.71937</td><td align="char" char="." rowspan="1" colspan="1">9.1668</td><td align="char" char="." rowspan="1" colspan="1">0.43263</td><td align="char" char="." rowspan="1" colspan="1">3.4961</td></tr><tr><td align="left" rowspan="1" colspan="1">AHHT(<italic>T</italic><sub><italic>d</italic></sub> = 70)</td><td align="char" char="." rowspan="1" colspan="1">0.63414</td><td align="char" char="." rowspan="1" colspan="1">8.6678</td><td align="char" char="." rowspan="1" colspan="1">0.29859</td><td align="char" char="." rowspan="1" colspan="1">2.6386</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec010"><title>Runtime comparison</title><p>The computational efficiency of the algorithm is a key factor that imposes a large influence upon its practical application. In this section, the efficiencies of the three techniques are compared as the execution time in seconds. Considering that the execution times for all compared techniques include two parts, <xref rid="pone.0226345.t006" ref-type="table">Table 6</xref> presents the mean time spent on the initial segmentation, the mean time spent on the Region-Merging process, and the mean total time (the sum of the two former) spent on BSDS300 as well as BSDS500.</p><table-wrap id="pone.0226345.t006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0226345.t006</object-id><label>Table 6</label><caption><title>Mean execution time (in seconds) of different algorithms.</title></caption><alternatives><graphic id="pone.0226345.t006g" xlink:href="pone.0226345.t006"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="2" colspan="1">Image</th><th align="left" colspan="3" rowspan="1">Histon</th><th align="left" colspan="3" rowspan="1">Roughness-index</th><th align="left" colspan="3" rowspan="1">AHHT</th></tr><tr><th align="left" rowspan="1" colspan="1">Initial segmentation</th><th align="left" rowspan="1" colspan="1">Merging process</th><th align="left" rowspan="1" colspan="1">Total</th><th align="left" rowspan="1" colspan="1">Initial segmentation</th><th align="left" rowspan="1" colspan="1">Merging process</th><th align="left" rowspan="1" colspan="1">Total</th><th align="left" rowspan="1" colspan="1">Initial segmentation</th><th align="left" rowspan="1" colspan="1">Merging process</th><th align="left" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">BSDS300</td><td align="char" char="." rowspan="1" colspan="1">8.1459</td><td align="char" char="." rowspan="1" colspan="1">2.0008</td><td align="char" char="." rowspan="1" colspan="1">10.14673</td><td align="char" char="." rowspan="1" colspan="1">9.1763</td><td align="char" char="." rowspan="1" colspan="1">2.7703</td><td align="char" char="." rowspan="1" colspan="1">11.9467</td><td align="char" char="." rowspan="1" colspan="1"><bold>0.0685</bold></td><td align="char" char="." rowspan="1" colspan="1"><bold>2.4703</bold></td><td align="char" char="." rowspan="1" colspan="1"><bold>2.5388</bold></td></tr><tr><td align="left" rowspan="1" colspan="1">BSDS500</td><td align="char" char="." rowspan="1" colspan="1">9.8467</td><td align="char" char="." rowspan="1" colspan="1">2.6347</td><td align="char" char="." rowspan="1" colspan="1">12.4814</td><td align="char" char="." rowspan="1" colspan="1">8.3534</td><td align="char" char="." rowspan="1" colspan="1">2.8033</td><td align="char" char="." rowspan="1" colspan="1">11.1567</td><td align="char" char="." rowspan="1" colspan="1"><bold>0.0681</bold></td><td align="char" char="." rowspan="1" colspan="1"><bold>2.2625</bold></td><td align="char" char="." rowspan="1" colspan="1"><bold>2.3307</bold></td></tr></tbody></table></alternatives></table-wrap><p>From <xref rid="pone.0226345.t006" ref-type="table">Table 6</xref>, we can see that the mean time of initial segmentation of an image is approximately 9 seconds for the histon-based and roughness-index-based techniques. By contrast, the time of initial segmentation of an image is approximately 0.07 seconds for the <italic>AHHT</italic> technique, which means that <italic>AHHT</italic> outperforms the histon-based and roughness-index-based techniques by up to two orders of magnitude in the matter of efficiency of initial segmentation. The complexity analysis shows that the major reason for the big difference is the time required to find the thresholds: the histon-based and roughness-index-based techniques need to calculate the Euclidean distance 24<italic>n</italic> times; however, the <italic>AHHT</italic> technique mainly needs 3<italic>n</italic> instances of pixel access with no complicated calculation involved. Therefore, the <italic>AHHT</italic> technique obtains the great advantage of efficiency in initial segmentation.</p><p>The time spent on the <italic>Region-Merging</italic> process mainly depends on the number of merged regions. In this process, the differences between the compared techniques are not noticeable. The full execution time to segment an image mainly depends on the initial segmentation for the histon-based and roughness-index-based techniques. In contrast, for the <italic>AHHT</italic> algorithm, the full execution time to segment an image largely depends on the merging process. From <xref rid="pone.0226345.t006" ref-type="table">Table 6</xref>, we can see that the <italic>AHHT</italic> technique obtains significantly faster running speeds.</p></sec></sec><sec sec-type="conclusions" id="sec011"><title>Conclusion</title><p>This paper presents a novel histogram thresholding&#x02014;Adaptive Hierarchical-Histogram Thresholding (AHHT), which is an adaptive thresholding algorithm used to perform color image segmentation. The contributions of the paper include the following. (1) A structure called <italic>hierarchical-histogram</italic> has been proposed in the paper. With the aid of <italic>hierarchical-histogram</italic>, the <italic>AHHT</italic> algorithm can adaptively identify the thresholds at valleys. (2) <italic>AHHT</italic> does not need to find the significant peaks. (3) The experimental results show that the <italic>AHHT</italic> algorithm can obtain better results for color image segmentation. (4) For the simplicity of implementation, the <italic>AHHT</italic> algorithm has fast running speed. The experimental results show that <italic>AHHT</italic> outperforms the compared algorithms by up to two orders of magnitude in the matter of efficiency of initial segmentation.</p></sec><sec sec-type="supplementary-material" id="sec012"><title>Supporting information</title><supplementary-material content-type="local-data" id="pone.0226345.s001"><label>S1 File</label><caption><title>Includes the results of AHHT (Td = 20) on each image in BSDS300.</title><p>(XLS)</p></caption><media xlink:href="pone.0226345.s001.xls"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0226345.s002"><label>S2 File</label><caption><title>Includes the results of AHHT (Td = 20) on each image in BSDS500.</title><p>(XLS)</p></caption><media xlink:href="pone.0226345.s002.xls"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0226345.s003"><label>S3 File</label><caption><title>Includes the results of AHHT (Td = 30) on each image in BSDS300.</title><p>(XLS)</p></caption><media xlink:href="pone.0226345.s003.xls"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0226345.s004"><label>S4 File</label><caption><title>Includes the results of Histon (Td = 20) on each image in BSDS300.</title><p>(XLS)</p></caption><media xlink:href="pone.0226345.s004.xls"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0226345.s005"><label>S5 File</label><caption><title>Includes the results of Histon (Td = 20) on each image in BSDS500.</title><p>(XLS)</p></caption><media xlink:href="pone.0226345.s005.xls"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0226345.s006"><label>S6 File</label><caption><title>Includes the results of PRI,BDE, GCE and VOI.</title><p>(XLSX)</p></caption><media xlink:href="pone.0226345.s006.xlsx"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0226345.s007"><label>S7 File</label><caption><title>Includes the results of Roughness(Td = 20) on each image in BSDS300.</title><p>(XLS)</p></caption><media xlink:href="pone.0226345.s007.xls"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0226345.s008"><label>S8 File</label><caption><title>Includes the results of Roughness(Td = 20) on each image in BSDS500.</title><p>(XLS)</p></caption><media xlink:href="pone.0226345.s008.xls"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0226345.s009"><label>S1 Data</label><caption><title>Includes the segmented results of AHHT (Td = 30) on each image in BSDS300.</title><p>The folder being used for calculating the metrics of PRI,BDE, GCE and VOI.</p><p>(ZIP)</p></caption><media xlink:href="pone.0226345.s009.zip"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0226345.s010"><label>S2 Data</label><caption><title>Includes the segmented results of AHHT (Td = 70) on each image in BSDS300, the folder being used for calculating the metrics of PRI,BDE, GCE and VOI.</title><p>(ZIP)</p></caption><media xlink:href="pone.0226345.s010.zip"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0226345.s011"><label>S3 Data</label><caption><title>Includes the segmented results of Histon (Td = 20) on each image in BSDS300, the folder being used for calculating the metrics of PRI,BDE, GCE and VOI.</title><p>(ZIP)</p></caption><media xlink:href="pone.0226345.s011.zip"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0226345.s012"><label>S4 Data</label><caption><title>Includes the segmented results of Histon (Td = 70) on each image in BSDS300, the folder being used for calculating the metrics of PRI,BDE, GCE and VOI.</title><p>(ZIP)</p></caption><media xlink:href="pone.0226345.s012.zip"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0226345.s013"><label>S5 Data</label><caption><title>Includes the segmented results of Roughness(Td = 20) on each image in BSDS300, the folder being used for calculating the metrics of PRI,BDE, GCE and VOI.</title><p>(ZIP)</p></caption><media xlink:href="pone.0226345.s013.zip"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0226345.s014"><label>S6 Data</label><caption><title>Includes the segmented results of Roughness(Td = 70) on each image in BSDS300, the folder being used for calculating the metrics of PRI,BDE, GCE and VOI.</title><p>(ZIP)</p></caption><media xlink:href="pone.0226345.s014.zip"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ref-list><title>References</title><ref id="pone.0226345.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Pare</surname><given-names>S</given-names></name>, <name><surname>Bhandari</surname><given-names>AK.</given-names></name>, <name><surname>Kumar</surname><given-names>A</given-names></name>, <name><surname>Singh</surname><given-names>GK</given-names></name>, <article-title>An optimal color image multilevel thresholding technique using grey-level co-occurrence matrix</article-title>, <source>Expert Systems With Applications</source>. <year>2017</year>;<volume>87</volume>:<fpage>335</fpage>&#x02013;<lpage>362</lpage>. <pub-id pub-id-type="doi">10.1016/j.eswa.2017.06.021</pub-id></mixed-citation></ref><ref id="pone.0226345.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Yue</surname><given-names>XD.</given-names></name>, <name><surname>Miao</surname><given-names>DQ</given-names></name>, <name><surname>Zhang</surname><given-names>N</given-names></name>, <name><surname>Cao</surname><given-names>LB</given-names></name>, <name><surname>Wu</surname><given-names>Q</given-names></name>. <article-title>Multiscale roughness measure for color image segmentation</article-title>. <source>Information Sciences</source>. <year>2012</year>; <volume>216</volume>(<issue>24</issue>): <fpage>93</fpage>&#x02013;<lpage>112</lpage>. <pub-id pub-id-type="doi">10.1016/j.ins.2012.05.025</pub-id>.</mixed-citation></ref><ref id="pone.0226345.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Aghbari</surname><given-names>ZA</given-names></name>, <name><surname>Al-Haj</surname><given-names>R</given-names></name>. <article-title>Hill-manipulation: An effective algorithm for color image segmentation</article-title>, <source>Image &#x00026; Vision Computing</source>. <year>2006</year>; <volume>24</volume>(<issue>8</issue>): <fpage>894</fpage>&#x02013;<lpage>903</lpage>. <pub-id pub-id-type="doi">10.1016/j.imavis.2006.02.013</pub-id></mixed-citation></ref><ref id="pone.0226345.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Yen</surname><given-names>JC</given-names></name>, <name><surname>Chang</surname><given-names>FJ</given-names></name>, <name><surname>Chang</surname><given-names>S</given-names></name>. <article-title>A new criterion for automatic multilevel thresholding</article-title>. <source>IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society</source>. <year>1995</year>; <volume>4</volume>: <fpage>370</fpage>&#x02013;<lpage>378</lpage>. <pub-id pub-id-type="doi">10.1109/83.366472</pub-id>
<pub-id pub-id-type="pmid">18289986</pub-id></mixed-citation></ref><ref id="pone.0226345.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Sahoo</surname><given-names>PK.</given-names></name>, <name><surname>Wilkins</surname><given-names>C</given-names></name>, <name><surname>Yeager</surname><given-names>J</given-names></name>. <article-title>Threshold selection using Renyi&#x02032;s entropy</article-title>. <source>Pattern Recognition</source>. <year>1997</year>; <volume>30</volume>: <fpage>71</fpage>&#x02013;<lpage>84</lpage>. <pub-id pub-id-type="doi">10.1016/S0031-3203(96)00065-9</pub-id></mixed-citation></ref><ref id="pone.0226345.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Canny</surname><given-names>J</given-names></name>. <article-title>A computational approach to edge detection</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>1986</year>; <volume>6</volume>: <fpage>679</fpage>&#x02013;<lpage>698</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.1986.4767851</pub-id></mixed-citation></ref><ref id="pone.0226345.ref007"><label>7</label><mixed-citation publication-type="other">Shoujun Zhou, Yao Lu. Nana Li, Yuanquan Wang. Extension of the virtual electric field model using bilateral-like filter for active contours. Signal, Image and Video Processing. 9 March 2019. <pub-id pub-id-type="doi">10.1007/s11760-019-01456-x</pub-id>.</mixed-citation></ref><ref id="pone.0226345.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Tremeau</surname><given-names>A</given-names></name>, <name><surname>Borel</surname><given-names>N</given-names></name>. <article-title>A region growing and merging algorithm to color segmentation</article-title>[J]. <source>Pattern Recognition</source>. <year>1997</year>; <volume>30</volume>(<issue>7</issue>):<fpage>1191</fpage>&#x02013;<lpage>1203</lpage>. <pub-id pub-id-type="doi">10.1016/s0031-3203(96)00147-1</pub-id></mixed-citation></ref><ref id="pone.0226345.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Sima</surname><given-names>H</given-names></name>, <name><surname>Guo</surname><given-names>P</given-names></name>, <name><surname>Zou</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Xu</surname><given-names>M</given-names></name>. <article-title>Bottom-Up Merging Segmentation for Color Images With Complex Areas</article-title>[J]. <source>IEEE Transactions on Systems Man &#x00026; Cybernetics Systems</source>. <year>2018</year>, <volume>48</volume>:<fpage>354</fpage>&#x02013;<lpage>365</lpage>. <pub-id pub-id-type="doi">10.1109/TSMC.2016.2608831</pub-id></mixed-citation></ref><ref id="pone.0226345.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Tan</surname><given-names>KS</given-names></name>, <name><surname>Isa</surname><given-names>NAM</given-names></name>. <article-title>Color image segmentation using histogram thresholding&#x02014;Fuzzy C-means hybrid approach</article-title>. <source>Pattern Recognition</source>. <year>2011</year>;<volume>44</volume>: <fpage>1</fpage>&#x02013;<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1016/j.patcog.2010.07.013</pub-id></mixed-citation></ref><ref id="pone.0226345.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Tan</surname><given-names>KS</given-names></name>, <name><surname>Isa</surname><given-names>NAM</given-names></name>, <name><surname>Lim</surname><given-names>Wei Hong</given-names></name>. <article-title>Color image segmentation using adaptive unsupervised clustering approach</article-title>. <source>Applied Soft Computing</source>. <year>2013</year>; <volume>13</volume>: <fpage>2017</fpage>&#x02013;<lpage>2036</lpage>. <pub-id pub-id-type="doi">10.1016/j.asoc.2012.11.038</pub-id></mixed-citation></ref><ref id="pone.0226345.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>A.Y</given-names></name>, <name><surname>Wright</surname><given-names>J</given-names></name>, <name><surname>Ma</surname><given-names>Y</given-names></name>, <name><surname>Sastry</surname><given-names>SS</given-names></name>. <article-title>Unsupervised segmentation of natural images via lossy data compression</article-title>[J]. <source>Computer Vision &#x00026; Image Understanding</source>. <year>2008</year>; <volume>2</volume>: <fpage>212</fpage>&#x02013;<lpage>225</lpage>. <pub-id pub-id-type="doi">10.1016/j.cviu.2007.07.005</pub-id></mixed-citation></ref><ref id="pone.0226345.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Vargas</surname><given-names>M&#x000fa;jica</given-names></name>, <name><surname>Funes Dante</surname><given-names>F.J.G.</given-names></name>, <name><surname>Rosalessilva</surname><given-names>A.J</given-names></name>. <article-title>A fuzzy clustering algorithm with spatial robust estimation constraint for noisy color image segmentation</article-title>. <source>Pattern Recognition Letters</source>.<year>2013</year>;<volume>34</volume>:<fpage>400</fpage>&#x02013;<lpage>413</lpage>. <pub-id pub-id-type="doi">10.1016/j.patrec.2012.10.004</pub-id></mixed-citation></ref><ref id="pone.0226345.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>H</given-names></name>, <name><surname>Feng</surname><given-names>Z</given-names></name>, <name><surname>Chaudhary</surname><given-names>V</given-names></name>. <article-title>Pareto-based interval type-2 fuzzy c-means with multi-scale JND color histogram for image segmentation</article-title>[J]. <source>Digital Signal Processing</source>. <year>2018</year>,<volume>76</volume>:<fpage>75</fpage>&#x02013;<lpage>83</lpage>. <pub-id pub-id-type="doi">10.1016/j.dsp.2018.02.005</pub-id></mixed-citation></ref><ref id="pone.0226345.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Mignotte</surname><given-names>M</given-names></name>. <article-title>Segmentation by fusion of histogram-based k-means clusters in different color spaces</article-title>. <source>IEEE Transactions on Image Processing</source>. <year>2008</year>; <volume>5</volume>: <fpage>780</fpage>&#x02013;<lpage>787</lpage>. <pub-id pub-id-type="doi">10.1109/TIP.2008.920761</pub-id>
<pub-id pub-id-type="pmid">18390382</pub-id></mixed-citation></ref><ref id="pone.0226345.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Dariusz</surname><given-names>Ma&#x00142;yszko</given-names></name>, <name><surname>Jaros&#x00142;aw</surname><given-names>Stepaniuk</given-names></name>. <article-title>Adaptive Rough Entropy Clustering Algorithms in Image Segmentation</article-title>. <source>Fundamenta Informaticae</source>. <year>2010</year>; <volume>98</volume>: <fpage>199</fpage>&#x02013;<lpage>231</lpage>. <pub-id pub-id-type="doi">10.3233/FI-2010-224</pub-id></mixed-citation></ref><ref id="pone.0226345.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>HP</given-names></name>, <name><surname>Shen</surname><given-names>XJ</given-names></name>, <name><surname>Long</surname><given-names>JW</given-names></name>. <article-title>Histogram-based colour image fuzzy clustering algorithm</article-title>. <source>Multimedia Tools &#x00026; Applications</source>. <year>2016</year>; <volume>18</volume>: <fpage>11417</fpage>&#x02013;<lpage>11432</lpage>. <pub-id pub-id-type="doi">10.1007/s11042-015-2860-6</pub-id>.</mixed-citation></ref><ref id="pone.0226345.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Cheng</surname><given-names>HD</given-names></name>, <name><surname>Jiang</surname><given-names>XH</given-names></name>, <name><surname>Sun</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>. <article-title>Color image segmentation: advances and prospects</article-title>. <source>Pattern Recognition</source>. <year>2001</year>; <volume>12</volume>: <fpage>2259</fpage>&#x02013;<lpage>2281</lpage>. <pub-id pub-id-type="doi">10.1016/s0031-3203(00)00149-7</pub-id></mixed-citation></ref><ref id="pone.0226345.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Hou</surname><given-names>Z</given-names></name>, <name><surname>Hu</surname><given-names>Q</given-names></name>, <name><surname>Nowinski</surname><given-names>WL</given-names></name>. <article-title>On minimum variance thresholding</article-title>[J]. <source>Pattern Recognition Letters</source>, <year>2006</year>, <volume>27</volume>:<fpage>1732</fpage>&#x02013;<lpage>1743</lpage>. <pub-id pub-id-type="doi">10.1016/j.patrec.2006.04.012</pub-id></mixed-citation></ref><ref id="pone.0226345.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>CH.</given-names></name>, <name><surname>Lee</surname><given-names>CK</given-names></name>. <article-title>Minimum cross entropy thresholding</article-title>. <source>Pattern Recognition</source>. <year>1993</year>; <volume>26</volume>: <fpage>617</fpage>&#x02013;<lpage>625</lpage>. <pub-id pub-id-type="doi">10.1016/0031-3203(93)90115-d</pub-id></mixed-citation></ref><ref id="pone.0226345.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Pal</surname><given-names>NR</given-names></name>. <article-title>On minimum cross-entropy thresholding</article-title>. <source>Pattern Recognition</source>. <year>1996</year>; <volume>4</volume>: <fpage>575</fpage>&#x02013;<lpage>580</lpage>. <pub-id pub-id-type="doi">10.1016/0031-3203(95)00111-5</pub-id></mixed-citation></ref><ref id="pone.0226345.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Malyszko</surname><given-names>Dariusz</given-names></name>, <name><surname>Stepaniuk</surname><given-names>Jaroslaw</given-names></name>. <article-title>Adaptive multilevel rough entropy evolutionary thresholding</article-title>. <source>Information Sciences</source>. <year>2010</year>; <volume>180</volume>: <fpage>1138</fpage>&#x02013;<lpage>1158</lpage>. <pub-id pub-id-type="doi">10.1016/j.ins.2009.11.034</pub-id></mixed-citation></ref><ref id="pone.0226345.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Otsu</surname><given-names>N.</given-names></name>
<article-title>A threshold selection method for grey level histograms</article-title>. <source>IEEE Transactions on System, Man and Cybernetics</source>. <year>1979</year>;<volume>1</volume>: <fpage>62</fpage>&#x02013;<lpage>66</lpage>.</mixed-citation></ref><ref id="pone.0226345.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Albuquerque</surname><given-names>MPD</given-names></name>, <name><surname>Esquef</surname><given-names>IA</given-names></name>., <name><surname>Mello</surname><given-names>ARG</given-names></name>., &#x00026; <name><surname>Albuquerque</surname><given-names>MPD</given-names></name>. <article-title>Image thresholding using tsallis entropy</article-title>. <source>Pattern Recognition Letters</source>. <year>2004</year>; <volume>9</volume>: <fpage>1059</fpage>&#x02013;<lpage>1065</lpage>. <pub-id pub-id-type="doi">10.1016/j.patrec.2004.03.003</pub-id></mixed-citation></ref><ref id="pone.0226345.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Yin</surname><given-names>PY</given-names></name>. <article-title>Multilevel minimum cross entropy threshold selection based on particle swarm optimization</article-title>. <source>Applied Mathematics &#x00026; Computation</source>. <year>2007</year>;<volume>2</volume>: <fpage>503</fpage>&#x02013;<lpage>513</lpage>. <pub-id pub-id-type="doi">10.1016/j.amc.2006.06.057</pub-id></mixed-citation></ref><ref id="pone.0226345.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Sahoo</surname><given-names>PK</given-names></name>, <name><surname>Arora</surname><given-names>G</given-names></name>. <article-title>A thresholding method based on two dimensional Renyi&#x02032;s entropy</article-title>. <source>Pattern Recognition</source>. <year>2004</year>;<volume>37</volume>: <fpage>1149</fpage>&#x02013;<lpage>1161</lpage>. <pub-id pub-id-type="doi">10.1016/j.patcog.2003.10.008</pub-id></mixed-citation></ref><ref id="pone.0226345.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Sarkar</surname><given-names>Soham</given-names></name>, <name><surname>Das</surname><given-names>Swagatam</given-names></name>, <name><surname>Chaudhuri</surname><given-names>Sheli Sinha</given-names></name>. <article-title>A Multilevel color image thresholding scheme based on minimum cross entropy and differential evolution</article-title>. <source>Pattern Recognition Letters</source>. <year>2015</year>;<volume>54</volume>:<fpage>27</fpage>&#x02013;<lpage>35</lpage>. <pub-id pub-id-type="doi">10.1016/j.patrec.2014.11.009</pub-id></mixed-citation></ref><ref id="pone.0226345.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Rosenfeld</surname><given-names>A</given-names></name>, <name><surname>De la Torre</surname><given-names>P</given-names></name><article-title>. Histogram concavity analysis as an aid in threshold selection</article-title>. <source>IEEE Transactions on Systems Man and Cybernetics</source>. <year>1983</year>;<volume>13</volume>: <fpage>231</fpage>&#x02013;<lpage>235</lpage>. <pub-id pub-id-type="doi">10.1109/TSMC.1983.6313118</pub-id></mixed-citation></ref><ref id="pone.0226345.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Lim</surname><given-names>YK</given-names></name>, <name><surname>Lee</surname><given-names>SU</given-names></name>. <article-title>On the color image segmentation algorithm based on the thresholding and the fuzzy c-means techniques</article-title>. <source>Pattern Recognition</source>. <year>1990</year>; <volume>23</volume>: <fpage>935</fpage>&#x02013;<lpage>952</lpage>. <pub-id pub-id-type="doi">10.1016/0031-3203(90)90103-r</pub-id></mixed-citation></ref><ref id="pone.0226345.ref030"><label>30</label><mixed-citation publication-type="other">Mohabey A, Ray AK. Rough set theory based segmentation of color images. In:19th Internat. Conf. North Amer. Fuzzy Inform. Process. Soc.(NAIPS),338&#x02013;342.</mixed-citation></ref><ref id="pone.0226345.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Mohabey</surname><given-names>A</given-names></name>, <name><surname>Ray</surname><given-names>AK</given-names></name>. <article-title>Fusion of rough set theoretic approximations and FCM for color image segmentation</article-title>. <source>IEEE International Conference on Systems, Man, and Cybermetics</source>. <year>2000</year>;<volume>2</volume>: <fpage>1529</fpage>&#x02013;<lpage>1534</lpage>. <pub-id pub-id-type="doi">10.1109/ICSMC.2000.886073</pub-id>.</mixed-citation></ref><ref id="pone.0226345.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Pawlak</surname><given-names>Z</given-names></name>. <article-title>Rough sets</article-title>. <source>International Journal of Computer and Information Sciences</source>. <year>1982</year>;<volume>5</volume>: <fpage>341</fpage>&#x02013;<lpage>356</lpage>. <pub-id pub-id-type="doi">10.1007/BF01001956</pub-id>.</mixed-citation></ref><ref id="pone.0226345.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Mushrif</surname><given-names>MM</given-names></name>, <name><surname>Ray</surname><given-names>AK</given-names></name>. <article-title>Color image segmentation: Rough-set theoretic approach</article-title>. <source>Pattern Recognition Letters</source>. <year>2008</year>;<volume>4</volume>: <fpage>483</fpage>&#x02013;<lpage>493</lpage>. <pub-id pub-id-type="doi">10.1016/j.patrec.2007.10.026</pub-id>.</mixed-citation></ref><ref id="pone.0226345.ref034"><label>34</label><mixed-citation publication-type="journal"><name><surname>Xie</surname><given-names>CH</given-names></name>, <name><surname>Liu</surname><given-names>YJ</given-names></name>, <name><surname>Chang</surname><given-names>JY</given-names></name>. <article-title>Medical image segmentation using rough set and local polynomial regression</article-title>. <source>Multimedia Tools &#x00026; Applications</source>. <year>2015</year>;<volume>6</volume>: <fpage>1885</fpage>&#x02013;<lpage>1914</lpage>. <pub-id pub-id-type="doi">10.1007/s11042-013-1723-2</pub-id>.</mixed-citation></ref><ref id="pone.0226345.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>M</given-names></name>, <name><surname>Shang</surname><given-names>CX</given-names></name>, <name><surname>Feng</surname><given-names>SZ</given-names></name>, <name><surname>Fan</surname><given-names>JP</given-names></name>. <article-title>Quick attribute reduction in inconsistent decision tables</article-title>. <source>Information Sciences</source>. <year>2014</year>; <volume>254</volume>: <fpage>155</fpage>&#x02013;<lpage>180</lpage>. <pub-id pub-id-type="doi">10.1016/j.ins.2013.08.038</pub-id>.</mixed-citation></ref><ref id="pone.0226345.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Cheng</surname><given-names>HD</given-names></name>, <name><surname>Jiang</surname><given-names>XH</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>. <article-title>Color image segmentation based on homogram thresholding and region merging</article-title>. <source>Pattern Recognition</source>. <year>2002</year>; <volume>35</volume>: <fpage>373</fpage>&#x02013;<lpage>393</lpage>. <pub-id pub-id-type="doi">10.1016/S0031-3203(01)00054-1</pub-id>.</mixed-citation></ref><ref id="pone.0226345.ref037"><label>37</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>J</given-names></name>, <name><surname>Yang</surname><given-names>YH</given-names></name>. <article-title>Multiresolution Color Image Segmentation</article-title>. <source>IEEE Transactions on Pattern Analysis &#x00026; Machine Intelligence</source>. <year>1994</year>; <volume>7</volume>: <fpage>689</fpage>&#x02013;<lpage>700</lpage>. <pub-id pub-id-type="doi">10.1109/34.297949</pub-id></mixed-citation></ref><ref id="pone.0226345.ref038"><label>38</label><mixed-citation publication-type="journal"><name><surname>Borsotti</surname></name>
<name><surname>Campadelli</surname></name>, <name><surname>Schettini</surname></name>. <article-title>Quantitative evaluation of color image segmentation results</article-title>. <source>Pattern Recognition Letters</source>. <year>1998</year>;<volume>8</volume>: <fpage>741</fpage>&#x02013;<lpage>747</lpage>. <pub-id pub-id-type="doi">10.1016/S0167-8655(98)00052-X</pub-id></mixed-citation></ref><ref id="pone.0226345.ref039"><label>39</label><mixed-citation publication-type="journal"><name><surname>Unnikrishnan</surname><given-names>R</given-names></name>, <name><surname>Pantofaru</surname><given-names>C</given-names></name>, <name><surname>Hebert</surname><given-names>M</given-names></name>. <article-title>Toward Objective Evaluation of Image Segmentation Algorithms</article-title>[J]. <source>IEEE Transactions on Pattern Analysis &#x00026; Machine Intelligence</source>. <year>2007</year>; <volume>29</volume>(<issue>6</issue>):<fpage>929</fpage>&#x02013;<lpage>944</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2007.1046</pub-id>.<pub-id pub-id-type="pmid">17431294</pub-id></mixed-citation></ref><ref id="pone.0226345.ref040"><label>40</label><mixed-citation publication-type="other">M Meil&#x00103;, Comparing clusterings: an axiomatic view, in Proceedings of the 22nd Int. Conf. on Machine Learning, ICML05, Bonn, 7&#x02013;11 August 2005 (ACM,New York, 2005), pp. 577&#x02013;584.</mixed-citation></ref><ref id="pone.0226345.ref041"><label>41</label><mixed-citation publication-type="book"><name><surname>Freixenet</surname><given-names>J</given-names></name>, <name><surname>Munoz</surname><given-names>X</given-names></name>, <name><surname>Raba</surname><given-names>D</given-names></name>, <name><surname>Marti</surname><given-names>J</given-names></name>, <name><surname>Cuff</surname><given-names>X</given-names></name>, <chapter-title>Yet another survey on image segmentation: region and boundary information integration, in ECCV 2002,Copenhagen, 27 May&#x02014;2 June 2002</chapter-title>
<source>Lecture Notes in Computer Science, 2352</source> (<publisher-name>Springer</publisher-name>, <publisher-loc>Berlin Heidelberg</publisher-loc>, <year>2002</year>), pp. <fpage>408</fpage>&#x02013;<lpage>422</lpage>.</mixed-citation></ref><ref id="pone.0226345.ref042"><label>42</label><mixed-citation publication-type="other">D Martin, C Fowlkes, D Tal, J Malik, A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics, in Proceedings of the 8th Int. Conf. Computer Vision, ICCV 2001, Vancouver, 7&#x02013;14, July 2001, vol. 2 (IEEE, Piscataway, 2001), pp. 416&#x02013;423.</mixed-citation></ref><ref id="pone.0226345.ref043"><label>43</label><mixed-citation publication-type="journal"><name><surname>Comanicu</surname><given-names>D.</given-names></name>, <name><surname>Meer</surname><given-names>P</given-names></name>. <article-title>Mean shift: a robust approach toward feature space analysis</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2002</year>; <volume>24</volume>: <fpage>603</fpage>&#x02013;<lpage>619</lpage>. <pub-id pub-id-type="doi">10.1109/34.1000236</pub-id></mixed-citation></ref><ref id="pone.0226345.ref044"><label>44</label><mixed-citation publication-type="other">J. Shi, J. Malik, Normalized cuts and image segmentation, in: Proceedings of International Conference on Computer Vision and Pattern Recognition. 1997, pp. 731&#x02013;737.</mixed-citation></ref><ref id="pone.0226345.ref045"><label>45</label><mixed-citation publication-type="journal"><name><surname>Felzenszwalb</surname><given-names>P.</given-names></name>, <name><surname>Huttenlocher</surname><given-names>D</given-names></name>. <article-title>Efficient graph-based image segmentation</article-title>. <source>International Journal on Computer Vision</source>. <year>2004</year>; <volume>59</volume> (<issue>2</issue>):<fpage>167</fpage>&#x02013;<lpage>181</lpage>. <pub-id pub-id-type="doi">10.1023/B:VISI.0000022288.19776.77</pub-id>.</mixed-citation></ref></ref-list></back></article>