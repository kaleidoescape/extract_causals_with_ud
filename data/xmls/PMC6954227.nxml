<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">31924842</article-id><article-id pub-id-type="pmc">6954227</article-id><article-id pub-id-type="publisher-id">56958</article-id><article-id pub-id-type="doi">10.1038/s41598-019-56958-y</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Comparison of different input modalities and network structures for deep learning-based seizure detection</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Cho</surname><given-names>Kyung-Ok</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4535-1560</contrib-id><name><surname>Jang</surname><given-names>Hyun-Jong</given-names></name><address><email>hjjang@catholic.ac.kr</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0470 4224</institution-id><institution-id institution-id-type="GRID">grid.411947.e</institution-id><institution>Department of Pharmacology, Department of Biomedicine &#x00026; Health Sciences, Catholic Neuroscience Institute, College of Medicine, </institution><institution>The Catholic University of Korea, </institution></institution-wrap>Seoul, 06591 South Korea </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0470 4224</institution-id><institution-id institution-id-type="GRID">grid.411947.e</institution-id><institution>Department of Physiology, Department of Biomedicine &#x00026; Health Sciences, Catholic Neuroscience Institute, College of Medicine, </institution><institution>The Catholic University of Korea, </institution></institution-wrap>Seoul, 06591 South Korea </aff></contrib-group><pub-date pub-type="epub"><day>10</day><month>1</month><year>2020</year></pub-date><pub-date pub-type="pmc-release"><day>10</day><month>1</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>10</volume><elocation-id>122</elocation-id><history><date date-type="received"><day>12</day><month>6</month><year>2019</year></date><date date-type="accepted"><day>16</day><month>12</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2020</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">The manual review of an electroencephalogram (EEG) for seizure detection is a laborious and error-prone process. Thus, automated seizure detection based on machine learning has been studied for decades. Recently, deep learning has been adopted in order to avoid manual feature extraction and selection. In the present study, we systematically compared the performance of different combinations of input modalities and network structures on a fixed window size and dataset to ascertain an optimal combination of input modalities and network structures. The raw time-series EEG, periodogram of the EEG, 2D images of short-time Fourier transform results, and 2D images of raw EEG waveforms were obtained from 5-s segments of intracranial EEGs recorded from a mouse model of epilepsy. A fully connected neural network (FCNN), recurrent neural network (RNN), and convolutional neural network (CNN) were implemented to classify the various inputs. The classification results for the test dataset showed that CNN performed better than FCNN and RNN, with the area under the curve (AUC) for the receiver operating characteristics curves ranging from 0.983 to 0.984, from 0.985 to 0.989, and from 0.989 to 0.993 for FCNN, RNN, and CNN, respectively. As for input modalities, 2D images of raw EEG waveforms yielded the best result with an AUC of 0.993. Thus, CNN can be the most suitable network structure for automated seizure detection when applied to the images of raw EEG waveforms, since CNN can effectively learn a general spatially-invariant representation of seizure patterns in 2D representations of raw EEG.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Data processing</kwd><kwd>Diagnostic markers</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100003725</institution-id><institution>National Research Foundation of Korea (NRF)</institution></institution-wrap></funding-source><award-id>NRF-2014R1A1A1003382</award-id><award-id>NRF-2017R1D1A1B03030998</award-id><principal-award-recipient><name><surname>Jang</surname><given-names>Hyun-Jong</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2020</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">Epilepsy is defined as having unprovoked recurrent seizures<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. The primary tool for seizure detection is the electroencephalogram (EEG). EEG continuously measures the electrical activity of the brain via electrodes placed on the scalp or the surface of the brain. Manual inspection of long, continuous EEGs for seizure detection is a time consuming and laborious process in both clinical and experimental settings. It can take many hours to meticulously examine days of EEG recordings for patients hospitalized to diagnose epilepsy. In an experimental setting, long-term EEG recordings (even up to several months) are often to be reviewed. Furthermore, the EEG readings made by different inspectors can be inconsistent as the criteria for abnormal EEG findings are experiential<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Therefore, the development of an automated method for seizure detection is necessary.</p><p id="Par3">For decades, various machine learning approaches have been applied to detect seizures automatically<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>. The difficulty in automatic seizure detection is due to the extreme variability in both inter- and intra-patient EEG<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Furthermore, EEG signals are highly non-stationary and nonlinear<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>. Thus, to construct a generalized seizure detector, discriminative features between seizure and non-seizure EEGs should be extracted. Many existing methods have been based on hand-engineered techniques for extracting features in the time domain, frequency domain, time-frequency domain, and using combinations of multiple domains from EEG signals<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. Time domain features include average wave amplitude and duration, coefficient of variation in the wave amplitude and duration, and skewness and kurtosis<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. Frequency domain features can be obtained by a fast Fourier transform (FFT) or periodogram<sup><xref ref-type="bibr" rid="CR10">10</xref>&#x02013;<xref ref-type="bibr" rid="CR12">12</xref></sup>. Time-frequency domain features can be extracted by a short-time Fourier transform (STFT) or wavelet transform<sup><xref ref-type="bibr" rid="CR13">13</xref>&#x02013;<xref ref-type="bibr" rid="CR16">16</xref></sup>. Nonlinear analysis, including an entropy-based approach, has also been used to extract features<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR18">18</xref></sup>. Many studies adopted combinations of multiple domain features to enhance their classification results<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR20">20</xref></sup>. These extracted features were then statistically analyzed, ranked, and classified. The best classifier was determined by comparing the performance of different classifiers for selected features. The types of classifiers have included an artificial neural network<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>, k-nearest neighbor<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, logistic regression<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, na&#x000ef;ve Bayes<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>, random forest<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, and support vector machine<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. Thus, machine learning-based seizure autodetection was traditionally composed of two separate procedures. The first part was the feature extraction process and the other was the classification process applied to the extracted features. Both the identification of the appropriate features and the choice of a proper classifier can play important roles in optimizing algorithm performance. These processes depend heavily on domain expertise and consume a great deal of time and effort to select proper features and classifiers.</p><p id="Par4">Thus, automatic feature learning has substantial advantages over traditional machine learning methods based on manual feature extraction and selection<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. This can be accomplished by the implementation of deep learning, which automatically discovers and learns the discriminative features needed for the classification of inputs<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. Recently, many studies have investigated deep learning for seizure detection. These studies have been based on different deep neural network structures, such as a fully connected neural network (FCNN)<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, convolutional neural network (CNN)<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR25">25</xref>&#x02013;<xref ref-type="bibr" rid="CR27">27</xref></sup>, and recurrent neural network (RNN)<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. These different neural networks can automatically learn discriminative features from various types of data input, including raw temporal EEG<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, FFT results<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, 2-dimensional (2D) representation of STFT results<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, and 2D images of raw EEG<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. The adoption of different input forms and network structures typically makes it difficult to directly compare performance among different deep learning methods. Furthermore, these studies adopted different window sizes for EEG segmentation (e.g., 1-<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, 2-<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, 3-<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, 5-<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>, 8-<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, and 23.6-s<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> windows). In addition, the classifiers were trained and tested on different datasets including public EEG datasets such as the Bonn<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>, CHB-MIT<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>, and Freiburg<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> datasets, or their own datasets<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>. Thus, it is almost impossible to directly compare the results of different studies to ascertain an optimal combination of input modalities and network structures.</p><p id="Par5">Therefore, in the present study, we compared the performance of deep learning-based seizure detection algorithms using combinations of different input forms and network structures to systematically investigate how the input modalities and network structures can affect the characteristics of automated seizure detectors. Since previous studies adopted the time, frequency, and time-frequency domain signals and the images of EEG as inputs, we decided to include all these input domains to meticulously search for the most suitable input forms. Thus, the raw time-series and periodogram of EEGs, 2D images of STFT results, and 2D raw EEG waveform images, which were obtained from experimental intracranial EEG (iEEG) traces in a mouse model of epilepsy, were adopted as input forms. Every input was made from a 5-s segment EEG trace. Since the FCNN, RNN, and CNN have been widely used for EEG classification, we included these network structures to classify our input data. For the raw time-series and periodogram of EEGs, all three networks were applied. For the 2D image inputs, only the CNN was applied. Thus, nine possible combinations of input modalities and network structures were tested in the present study. We considered the nine combinations can provide decent comparison for the classification performance of the widely adopted input modalities and network structures. Then, we tested three previously reported classifiers on our experimental iEEG to compare with the results of the current study. Finally, our classifiers were tested on a human iEEG dataset to validate the results of this study.</p></sec><sec id="Sec2"><title>Methods</title><sec id="Sec3"><title>EEG recording</title><p id="Par6">The iEEGs used in the present study were recorded from mice for epilepsy research. The animal experiments were approved by the Ethics Committee of the Catholic University of Korea and were carried out in accordance with the National Institutes of Health Guide for the Care and Use of Laboratory Animals (NIH Publications No. 80-23). Details of establishing the mouse model of pilocarpine-induced epilepsy were the same as previously described<sup><xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR31">31</xref></sup>. Between 4 and 7 weeks after pilocarpine injection, video/EEG monitoring was conducted for 2 weeks as previously described<sup><xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>. Each mouse was stereotactically implanted with a single epidural recording electrode, placed at AP &#x02212;0.2&#x02009;mm and ML&#x02009;+0.22&#x02009;mm from bregma. Reference electrode was implanted at AP&#x02009;+0.1&#x02009;mm and ML&#x02009;+0.1&#x02009;mm from bregma. Mice underwent continuous monitoring by a wireless video/EEG monitoring system (Data Sciences International). An expert epileptologist evaluated all the EEG traces to detect generalized tonic-clonic seizures with the baseline suppression and the restoration of the EEG amplitudes to the baseline as the onset and offset of the seizures, respectively. Convulsive seizures were further defined by repetitive epileptiform spiking (&#x02265;3&#x02009;Hz) that persisted for more than 3&#x02009;s and were also confirmed by video recordings.</p></sec><sec id="Sec4"><title>Datasets</title><p id="Par7">We obtained the training and test sets from completely separate groups of mice. Single channel iEEG data recorded from 17 mice (total 4,704&#x02009;h) were used as the training set, which contained 249 human-annotated seizure events ranging from 8.34 to 61.25&#x02009;s in duration<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. The test set consisted of 4,272&#x02009;h of EEG recordings from another 15 mice, containing 324 seizure events. To construct a training dataset, we collected seizure EEG segments from annotated seizure events using a 5-s sliding window with a 0.25-s interval. Non-seizure segments were collected from 5&#x02009;min of EEG traces before and after each seizure event using a 5-s sliding window with a 2.5-s interval. We used different intervals for the collection of seizure and non-seizure segments because seizure EEGs were relatively scarce. The total numbers of training segments were 15,828 and 46,753 for the seizure and non-seizure, respectively. We chose to analyze the 5-s segment of EEGs based on our previous study because it was the most efficient for seizure event detection<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. Both seizure and non-seizure segments showed extremely varied patterns (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>), suggesting that it is a very challenging task to extract general features.<fig id="Fig1"><label>Figure 1</label><caption><p>Non-seizure and seizure segments obtained from seven different recordings. Both showed extremely variable patterns. The non-seizure segment was recorded 5&#x02009;min before each seizure segment.</p></caption><graphic xlink:href="41598_2019_56958_Fig1_HTML" id="d29e457"/></fig></p></sec><sec id="Sec5"><title>Input forms</title><p id="Par8">The 5-s segments were transformed into different forms for the input into deep neural networks (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>). The original iEEG was recorded at 1,000&#x02009;Hz. It was down-sampled to 100&#x02009;Hz by averaging sampling, thus resulting in 500 raw EEG data points for 5-s segments (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2a</xref>). A periodogram between 0 and 99&#x02009;Hz (100&#x000a0;data&#x000a0;points) was analyzed from the 5-s segments of the original EEGs (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2b</xref>). The 5-s segments were also transformed into a gray image by an STFT using the Hamming window (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2c</xref>). Finally, the raw EEG waveform was transformed into a black and white image with dimensions of 40&#x02009;&#x000d7;&#x02009;250 pixels (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2d</xref>). In order to more accurately capture the characteristics of seizure EEGs that markedly differ from pre- and post-convulsive EEG patterns<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, we additionally constructed the last input form by concatenating three black and white images of 5-s EEG segments, which were separated by a 2.5-min interval (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2e</xref>). Thus, the last input form became a set of 40&#x02009;&#x000d7;&#x02009;750 pixel-images of the EEG. Because the longest seizure was approximately 60&#x02009;s in our experimental data set, a 2.5-min interval could clearly separate seizure and non-seizure EEGs. In summary, a total of five different input forms, i.e., a down-sampled raw EEG time-series with 500 data points, periodogram results with 100 data points, 50&#x02009;&#x000d7;&#x02009;20 pixel gray images of STFT, and black and white images of raw EEG with dimensions of either 40&#x02009;&#x000d7;&#x02009;250 pixels or 40&#x02009;&#x000d7;&#x02009;750 pixels, were used for the deep learning-based seizure autodetectors.<fig id="Fig2"><label>Figure 2</label><caption><p>The different input forms used in the present study. (<bold>a</bold>) A down-sampled raw time-series EEG with 500 data points. (<bold>b</bold>) Periodogram result with 100 data points. (<bold>c</bold>) Image of an STFT at 50&#x02009;&#x000d7;&#x02009;20 pixels. The gray STFT image was pseudo-colored for demonstration purposes. (<bold>d</bold>) Image of an EEG waveform at 40&#x02009;&#x000d7;&#x02009;250 pixels. (<bold>e</bold>) Concatenated image of three temporally separated images of EEG waveforms at 40&#x02009;&#x000d7;&#x02009;750 pixels. The images are not presented to reflect their actual sizes.</p></caption><graphic xlink:href="41598_2019_56958_Fig2_HTML" id="d29e509"/></fig></p></sec><sec id="Sec6"><title>Network structures</title><p id="Par9">Three different network structures, including the FCNN, RNN, and 1D CNN were used to construct seizure detectors from the raw EEG time-series and periodogram results (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>). We adopted a simple grid search strategy to determine the most suitable network structures during training. We generally tested only three to four values for each parameter because there were too many combinations of input modalities and network structures to perform an extensive search. Two to four layers with various node numbers were tested for the FCNN. For the RNN, unit size for the memory cell was searched. For the CNN, kernel size and the number of convolution layers were grid-searched but the strides were fixed as 1 and 2 for the convolution and pooling layers, respectively. We adopted the simplest structures that yielded the best results. Furthermore, three minibatch sizes were tested for each structure: 64, 128, and 256 samples per minibatch. The FCNN consisted of two hidden layers and an output layer containing two nodes for seizure and non-seizure classification (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3a</xref>). Because the current problem is binary classification, only one node can be used for the output layer. However, we assigned nodes per class for the future implementation of the multi-class problem (i.e., interictal, preictal, and ictal classification). The RNN was implemented with a long short-term memory (LSTM) cell (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3b</xref>). We averaged the outputs from the LSTM cell for each input sequence to construct a layer before the output layer, similar to the structure used in the work by Hussein <italic>et al</italic>. Every two consecutive data points in the input data were passed to the LSTM cell. Thus, the down-sampled raw EEG and periodogram yielded 250 and 50 sequence lengths for the RNN, respectively. The LSTM unit size was determined to be 20. Since the raw time-series EEG and periodogram results were basically 1D data forms, we implemented 1D CNNs for these inputs (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3c</xref>). For both input forms, two consecutive convolutional-pooling layers were sufficient to yield the best results. The outputs of the second pooling layer were flattened and passed to two fully-connected layers. For the gray images of the STFT and black and white images of the raw EEG waveform, only 2D CNNs were applied to construct seizure detectors (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>). As in the case of the 1D CNN, two consecutive convolutional-pooling layers were adequate and achieved the best results. The details of the network structures are summarized in Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>. The deep neural networks were implemented with the TensorFlow deep learning library (<ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org">https://www.tensorflow.org</ext-link>). A dropout was applied on every fully connected hidden layer in each network structure with a ratio of 0.3. The networks were trained on minibatches with sizes ranging from 128 to 256 using the Adam optimizer with default parameters (learning rate: 0.001, decay rate of the first and the second moments: 0.9 and 0.999). Ten percent of training data was randomly selected as a validation set and the validation loss was used as an early stopping criterion to avoid overfitting.<fig id="Fig3"><label>Figure 3</label><caption><p>Network structures used to classify a down-sampled raw time-series EEG and periodogram result. (<bold>a</bold>) A fully connected neural network (FCNN). <bold>(b</bold>) Recurrent neural network (RNN) implemented with a long short-term memory (LSTM) cell. (<bold>c</bold>) Convolutional neural network (CNN) for 1-dimensional (1D) input.</p></caption><graphic xlink:href="41598_2019_56958_Fig3_HTML" id="d29e559"/></fig><fig id="Fig4"><label>Figure 4</label><caption><p>The network structure for 2-dimensional (2D) images. A convolutional neural network (CNN) was used for 2D input.</p></caption><graphic xlink:href="41598_2019_56958_Fig4_HTML" id="d29e568"/></fig></p></sec><sec id="Sec7"><title>Seizure event detection</title><p id="Par10">In our previous study, we built a seizure event detector based on the classifier for the 5-s EEG segments<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. In the present study, we adopted the same procedure to detect seizure events. Briefly, the 5-s EEG segments were continuously classified and nearby seizure segments were joined to form a seizure event when separated seizure segments were detected within 10&#x02009;s in continuous EEGs. Single discrete seizure segment was removed. Then, if the seizure event had 1.2 times higher mean absolute amplitude compared to nearby EEG, the event was finalized as a seizure event. If there was no amplitude change, the event was not considered as a seizure event. This post-processing effectively eliminated much of false positive (FP) segments. The mean absolute amplitude was used to meaningfully estimate EEG amplitudes because positive and negative signals in the EEG can negate each other if they are not converted to absolute values. When the detected event did not overlap with human annotated seizure events, it was considered as a false detection.</p></sec><sec id="Sec8"><title>Validation of models with human iEEGs</title><p id="Par11">To validate the performance difference in the present study, we tested our models on a human iEEG dataset which was used for &#x02018;UPenn and Mayo Clinic&#x02019;s Seizure Detection Challenge&#x02019; held at kaggle.com (<ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/c/seizure-detection">https://www.kaggle.com/c/seizure-detection</ext-link>). There were iEEGs recorded from 8 patients with channel numbers ranging from 16 to 72. Sampling frequency was 5,000&#x02009;Hz except for 500&#x02009;Hz in patient 1. The dataset offered the EEGs as the independent files of 1-s EEG segments. The 1-s segments were divided into ictal, interictal, and test sets. Because we did not have the answer for the test set, we only used the ictal and interictal sets in the current study. We set the frequency to 500&#x02009;Hz and concatenated five of the continuous 1-s segments to build a 5-s segment of EEG. Because the minimum channel was 16, we selected 16 channels to train the seizure classifiers for every patient. We built classifiers for the periodogram with 1D CNN, for the images of the STFT with 2D CNN, and for the images of raw EEG waveform with 2D CNN. We tested only the CNN-based classifiers because the multi-channel data can be easily incorporated with the data channels in CNNs. Both intra- and inter-patient classification were tested. Intra-patient classification was performed with 10-fold cross-validation scheme for each patient. Because there were only 8 patients, inter-patient classification was 8-fold cross-validated, i.e., classifiers trained with 7 patients were tested on the remaining one patient. To draw the ROC curves, the results for all folds were concatenated.</p></sec><sec id="Sec9"><title>Performance metrics and statistics</title><p id="Par12">The true positive (TP) denotes the seizure segment when it is classified as a seizure by a deep neural network. The false negative (FN) is the seizure segment falsely classified as a non-seizure. The true negative (TN) and FP are non-seizure segments classified as a non-seizure and falsely as a seizure, respectively. TP, FN, TN, and FP numbers were presented as mean&#x02009;&#x000b1;&#x02009;SE. One-way analysis of variance (ANOVA) and Tukey&#x02019;s post hoc test were used to compare the numbers. The accuracy, sensitivity, specificity, and F1 score are well-known performance metrics used to evaluate the performance of a binary classifier, and are calculated as follows:<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{rcl}sensitivity &#x00026; = &#x00026; \frac{{\rm{TP}}}{{\rm{TP}}+{\rm{FN}}}\\ specificity &#x00026; = &#x00026; \frac{{\rm{TN}}}{{\rm{FP}}+{\rm{TN}}}\\ accuracy &#x00026; = &#x00026; \frac{{\rm{TP}}+{\rm{TN}}}{{\rm{TP}}+{\rm{FN}}+{\rm{FP}}+{\rm{TN}}}\\ F1\,score &#x00026; = &#x00026; \frac{2\cdot {\rm{TP}}}{2\cdot {\rm{TP}}+{\rm{FP}}+{\rm{FN}}}\end{array}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="right"><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FN</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="right"><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TN</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">FP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">TN</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="right"><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">TN</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">TN</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mspace width=".25em"/><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="normal">TP</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FN</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="41598_2019_56958_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par13">A receiver operating characteristics (ROC) curve was plotted for every classifier and the difference between the two ROC curves was evaluated by the permutation test using 1,000 permutations<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. To compare the performance for event detection, false detection rate (FDR) per hour (number of falsely detected seizure events/hour) was also presented. A <italic>p</italic>-value&#x02009;&#x0003c;0.05 was considered significant.</p></sec></sec><sec id="Sec10" sec-type="results"><title>Results</title><p id="Par14">We trained each combination of input forms and network structures five times and let them classify the 5-s EEG segments collected at 2.5-s intervals from 4,272&#x02009;h of the test data set (n&#x02009;=&#x02009;5 for every classifier). The total number of test segments for the seizures and non-seizures were 928 and 3,804,948, respectively. There were slightly fewer segments for concatenated raw EEG images with dimensions of 40&#x02009;&#x000d7;&#x02009;750 pixels (seizure: 923 and non-seizure 3,730,236).</p><p id="Par15">We first applied the FCNN, RNN, and 1D CNN to raw time-series EEGs (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5a</xref>). The FP numbers decreased in the order of the FCNN, RNN, and 1D CNN as 55,695.8&#x02009;&#x000b1;&#x02009;2,377.05, 25,349.8&#x02009;&#x000b1;&#x02009;1,464.69, and 13,314.8&#x02009;&#x000b1;&#x02009;610.87 [F(2, 12)&#x02009;=&#x02009;175.2, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, one-way ANOVA], respectively (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5b</xref>). The FN numbers among the network structures were not significantly different: 33.6&#x02009;&#x000b1;&#x02009;0.67, 30.8&#x02009;&#x000b1;&#x02009;0.91, and 32.4&#x02009;&#x000b1;&#x02009;0.81 for the FCNN, RNN, and 1D CNN [F(2, 12)&#x02009;=&#x02009;2.741, <italic>p</italic>&#x02009;=&#x02009;0.104, one-way ANOVA], respectively (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5c</xref>). The ROC curve yielded an AUC of 0.983 (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5d</xref>), 0.989 (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5e</xref>), and 0.990 (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5f</xref>) for the FCNN, RNN, and 1D CNN, respectively (<italic>p</italic>&#x02009;=&#x02009;0.039 for the FCNN vs. RNN, <italic>p</italic>&#x02009;=&#x02009;0.012 for the FCNN vs. 1D CNN and <italic>p</italic>&#x02009;=&#x02009;0.327 for the RNN vs. 1D CNN by the permutation test). When the FCNN, RNN, and 1D CNN were applied to periodogram results (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6a</xref>), the pattern was more complex. The FP numbers fluctuated with the FCNN, RNN, and 1D CNN yielding 36,345.0&#x02009;&#x000b1;&#x02009;1,706.69, 66,044.4&#x02009;&#x000b1;&#x02009;1,594.71, and 14,669.2&#x02009;&#x000b1;&#x02009;757.95 [F(2, 12)&#x02009;=&#x02009;330.9, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, one-way ANOVA], respectively (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6b</xref>). The FN number showed a reversed pattern with 34.6&#x02009;&#x000b1;&#x02009;0.81, 30.6&#x02009;&#x000b1;&#x02009;0.67, and 34.8&#x02009;&#x000b1;&#x02009;0.96 for the FCNN, RNN, and 1D CNN [F(2, 12)&#x02009;=&#x02009;8.175, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01, one-way ANOVA], respectively (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6c</xref>). The AUCs were 0.984 (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6d</xref>), 0.985 (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6e</xref>), and 0.989 (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6f</xref>) for the FCNN, RNN, and 1D CNN, respectively (<italic>p</italic>&#x02009;=&#x02009;0.382 for the FCNN vs. RNN, <italic>p</italic>&#x02009;=&#x02009;0.052 for the FCNN vs. 1D CNN, and <italic>p</italic>&#x02009;=&#x02009;0.117 for the RNN vs. 1D CNN by the permutation test).<fig id="Fig5"><label>Figure 5</label><caption><p>Classification results for the down-sampled raw time-series EEGs. (<bold>a</bold>) The inputs were classified with a fully connected neural network (FCNN), recurrent neural network (RNN) and convolutional neural network (CNN) for 1D input. (<bold>b</bold>) False positive (FP) numbers for the FCNN, RNN, and CNN. ***<italic>p</italic>&#x02009; &#x0003c;&#x02009;0.001 vs. FCNN, ###<italic>p</italic>&#x02009; &#x0003c;&#x02009;0.001 vs. RNN. (<bold>c</bold>) False negative (FN) numbers for the FCNN, RNN, and CNN. (<bold>d</bold>) The receiver operating characteristics (ROC) curve for the classification result of the FCNN. (<bold>e</bold>) The ROC curve for the classification result of the RNN. (<bold>f</bold>) The ROC curve for the classification result of the 1D CNN. The area under the curve (AUC) is presented for each ROC curve.</p></caption><graphic xlink:href="41598_2019_56958_Fig5_HTML" id="d29e894"/></fig><fig id="Fig6"><label>Figure 6</label><caption><p>Classification results for the periodogram. (<bold>a</bold>) The inputs were classified using a fully connected neural network (FCNN), recurrent neural network (RNN) and convolutional neural network (CNN) for 1D input. (<bold>b</bold>) False positive (FP) numbers for the FCNN, RNN, and CNN. ***<italic>p</italic>&#x02009; &#x0003c;&#x02009;0.001 vs. FCNN, <sup>###</sup><italic>p</italic>&#x02009; &#x0003c;&#x02009;0.001 vs. RNN. (<bold>c</bold>) False negative (FN) numbers for the FCNN, RNN, and CNN. *<italic>p</italic>&#x02009; &#x0003c;&#x02009;0.05 vs. FCNN, <sup>##</sup><italic>p</italic>&#x02009; &#x0003c;&#x02009;0.01 vs. RNN. (<bold>d</bold>) The receiver operating characteristics (ROC) curve for the classification result of the FCNN. (<bold>e</bold>) The ROC curve for the classification result of the RNN. (<bold>f</bold>) The ROC curve for the classification result of the 1D CNN. The area under the curve (AUC) is presented for each ROC curve.</p></caption><graphic xlink:href="41598_2019_56958_Fig6_HTML" id="d29e939"/></fig></p><p id="Par16">For the 2D image inputs including images of the STFT, raw EEG images with dimensions of 40&#x02009;&#x000d7;&#x02009;250 pixels and concatenated raw EEG images with dimensions of 40&#x02009;&#x000d7;&#x02009;750 pixels, only the 2D CNNs were applied (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7a</xref>). Compared to classifiers applied to the raw time-series EEG and periodogram results, there were many fewer FPs associated with the 2D image inputs. The FP numbers for the STFT image, 40&#x02009;&#x000d7;&#x02009;250 image, and 40&#x02009;&#x000d7;&#x02009;750 image were 7,404.0&#x02009;&#x000b1;&#x02009;569.35, 2,576.1&#x02009;&#x000b1;&#x02009;61.64, and 1,814.6&#x02009;&#x000b1;&#x02009;46.23 [F(2, 12)&#x02009;=&#x02009;83.5, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, one-way ANOVA], respectively (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7b</xref>). The FN numbers were not different among the input modalities: 30.0&#x02009;&#x000b1;&#x02009;0.94, 31.2&#x02009;&#x000b1;&#x02009;1.11, and 28.2&#x02009;&#x000b1;&#x02009;0.58 for the STFT image, 40&#x02009;&#x000d7;&#x02009;250 image, and 40&#x02009;&#x000d7;&#x02009;750 image [F(2, 12)&#x02009;=&#x02009;2.758, <italic>p</italic>&#x02009;=&#x02009;0.103 by one-way ANOVA], respectively (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7c</xref>). The AUC values were 0.991 (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7d</xref>), 0.993 (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7e</xref>), and 0.998 (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7f</xref>) (<italic>p</italic>&#x02009;=&#x02009;0.314 for STFT vs. 40&#x02009;&#x000d7;&#x02009;250, <italic>p</italic>&#x02009;=&#x02009;0.003 for the STFT vs. 40&#x02009;&#x000d7;&#x02009;750, and <italic>p</italic>&#x02009;=&#x02009;0.043 for 40&#x02009;&#x000d7;&#x02009;250 vs. 40&#x02009;&#x000d7;&#x02009;750 by the permutation test). The 40&#x02009;&#x000d7;&#x02009;750 images showed significantly better permutation test results compared to all of the other combinations of input modalities and network structures (data not shown). The accuracy, sensitivity, specificity, and F1 score of each classifier were summarized in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>.<fig id="Fig7"><label>Figure 7</label><caption><p>Classification results for the STFT images at 50&#x02009;&#x000d7;&#x02009;20 pixels, images of the EEG waveform at 40&#x02009;&#x000d7;&#x02009;250 pixels and the concatenated images of three temporally separated images of EEG waveforms at 40&#x02009;&#x000d7;&#x02009;750 pixels. (<bold>a</bold>) The inputs were classified with a convolutional neural network (CNN) for 2D input. (<bold>b</bold>) The false positive (FP) numbers for the STFT images, waveform images at 40&#x02009;&#x000d7;&#x02009;250 pixels and waveform images at 40&#x02009;&#x000d7;&#x02009;750 pixels. ***<italic>p</italic>&#x02009; &#x0003c;&#x02009;0.001 vs. STFT. (<bold>c</bold>) False negative (FN) numbers for the STFT images, waveform images at 40&#x02009;&#x000d7;&#x02009;250 pixels and waveform images at 40&#x02009;&#x000d7;&#x02009;750 pixels. (<bold>d</bold>) The receiver operating characteristics (ROC) curve of the classification result for the STFT images. (<bold>e</bold>) The ROC curve of the classification result for the waveform images at 40&#x02009;&#x000d7;&#x02009;250 pixels. <bold>(f)</bold> The ROC curve of the classification result for the waveform images at 40&#x02009;&#x000d7;&#x02009;750 pixels. The area under the curve (AUC) is presented for each ROC curve.</p></caption><graphic xlink:href="41598_2019_56958_Fig7_HTML" id="d29e1010"/></fig><table-wrap id="Tab1"><label>Table 1</label><caption><p>Classification results for all the input modalities and network structures.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Input forms</th><th>Network structures</th><th>Accuracy</th><th>Sensitivity</th><th>Specificity</th><th>F1 score</th><th>AUC</th><th>FDR</th></tr></thead><tbody><tr><td rowspan="3">Raw time-series EEG</td><td>FCNN</td><td>0.985</td><td>0.963</td><td>0.985</td><td>0.031</td><td>0.983</td><td>0.020</td></tr><tr><td>RNN</td><td>0.993</td><td>0.966</td><td>0.993</td><td>0.066</td><td>0.989</td><td>0.018</td></tr><tr><td>1D CNN</td><td>0.996</td><td>0.965</td><td>0.996</td><td>0.118</td><td>0.990</td><td>0.015</td></tr><tr><td rowspan="3">Periodogram</td><td>FCNN</td><td>0.985</td><td>0.963</td><td>0.985</td><td>0.046</td><td>0.984</td><td>0.020</td></tr><tr><td>RNN</td><td>0.982</td><td>0.967</td><td>0.982</td><td>0.026</td><td>0.985</td><td>0.024</td></tr><tr><td>1D CNN</td><td>0.996</td><td>0.962</td><td>0.996</td><td>0.108</td><td>0.989</td><td>0.016</td></tr><tr><td>Image of STFT</td><td>2D CNN</td><td>0.998</td><td>0.967</td><td>0.998</td><td>0.194</td><td>0.991</td><td>0.011</td></tr><tr><td>40&#x02009;&#x000d7;&#x02009;250 image of EEG</td><td>2D CNN</td><td>0.999</td><td>0.966</td><td>0.999</td><td>0.407</td><td>0.993</td><td>0.009</td></tr><tr><td>40&#x02009;&#x000d7;&#x02009;750 image of EEG</td><td>2D CNN</td><td>0.999</td><td>0.969</td><td>0.999</td><td>0.492</td><td>0.998</td><td>0.008</td></tr><tr><td>O&#x02019;shea <italic>et al.</italic><sup>26</sup></td><td>1D CNN</td><td>0.997</td><td>0.959</td><td>0.997</td><td>0.136</td><td>0.990</td><td>0.012</td></tr><tr><td>Zhou <italic>et al</italic>.<sup>25</sup></td><td>1D CNN</td><td>0.995</td><td>0.957</td><td>0.995</td><td>0.089</td><td>0.989</td><td>0.017</td></tr><tr><td>Cao <italic>et al</italic>.<sup>29</sup></td><td>2D CNN</td><td>0.997</td><td>0.962</td><td>0.997</td><td>0.136</td><td>0.990</td><td>0.015</td></tr></tbody></table><table-wrap-foot><p>AUC: area under the curve, CNN: convolutional neural network, EEG: electroencephalogram, FCNN: fully connected neural network, FDR: false detection rate (n/h), RNN: recurrent neural network, STFT: short-time Fourier transform.</p></table-wrap-foot></table-wrap></p><p id="Par17">Next, we also tested three recently reported seizure classifiers on our iEEG data (bottom three rows of Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>). Briefly, O&#x02019;shea <italic>et al</italic>. built a classifier on the 8-s segments of raw temporal EEG with an 11 layer CNN. Another classifier from Zhou <italic>et al</italic>. was based on 1-s segments of FFT results with a 3 layer CNN. Finally, Cao <italic>et al</italic>. used 2-s segments of the STFT gray image with a 2 layer CNN. AUCs were 0.990, 0.989, and 0.990 for the structures of O&#x02019;shea <italic>et al</italic>., Zhou <italic>et al</italic>., and Cao <italic>et al</italic>., respectively.</p><p id="Par18">The final purpose of the segment classification is to build a seizure event detector. We generated the seizure events based on the detected seizure segments as described in the methods section. Every classifier missed a same short seizure event, i.e., sensitivity for event detection was 0.997 for all the classifiers. However, there were various numbers of false detections. The FDRs are listed in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>. Basically, the FDRs and the numbers of FP segments showed correlation because the seizure events were generated from the results of segment classification. However, since we eliminated much of the discrete FP segments while constructing the seizure events, FDRs were relatively low despite of the many FP segments.</p><p id="Par19">The accuracy, sensitivity, specificity, F1 score, AUC, and FDR for all the classifiers are summarized in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> for comparison. F1 scores were generally very low because of the extreme imbalance in the seizure and non-seizure data, i.e., the FP numbers were much higher than the TP and FN numbers. However, the F1 scores can be greatly improved by the adoption of the 2D raw EEG waveform images.</p><p id="Par20">Because we compared the performance of different classifiers with the experimental mice iEEGs, there are a few concerns with respect to our classifiers being applied in the clinical situation. First, these may not generalize to human seizures since we used EEGs obtained from a mouse model of epilepsy. Second, although multi-channel recordings are common in the clinical setting, the mice iEEGs only had a single channel. Thus, we validated our models with a multi-channel human iEEG dataset used for Kaggle seizure detection challenge. The EEGs were supplied as files of 1-s EEG segments with continuous numbering. When we concatenated the five continuous EEG files, they were joined seamlessly (Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8a</xref>). Thus, we could build classifiers based on 5-s iEEG segments. The channel number was fixed to 16 because patient 2 and 8 had only 16 channels. Channels 1&#x02013;16 were used for all the patients other than patient 7, where channels 16&#x02013;31 were used. AUCs for intra-patient classification were 0.961, 0.998, and 1.000 for the CNN-based classifiers for the periodogram, images of STFT, and images of raw EEG waveforms, respectively (Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8b&#x02013;d</xref>). In case of inter-patient classification, the AUCs were much lower at 0.665, 0.769, and 0.824 (Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8e&#x02013;g</xref>), because the training dataset for human iEEG were much smaller than our mice iEEG dataset. However, the performance order was the same as the mice iEEG, i.e., the performance got better in the order of periodogram, STFT image, and raw EEG waveform image.<fig id="Fig8"><label>Figure 8</label><caption><p>Classification results for the human iEEG. <bold>(a</bold>) Five continuously numbered files were concatenated to form 5-s iEEG segments. Left and right half demonstrate 8 channels of 5-s non-seizure and seizure EEG segments, respectively, of patient 1. The seamless continuation of the EEGs suggested that the files were ordered in a continuous manner. (<bold>b)</bold> The receiver operating characteristics (ROC) curve of the intra-patient classification result for the periodogram results. (<bold>c</bold>) The ROC curve of the intra-patient classification result for the STFT images. (<bold>d</bold>) The ROC curve of the intra-patient classification result for the waveform images at 40&#x02009;&#x000d7;&#x02009;250 pixels. <bold>(e</bold>) The ROC curve of the inter-patient classification result for the periodogram results. (<bold>f)</bold> The ROC curve of the inter-patient classification result for the STFT images. (<bold>g</bold>) The ROC curve of the inter-patient classification result for the waveform images at 40&#x02009;&#x000d7;&#x02009;250 pixels.</p></caption><graphic xlink:href="41598_2019_56958_Fig8_HTML" id="d29e1435"/></fig></p></sec><sec id="Sec11" sec-type="discussion"><title>Discussion</title><p id="Par21">Recent studies on deep learning-based seizure detection adopted different input forms, window sizes, network structures, and datasets (Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">2</xref>). Because of these multiple different factors, it is not possible to directly compare the performance of different approaches for deep learning-based seizure detection. Thus, in the present study, we endeavored to systematically compare how different combinations of input forms and network structures determine the performance of the classifier by using a fixed window size on the same dataset. We included a total of nine different combinations of input modalities and network structures for the comparison.</p><p id="Par22">When the down-sampled raw time-series EEGs were used as the inputs to the FCNN, RNN, and 1D CNN, the CNN resulted in the best AUC with significantly fewer FP results (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>). The RNN showed an intermediate performance with respect to both the FP numbers and AUC. However, with a periodogram, the RNN yielded overall highest FP results but relatively low FN results compared with the FCNN and CNN (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>). These results suggest that the information in the periodograms was not discriminative enough for our RNN implementation and resulted in a greater preference for classifying the given data as seizures than non-seizures. Since the periodogram data had only 100 data points, it may not be able to supply enough information to be encoded in the recurrent network, considering the superior performance of an RNN using 500 data points of down-sampled raw time-series EEG. In a recent report, an RNN utilizing 4,096 data points showed good classification results<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Thus, RNN implementation for EEG analysis might benefit from a longer sequence of data relative to a shorter sequence. However, the CNN also showed the best performance with a periodogram. These results indicate that a CNN could be the best candidate for the construction of a classifier for 1D input data such as raw time-series EEG, periodogram, and FFT. In a recent paper, Zhou <italic>et al</italic>. compared the performance of a CNN-based classifier, assessing both time domain and frequency domain inputs<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. In that study, the frequency domain inputs yielded better results with a 1-s window. However, our results showed no difference between the time and frequency domains when a CNN was used. We speculate that the window size may be a critical factor determining the information in the time domain and a 1-s window was too short to contain enough information for the CNN to extract sufficient discriminative features.</p><p id="Par23">Traditionally, CNNs have been widely used to classify 2D images because the CNN is motivated by the neurons in the visual cortex<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR36">36</xref></sup>. Thus, we tested if a 2D image representation of an EEG could be an adequate input for seizure detection (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>). When a 2D CNN was applied to the images of an STFT, the FP number was much lower than all of the previous classifiers for the raw EEG time-series and periodogram. Moreover, further improvement was achieved with the raw EEG waveform images as input. We speculate that raw EEG images may reflect the information contained in EEG most faithfully and that the CNN can optimally extract the features of EEG signals from these images. This process clearly resembles the human inspection of EEG from the images on a computer screen. Our classifier for the raw EEG waveform images also showed better results compared to the three recent CNN-based seizure classifiers for the raw temporal EEG<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, FFT results<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, and STFT images<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> (Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>). More importantly, classifiers for the multi-channel human iEEG also demonstrated a clear performance advantage with the images of the raw EEG waveform (Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>). These results indicated that the current methods were not confined to the animal model of epilepsy but also applicable to the human epilepsy. Thus, we suggest that the image representation of EEG waveforms is the better option as input for CNN-based seizure classifiers. Furthermore, when making a decision on ambiguous EEG seizures, human inspectors may refer to the EEGs before and after the specific seizure event because an ictal EEG shows a different pattern compared to preictal and postictal EEGs<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. We tried to utilize this strategy by projecting three temporally separated EEGs into one input image. The result was excellent for both the FP number and the AUC. Thus, the concatenation of temporally separated EEGs may provide additional information for distinguishing seizure activity, as expected from the human diagnostic process.</p><p id="Par24">When we used a CNN, the best result was achieved with just two convolutional-pooling layers. In contrast, many researchers implemented more than 6 layers for the CNN<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR37">37</xref>&#x02013;<xref ref-type="bibr" rid="CR39">39</xref></sup>. However, these studies did not include multiple fully connected hidden layers between the convolutional and output layers. In our CNN structures, the two hidden layers may contribute to feature extraction processes and thus diminish the necessity of very deep CNN layers. Otherwise, this may reflect the different requirements based on the receptive field size between different EEG modalities. The receptive fields hierarchically increase with additional convolutional-pooling layers. Because our iEEGs did not require deep CNN architectures, we speculate that the iEEG may contain enough discriminative features even in relatively narrow receptive fields. On the other hand, a scalp EEG may require wider receptive fields because of the averaging effect of the dura and skull<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. Thus, although it seems clear that the CNN is the best option for seizure detection from iEEGs, detailed structures should be adjusted to obtain the best results depending on the EEG modality.</p></sec><sec id="Sec12" sec-type="conclusion"><title>Conclusion</title><p id="Par25">To the best of our knowledge, this is the first report investigating how different input modalities and network structures can affect EEG classification results for seizure detection. Our results demonstrated that a CNN can improve the discrimination between seizure and non-seizure EEGs when EEG segments are presented as raw waveform images. Since any kind of extracted features inevitably loose some information present in the original data, more discriminative information might be learned from raw EEG images, provided that the neural network has the potential to extract features contained in the data. Thus, we conclude that the CNN has a remarkably strong potential for the classification of complex signals including EEGs, electrocardiograms, and electromyograms. The CNN works particularly well when an image representation of the signals is provided, although the detailed structure of the CNN should be adjusted depending on signal modality.</p></sec><sec sec-type="supplementary-material"><title>Supplementary information</title><sec id="Sec13"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2019_56958_MOESM1_ESM.pdf"><caption><p>Supplementary Tables.</p></caption></media></supplementary-material>
</p></sec></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary information</title><p>is available for this paper at 10.1038/s41598-019-56958-y.</p></sec><ack><title>Acknowledgements</title><p>This work was supported by National Research Foundation of Korea (NRF) grants (NRF-2014R1A1A1003382 and NRF-2017R1D1A1B03030998) funded by the Korean government.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>Study design: K-O.C., H-J.J. Seizure experiment: K-O.C. Data analysis: K-O.C., H-J.J. Data interpretation: H-J.J. Writing of first draft: K-O.C., H-J.J. Editing draft: K-O.C., H-J.J.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The source codes for the classifiers are available as open-source Python code on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/jajman/CalssifiersforiEEG">https://github.com/jajman/CalssifiersforiEEG</ext-link>.</p></notes><notes notes-type="COI-statement"><title>Competing interests</title><p id="Par26">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>RS</given-names></name><etal/></person-group><article-title>ILAE official report: a practical clinical definition of epilepsy</article-title><source>Epilepsia</source><year>2014</year><volume>55</volume><fpage>475</fpage><lpage>482</lpage><pub-id pub-id-type="doi">10.1111/epi.12550</pub-id><pub-id pub-id-type="pmid">24730690</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scheffer</surname><given-names>IE</given-names></name><etal/></person-group><article-title>ILAE classification of the epilepsies: Position paper of the ILAE Commission for Classification and Terminology</article-title><source>Epilepsia</source><year>2017</year><volume>58</volume><fpage>512</fpage><lpage>521</lpage><pub-id pub-id-type="doi">10.1111/epi.13709</pub-id><pub-id pub-id-type="pmid">28276062</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>M</given-names></name></person-group><article-title>AR based quadratic feature extraction in the VMD domain for the automated seizure detection of EEG using random forest classifier</article-title><source>Biomed. Signal Process. Control</source><year>2017</year><volume>31</volume><fpage>550</fpage><lpage>559</lpage><pub-id pub-id-type="doi">10.1016/j.bspc.2016.10.001</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohseni</surname><given-names>HR</given-names></name><name><surname>Maghsoudi</surname><given-names>A</given-names></name><name><surname>Shamsollahi</surname><given-names>MB</given-names></name></person-group><article-title>Seizure detection in EEG signals: a comparison of different approaches</article-title><source>In: Proceedings of the IEEE Engineering in Medicine and Biology Society</source><year>2006</year><volume>Suppl</volume><fpage>6724</fpage><lpage>6727</lpage><pub-id pub-id-type="doi">10.1109/IEMBS.2006.260931</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acharya</surname><given-names>UR</given-names></name><name><surname>Sree</surname><given-names>SV</given-names></name><name><surname>Swapna</surname><given-names>G</given-names></name><name><surname>Martis</surname><given-names>RJ</given-names></name><name><surname>Suri</surname><given-names>JS</given-names></name></person-group><article-title>Automated EEG analysis of epilepsy: a review</article-title><source>Knowledge-Based Systems</source><year>2013</year><volume>45</volume><fpage>147</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1016/j.knosys.2013.02.014</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McShane</surname><given-names>T</given-names></name></person-group><article-title>A clinical guide to epileptic syndromes and their treatment</article-title><source>Arch. Dis. Child.</source><year>2004</year><volume>89</volume><issue>6</issue><fpage>591</fpage></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palus</surname><given-names>M</given-names></name></person-group><article-title>Nonlinearity in normal human EEG: cycles, temporal asymmetry, nonstationarity and randomness, not chaos</article-title><source>Biol. Cybern.</source><year>1996</year><volume>75</volume><fpage>389</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1007/s004220050304</pub-id><pub-id pub-id-type="pmid">8983161</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Subha</surname><given-names>DP</given-names></name><name><surname>Joseph</surname><given-names>PK</given-names></name><name><surname>Acharya</surname><given-names>UR</given-names></name><name><surname>Lim</surname><given-names>CM</given-names></name></person-group><article-title>EEG signal analysis: a survey</article-title><source>J. Med. Syst.</source><year>2010</year><volume>34</volume><fpage>195</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1007/s10916-008-9231-z</pub-id><pub-id pub-id-type="pmid">20433058</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Guarnizo, C. &#x00026; Delgado, E. EEG single-channel seizure recognition using empirical mode decomposition and normalized mutual information. <italic>In: Proceedings of the IEEE International Conference on Signal Processing (ICSP). Beijing</italic>, 1&#x02013;4 (2010).</mixed-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>AM</given-names></name><name><surname>Sun</surname><given-names>FT</given-names></name><name><surname>Boto</surname><given-names>EH</given-names></name><name><surname>Wingeier</surname><given-names>BM</given-names></name></person-group><article-title>Automated seizure onset detection for accurate onset time determination in intracranial EEG</article-title><source>Clin. Neurophysiol.</source><year>2008</year><volume>119</volume><fpage>2687</fpage><lpage>2696</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2008.08.025</pub-id><pub-id pub-id-type="pmid">18993113</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alkan</surname><given-names>A</given-names></name><name><surname>Koklukaya</surname><given-names>E</given-names></name><name><surname>Subasi</surname><given-names>A</given-names></name></person-group><article-title>Automatic seizure detection in EEG using logistic regression and artificial neural network</article-title><source>J. Neurosci. Methods</source><year>2005</year><volume>148</volume><fpage>167</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2005.04.009</pub-id><pub-id pub-id-type="pmid">16023730</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akin</surname><given-names>M</given-names></name><name><surname>Kiymik</surname><given-names>MK</given-names></name></person-group><article-title>Application of periodogram and AR spectral analysis to EEG signals</article-title><source>J. Med. Syst.</source><year>2000</year><volume>24</volume><fpage>247</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1023/A:1005553931564</pub-id><pub-id pub-id-type="pmid">11057403</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tzallas</surname><given-names>AT</given-names></name><name><surname>Tsipouras</surname><given-names>MG</given-names></name><name><surname>Fotiadis</surname><given-names>DI</given-names></name></person-group><article-title>Epileptic seizure detection in EEGs using time-frequency analysis</article-title><source>IEEE Trans. Inf. Technol. Biomed.</source><year>2009</year><volume>13</volume><fpage>703</fpage><lpage>710</lpage><pub-id pub-id-type="doi">10.1109/TITB.2009.2017939</pub-id><pub-id pub-id-type="pmid">19304486</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>L</given-names></name><name><surname>Rivero</surname><given-names>D</given-names></name><name><surname>Pazos</surname><given-names>A</given-names></name></person-group><article-title>Epileptic seizure detection using multiwavelet transform based approximate entropy and artificial neural networks</article-title><source>J. Neurosci. Methods</source><year>2010</year><volume>193</volume><fpage>156</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2010.08.030</pub-id><pub-id pub-id-type="pmid">20817036</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>D</given-names></name><name><surname>Wan</surname><given-names>S</given-names></name><name><surname>Xiang</surname><given-names>J</given-names></name><name><surname>Bao</surname><given-names>FS</given-names></name></person-group><article-title>A high-performance seizure detection algorithm based on Discrete Wavelet Transform (DWT) and EEG</article-title><source>PLoS One</source><year>2017</year><volume>12</volume><fpage>e0173138</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0173138</pub-id><pub-id pub-id-type="pmid">28278203</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharmila</surname><given-names>A</given-names></name><name><surname>Geethanjali</surname><given-names>P</given-names></name></person-group><article-title>DWT based detection of epileptic seizure from EEG signals using naive Bayes and k-NN classifiers</article-title><source>IEEE Access</source><year>2016</year><volume>4</volume><fpage>7716</fpage><lpage>7727</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2016.2585661</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivasan</surname><given-names>V</given-names></name><name><surname>Eswaran</surname><given-names>C</given-names></name><name><surname>Sriraam</surname><given-names>N</given-names></name></person-group><article-title>Approximate entropy-based epileptic EEG detection using artificial neural networks</article-title><source>IEEE Trans. Inf. Technol. Biomed.</source><year>2007</year><volume>11</volume><fpage>288</fpage><lpage>295</lpage><pub-id pub-id-type="doi">10.1109/TITB.2006.884369</pub-id><pub-id pub-id-type="pmid">17521078</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acharya</surname><given-names>UR</given-names></name><name><surname>Fujita</surname><given-names>H</given-names></name><name><surname>Sudarshan</surname><given-names>VK</given-names></name><name><surname>Bhat</surname><given-names>S</given-names></name><name><surname>Koh</surname><given-names>JEW</given-names></name></person-group><article-title>Application of entropies for automated diagnosis of epilepsy using EEG signals: A review</article-title><source>Knowledge-Based Systems</source><year>2015</year><volume>88</volume><fpage>85</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1016/j.knosys.2015.08.004</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aarabi</surname><given-names>A</given-names></name><name><surname>Fazel-Rezai</surname><given-names>R</given-names></name><name><surname>Aghakhani</surname><given-names>Y</given-names></name></person-group><article-title>A fuzzy rule-based system for epileptic seizure detection in intracranial EEG</article-title><source>Clin. Neurophysiol.</source><year>2009</year><volume>120</volume><fpage>1648</fpage><lpage>1657</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2009.07.002</pub-id><pub-id pub-id-type="pmid">19632891</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><etal/></person-group><article-title>Automatic epileptic seizure detection in EEG signals using multi-domain feature extraction and nonlinear analysis</article-title><source>Entropy</source><year>2017</year><volume>19</volume><fpage>1</fpage><lpage>17</lpage></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pradhan</surname><given-names>N</given-names></name><name><surname>Sadasivan</surname><given-names>PK</given-names></name><name><surname>Arunodaya</surname><given-names>GR</given-names></name></person-group><article-title>Detection of seizure activity in EEG by an artificial neural network: a preliminary study</article-title><source>Comput. Biomed. Res.</source><year>1996</year><volume>29</volume><fpage>303</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1006/cbmr.1996.0022</pub-id><pub-id pub-id-type="pmid">8812076</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ullah</surname><given-names>I</given-names></name><name><surname>Hussain</surname><given-names>M</given-names></name><name><surname>Qazi</surname><given-names>EUH</given-names></name><name><surname>Aboalsamh</surname><given-names>H</given-names></name></person-group><article-title>An Automated System for epilepsy detection using eeg brain signals based on deep learning approach</article-title><source>Expert Systems with Applications</source><year>2018</year><volume>107</volume><fpage>61</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1016/j.eswa.2018.04.021</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jang</surname><given-names>HJ</given-names></name><name><surname>Cho</surname><given-names>KO</given-names></name></person-group><article-title>Dual deep neural network-based classifiers to detect experimental seizures</article-title><source>Korean J Physiol Pharmacol</source><year>2019</year><volume>23</volume><fpage>131</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.4196/kjpp.2019.23.2.131</pub-id><pub-id pub-id-type="pmid">30820157</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>M</given-names></name><etal/></person-group><article-title>Epileptic Seizure Detection Based on EEG Signals and CNN</article-title><source>Front. Neuroinform.</source><year>2018</year><volume>12</volume><fpage>95</fpage><pub-id pub-id-type="doi">10.3389/fninf.2018.00095</pub-id><pub-id pub-id-type="pmid">30618700</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O&#x02019;Shea</surname><given-names>A</given-names></name><name><surname>Lightbody</surname><given-names>G</given-names></name><name><surname>Boylan</surname><given-names>G</given-names></name><name><surname>Temko</surname><given-names>A</given-names></name></person-group><article-title>Investigating the Impact of CNN Depth on Neonatal Seizure Detection Performance</article-title><source>In: Proceedings of the IEEE Engineering in Medicine and Biology Society</source><year>2018</year><volume>2018</volume><fpage>5862</fpage><lpage>5865</lpage><pub-id pub-id-type="doi">10.1109/EMBC.2018.8513617</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>X</given-names></name><name><surname>Zhou</surname><given-names>L</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name></person-group><article-title>Automatic seizure detection using three-dimensional CNN based on multi-channel EEG</article-title><source>BMC Med. Inform. Decis. Mak.</source><year>2018</year><volume>18</volume><fpage>111</fpage><pub-id pub-id-type="doi">10.1186/s12911-018-0693-8</pub-id><pub-id pub-id-type="pmid">30526571</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hussein</surname><given-names>R</given-names></name><name><surname>Palangi</surname><given-names>H</given-names></name><name><surname>Ward</surname><given-names>RK</given-names></name><name><surname>Wang</surname><given-names>ZJ</given-names></name></person-group><article-title>Optimized deep neural network architecture for robust detection of epileptic seizures using EEG signals</article-title><source>Clin. Neurophysiol.</source><year>2019</year><volume>130</volume><fpage>25</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2018.10.010</pub-id><pub-id pub-id-type="pmid">30472579</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Cao, Y., Guo, Y., Yu, H. &#x00026; Yu, X. Epileptic seizure auto-detection using deep learning method. <italic>In</italic><italic>: 4th International Conference on Systems and Informatics (ICSAI)</italic>, 1076&#x02013;1081 (2017).</mixed-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeong</surname><given-names>KH</given-names></name><name><surname>Lee</surname><given-names>KE</given-names></name><name><surname>Kim</surname><given-names>SY</given-names></name><name><surname>Cho</surname><given-names>KO</given-names></name></person-group><article-title>Upregulation of Kruppel-like factor 6 in the mouse hippocampus after pilocarpine-induced status epilepticus</article-title><source>Neuroscience</source><year>2011</year><volume>186</volume><fpage>170</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2011.02.046</pub-id><pub-id pub-id-type="pmid">21362463</pub-id></element-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Kim, J. E. &#x00026; Cho, K. O. The Pilocarpine Model of Temporal Lobe Epilepsy and EEG Monitoring Using Radiotelemetry System in Mice. <italic>Journal of Visualized Experiments</italic>, 10.3791/56831 (2018).</mixed-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brulet</surname><given-names>R</given-names></name><name><surname>Zhu</surname><given-names>J</given-names></name><name><surname>Aktar</surname><given-names>M</given-names></name><name><surname>Hsieh</surname><given-names>J</given-names></name><name><surname>Cho</surname><given-names>KO</given-names></name></person-group><article-title>Mice with conditional NeuroD1 knockout display reduced aberrant hippocampal neurogenesis but no change in epileptic seizures</article-title><source>Exp. Neurol.</source><year>2017</year><volume>293</volume><fpage>190</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1016/j.expneurol.2017.04.005</pub-id><pub-id pub-id-type="pmid">28427858</pub-id></element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>KO</given-names></name><etal/></person-group><article-title>Aberrant hippocampal neurogenesis contributes to epilepsy and associated cognitive decline</article-title><source>Nature Communications</source><year>2015</year><volume>6</volume><fpage>6606</fpage><pub-id pub-id-type="doi">10.1038/ncomms7606</pub-id><pub-id pub-id-type="pmid">25808087</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Venkatraman</surname><given-names>ES</given-names></name></person-group><article-title>A permutation test to compare receiver operating characteristic curves</article-title><source>Biometrics</source><year>2000</year><volume>56</volume><fpage>1134</fpage><lpage>1138</lpage><pub-id pub-id-type="doi">10.1111/j.0006-341X.2000.01134.x</pub-id><pub-id pub-id-type="pmid">11129471</pub-id></element-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fukushima</surname><given-names>K</given-names></name></person-group><article-title>Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</article-title><source>Biol. Cybern.</source><year>1980</year><volume>36</volume><issue>4</issue><fpage>193</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1007/BF00344251</pub-id><pub-id pub-id-type="pmid">7370364</pub-id></element-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Haffner</surname><given-names>P</given-names></name></person-group><article-title>Gradient-Based Learning Applied to Document Recognition</article-title><source>Proceedings of the IEEE</source><year>1998</year><volume>86</volume><issue>11</issue><fpage>2278</fpage><lpage>2324</lpage><pub-id pub-id-type="doi">10.1109/5.726791</pub-id></element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ansari</surname><given-names>Amir H.</given-names></name><name><surname>Cherian</surname><given-names>Perumpillichira J.</given-names></name><name><surname>Caicedo</surname><given-names>Alexander</given-names></name><name><surname>Naulaers</surname><given-names>Gunnar</given-names></name><name><surname>De Vos</surname><given-names>Maarten</given-names></name><name><surname>Van Huffel</surname><given-names>Sabine</given-names></name></person-group><article-title>Neonatal Seizure Detection Using Deep Convolutional Neural Networks</article-title><source>International Journal of Neural Systems</source><year>2019</year><volume>29</volume><issue>04</issue><fpage>1850011</fpage><pub-id pub-id-type="doi">10.1142/S0129065718500119</pub-id><pub-id pub-id-type="pmid">29747532</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Thodoroff, P., Pineau, J. &#x00026; Lim, A. Learning robust features using deep learning for automatic seizure detection. <italic>arXiv</italic><italic>:</italic><italic>1608</italic>.<italic>00220</italic> (2016).</mixed-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acharya</surname><given-names>UR</given-names></name><name><surname>Oh</surname><given-names>SL</given-names></name><name><surname>Hagiwara</surname><given-names>Y</given-names></name><name><surname>Tan</surname><given-names>JH</given-names></name><name><surname>Adeli</surname><given-names>H</given-names></name></person-group><article-title>Deep convolutional neural network for the automated detection and diagnosis of seizure using EEG signals</article-title><source>Comput. Biol. Med.</source><year>2018</year><volume>100</volume><fpage>270</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1016/j.compbiomed.2017.09.017</pub-id><pub-id pub-id-type="pmid">28974302</pub-id></element-citation></ref></ref-list></back></article>